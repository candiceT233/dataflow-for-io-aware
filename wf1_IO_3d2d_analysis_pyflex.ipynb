{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import json\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "project_root = \".\"\n",
    "\n",
    "pattern_configs = {\n",
    "    \"pyflex\":{\n",
    "        \"task_order_list\" : f\"{project_root}/pyflextrkr/pyflextrkr_s9_script_order.json\",\n",
    "        \"csv_file_path\" : f'{project_root}/workflow_data/summer_sam_4n_pfs_s9_tr_estimated_fixed.csv',\n",
    "        \"plot_file_name\" : 'pyflex_3d_relationship_files.pdf',\n",
    "        \"file_group_patterns\" : [\n",
    "            \"^pr_rlut_.*\\\\.nc\",\n",
    "            \"^cloudid_.*\\\\.nc\",\n",
    "            \"^track_.*\\\\.nc\",\n",
    "            \"^tracknum.*\\\\.nc\",\n",
    "            \"^trackstats_.*\\\\.nc\",\n",
    "            \"^trackstats_sparse_.*\\\\.nc\",\n",
    "            \"^mcs_tracks_.*\\\\.nc\",\n",
    "            \"^IMERG_.*\\\\.nc\",\n",
    "            \"^mcs_tracks_pf_.*\\\\.nc\",\n",
    "            \"^mcs_tracks_robust_.*\\\\.nc\",\n",
    "            \"^mcstrack_.*\\\\.nc\",\n",
    "            \"^mcs_tracks_final_.*\\\\.nc\"\n",
    "        ],\n",
    "        \"result_path\": f\"./pyflex_plots\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define file grouping file_group_patterns\n",
    "\n",
    "FLOW_SCALE = 800\n",
    "d3_plots = False\n",
    "\n",
    "def shorten_task_name(name):\n",
    "    parts = name.split(\"_\")\n",
    "    # print(f\"Original parts: {parts}\")\n",
    "    shortened = [p[:10] for i, p in enumerate(parts) if i % 2 == 0 or i == len(parts) - 1]\n",
    "    # print(f\"Shortened: {shortened}\")\n",
    "    return \"_\".join(shortened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read CSV file\n",
    "# csv_file_path = '{project_root}/workflow_data/par_9000_1n_pfs_ps300_fixed.csv'\n",
    "# plot_file_name = '9000_3d_relationship_files.pdf'\n",
    "\n",
    "# csv_file_path = '{project_root}/workflow_data/summer_sam_4n_pfs_s9_tr_estimated.csv'\n",
    "# plot_file_name = 'pyflex_3d_relationship_files.pdf'\n",
    "\n",
    "CURR_WF = \"pyflex\" # pyflex, ddmd, 1kgenome\n",
    "\n",
    "csv_file_path = pattern_configs[CURR_WF][\"csv_file_path\"]\n",
    "plot_file_name = pattern_configs[CURR_WF][\"plot_file_name\"]\n",
    "file_group_patterns = pattern_configs[CURR_WF][\"file_group_patterns\"]\n",
    "extension_grouping = pattern_configs[CURR_WF].get(\"extension_grouping\", False)\n",
    "result_path = pattern_configs[CURR_WF].get(\"result_path\", \"./result_plots\")\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size, df.shape, df.columns, df['operation'].unique()\n",
    "\n",
    "# Replace with shortened task name in all taskName columns\n",
    "short_taskName_dict = {\n",
    "    \"run_idfeature\": \"idfea\",\n",
    "    \"run_tracksingle\": \"trksg\",\n",
    "    \"run_gettracks\": \"gettr\",\n",
    "    \"run_trackstats\": \"trkst\",\n",
    "    \"run_identifymcs\": \"idmcs\",\n",
    "    \"run_matchpf\": \"mtpf\",\n",
    "    \"run_robustmcs\": \"rmcs\",\n",
    "    \"run_mapfeature\": \"mapf\",\n",
    "    \"run_speed\": \"speed\",\n",
    "    \"final_data\": \"final\",\n",
    "    \"initial_data\": \"initial\",\n",
    "}\n",
    "\n",
    "# Replace task names in the DataFrame\n",
    "df['taskName'] = df['taskName'].replace(short_taskName_dict)\n",
    "# Replace task names in the DataFrame\n",
    "df['prevTask'] = df['prevTask'].replace(short_taskName_dict)\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Dataframe Info:\n",
    "\n",
    "# # of unique Stages\n",
    "unique_stages = df['stageOrder'].unique()\n",
    "# # of unique Tasks\n",
    "unique_tasks = df['taskName'].unique()\n",
    "# # Task instance count based on dominant I/O type\n",
    "task_instance_counts = (\n",
    "    df.groupby(['taskName', 'operation'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .max(axis=1)\n",
    ")\n",
    "\n",
    "# If needed as a total count\n",
    "total_task_instances = task_instance_counts.sum()\n",
    "\n",
    "# # of I/O entries (rows in the DataFrame)\n",
    "num_io_entries = df.shape[0]\n",
    "# # of unique I/O files\n",
    "unique_files = df['fileName'].unique()\n",
    "# # average file resues: calculate the average number of times each file is reused for each unique file\n",
    "avg_file_reuses = df.groupby('fileName').size().mean()\n",
    "# Total I/O size\n",
    "total_io_size = df['aggregateFilesizeMB'].sum()\n",
    "\n",
    "# Print all the information\n",
    "print(\"=========================================\")\n",
    "print(f\"Number of unique stages: {len(unique_stages)}\")\n",
    "# print(f\"Number of unique tasks: {len(unique_tasks)}\")\n",
    "print(f\"Total task instances: {total_task_instances}\")\n",
    "print(f\"Total I/O size (MB): {total_io_size:.2f}\")\n",
    "print(f\"Total I/O size (GB): {total_io_size / 1024:.2f}\")\n",
    "# print(f\"Number of I/O entries: {num_io_entries}\")\n",
    "print(f\"Number of unique I/O files: {len(unique_files)}\")\n",
    "print(f\"Average file reuses: {avg_file_reuses:.2f}\")\n",
    "print(\"=========================================\")\n",
    "# # Print unique stages and tasks and File names\n",
    "# print(f\"Unique stages: {unique_stages}\")\n",
    "# print(f\"Unique tasks: {unique_tasks}\")\n",
    "# print(f\"Unique file names: {unique_files}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 95%ile of the aggregateFilesizeMB\n",
    "# 50%ile\n",
    "percentile_50 = df['aggregateFilesizeMB'].quantile(0.50)\n",
    "print(f\"50th percentile of aggregateFilesizeMB: {percentile_50:.2f}\")\n",
    "# 75th percentile\n",
    "percentile_75 = df['aggregateFilesizeMB'].quantile(0.75)\n",
    "print(f\"75th percentile of aggregateFilesizeMB: {percentile_75:.2f}\")\n",
    "# 85th percentile\n",
    "percentile_85 = df['aggregateFilesizeMB'].quantile(0.85)\n",
    "print(f\"85th percentile of aggregateFilesizeMB: {percentile_85:.2f}\")\n",
    "# 95th percentile\n",
    "percentile_95 = df['aggregateFilesizeMB'].quantile(0.95)\n",
    "# 99th percentile\n",
    "percentile_99 = df['aggregateFilesizeMB'].quantile(0.99)\n",
    "print(f\"95th percentile of aggregateFilesizeMB: {percentile_95:.2f}\")\n",
    "print(f\"99th percentile of aggregateFilesizeMB: {percentile_99:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get the max trMiB of taskName tracksingle\n",
    "max_trMiB = df[df['taskName'] == 'trksg']['trMiB'].max()\n",
    "print(f\"Max trMiB of taskName tracksingle: {max_trMiB:.2f}\")\n",
    "\n",
    "max_aggregateFilesizeMB = df[df['taskName'] == 'trksg']['aggregateFilesizeMB'].max()\n",
    "print(f\"Max aggregateFilesizeMB of taskName tracksingle: {max_aggregateFilesizeMB:.2f}\")\n",
    "\n",
    "# get max trMiB of df \n",
    "max_trMiB = df['trMiB'].max()\n",
    "print(f\"Max trMiB of all tasks: {max_trMiB:.2f}\")\n",
    "\n",
    "# get average trMiB of all tasks\n",
    "avg_trMiB = df['trMiB'].mean()\n",
    "print(f\"Avg trMiB of all tasks: {avg_trMiB:.2f}\")\n",
    "\n",
    "# get the average trMiB of all unique tasks\n",
    "avg_trMiB = df.groupby('taskName')['trMiB'].mean()\n",
    "print(f\"Avg trMiB of all unique tasks: {avg_trMiB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the percent of I/O entries with taskName mapfeature\n",
    "num_mapfeature_entries = df[df['taskName'] == 'mapf'].shape[0]\n",
    "num_io_entries = df.shape[0]\n",
    "print(f\"Percent of I/O entries with taskName mapfeature: {num_mapfeature_entries / num_io_entries:.2%}\")\n",
    "num_mapfeature_read_entries = df[(df['taskName'] == 'mapf') & (df['operation'] == 1)].shape[0]\n",
    "print(f\"Percent of read I/O entries with taskName mapfeature: {num_mapfeature_read_entries / num_mapfeature_entries:.2%}\")\n",
    "num_mapfeature_write_entries = df[(df['taskName'] == 'mapf') & (df['operation'] == 0)].shape[0]\n",
    "print(f\"Percent of write I/O entries with taskName mapfeature: {num_mapfeature_write_entries / num_mapfeature_entries:.2%}\")\n",
    "\n",
    "# get number of write I/O entries\n",
    "num_write_entries = df[df['operation'] == 0].shape[0]\n",
    "ave_write_bw = df[df['operation'] == 0]['trMiB'].mean()\n",
    "print(f\"Number of write I/O entries: {num_write_entries}\")\n",
    "print(f\"Average write I/O bandwidth: {ave_write_bw:.2f} MiB\")\n",
    "# get number of read I/O entries\n",
    "num_read_entries = df[df['operation'] == 1].shape[0]\n",
    "ave_read_bw = df[df['operation'] == 1]['trMiB'].mean()\n",
    "print(f\"Number of read I/O entries: {num_read_entries}\")\n",
    "print(f\"Average read I/O bandwidth: {ave_read_bw:.2f} MiB\")\n",
    "\n",
    "# printe percent of read and write I/O entries\n",
    "print(f\"Percent of read I/O entries: {num_read_entries / num_io_entries:.2%}\")\n",
    "print(f\"Percent of write I/O entries: {num_write_entries / num_io_entries:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 95%ile of the aggregateFilesizeMB\n",
    "# 50%ile\n",
    "percentile_50 = df['aggregateFilesizeMB'].quantile(0.50)\n",
    "print(f\"50th percentile of aggregateFilesizeMB: {percentile_50:.2f}\")\n",
    "# 75th percentile\n",
    "percentile_75 = df['aggregateFilesizeMB'].quantile(0.75)\n",
    "print(f\"75th percentile of aggregateFilesizeMB: {percentile_75:.2f}\")\n",
    "# 85th percentile\n",
    "percentile_85 = df['aggregateFilesizeMB'].quantile(0.85)\n",
    "print(f\"85th percentile of aggregateFilesizeMB: {percentile_85:.2f}\")\n",
    "# 95th percentile\n",
    "percentile_95 = df['aggregateFilesizeMB'].quantile(0.95)\n",
    "# 99th percentile\n",
    "percentile_99 = df['aggregateFilesizeMB'].quantile(0.99)\n",
    "print(f\"95th percentile of aggregateFilesizeMB: {percentile_95:.2f}\")\n",
    "print(f\"99th percentile of aggregateFilesizeMB: {percentile_99:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get the max trMiB of taskName tracksingle\n",
    "max_trMiB = df[df['taskName'] == 'IterDe']['trMiB'].max()\n",
    "print(f\"Max trMiB of taskName tracksingle: {max_trMiB:.2f}\")\n",
    "\n",
    "max_aggregateFilesizeMB = df[df['taskName'] == 'IterDe']['aggregateFilesizeMB'].max()\n",
    "print(f\"Max aggregateFilesizeMB of taskName tracksingle: {max_aggregateFilesizeMB:.2f}\")\n",
    "\n",
    "# get max trMiB of df \n",
    "max_trMiB = df['trMiB'].max()\n",
    "print(f\"Max trMiB of all tasks: {max_trMiB:.2f}\")\n",
    "\n",
    "# get min trMiB of df\n",
    "min_trMiB = df['trMiB'].min()\n",
    "print(f\"Min trMiB of all tasks: {min_trMiB:.2f}\")\n",
    "\n",
    "# get 50 percentile of trMiB\n",
    "percentile_50 = df['trMiB'].quantile(0.50)\n",
    "print(f\"50th percentile of trMiB: {percentile_50:.2f}\")\n",
    "\n",
    "# get 75 percentile of trMiB\n",
    "percentile_75 = df['trMiB'].quantile(0.75)\n",
    "print(f\"75th percentile of trMiB: {percentile_75:.2f}\")\n",
    "\n",
    "# get 85 percentile of trMiB\n",
    "percentile_85 = df['trMiB'].quantile(0.85)\n",
    "print(f\"85th percentile of trMiB: {percentile_85:.2f}\")\n",
    "\n",
    "# get average trMiB of all tasks\n",
    "avg_trMiB = df['trMiB'].mean()\n",
    "print(f\"Avg trMiB of all tasks: {avg_trMiB:.2f}\")\n",
    "\n",
    "# get the average trMiB of all unique tasks\n",
    "avg_trMiB = df.groupby('taskName')['trMiB'].mean()\n",
    "print(f\"Avg trMiB of all unique tasks: {avg_trMiB}\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "unique_tasks = df['taskName'].unique()\n",
    "\n",
    "for task in unique_tasks:\n",
    "    avg_trMiB = df[df['taskName'] == task]['trMiB'].mean()\n",
    "    print(f\"Avg trMiB of task {task}: {avg_trMiB:.2f}\")\n",
    "    # min trMiB of task\n",
    "    min_trMiB = df[df['taskName'] == task]['trMiB'].min()\n",
    "    print(f\"Min trMiB of task {task}: {min_trMiB:.2f}\")\n",
    "    # max trMiB of task\n",
    "    max_trMiB = df[df['taskName'] == task]['trMiB'].max()\n",
    "    print(f\"Max trMiB of task {task}: {max_trMiB:.2f}\")\n",
    "    \n",
    "    # get the average opCount of task\n",
    "    avg_opCount = df[df['taskName'] == task]['opCount'].mean()\n",
    "    print(f\"Avg opCount of task {task}: {avg_opCount:.2f}\")\n",
    "    # min opCount of task\n",
    "    min_opCount = df[df['taskName'] == task]['opCount'].min()\n",
    "    print(f\"Min opCount of task {task}: {min_opCount:.2f}\")\n",
    "    # max opCount of task\n",
    "    max_opCount = df[df['taskName'] == task]['opCount'].max()\n",
    "    print(f\"Max opCount of task {task}: {max_opCount:.2f}\")\n",
    "    \n",
    "    \n",
    "    # get number of entries of task\n",
    "    num_entries = df[df['taskName'] == task].shape[0]\n",
    "    print(f\"Number of entries of task {task}: {num_entries}\")\n",
    "    # get the average aggregateFilesizeMB of task\n",
    "    avg_aggregateFilesizeMB = df[df['taskName'] == task]['aggregateFilesizeMB'].mean()\n",
    "    print(f\"Avg aggregateFilesizeMB of task {task}: {avg_aggregateFilesizeMB:.2f}\")\n",
    "    # get number of read entries of task\n",
    "    num_read_entries = df[(df['taskName'] == task) & (df['operation'] == 1)].shape[0]\n",
    "    print(f\"Number of read entries of task {task}: {num_read_entries}\")\n",
    "    # get number of write entries of task\n",
    "    num_write_entries = df[(df['taskName'] == task) & (df['operation'] == 0)].shape[0]\n",
    "    print(f\"Number of write entries of task {task}: {num_write_entries}\")\n",
    "    \n",
    "    # get the parallelized trMiB of task\n",
    "    task_parallelism = df[df['taskName'] == task]['parallelism'].mean()\n",
    "    print(f\"Parallelism of task {task}: {task_parallelism:.2f}\")\n",
    "    if task_parallelism == 1:\n",
    "        task_total_trMiB = df[df['taskName'] == task]['trMiB'].mean()\n",
    "        print(f\"Total trMiB of task {task}: {task_total_trMiB:.2f}\")\n",
    "    else:\n",
    "        task_total_trMiB = df[df['taskName'] == task]['trMiB'].sum()\n",
    "        print(f\"Total trMiB of task {task}: {task_total_trMiB:.2f}\")\n",
    "    task_parallelised_trMiB = task_total_trMiB / task_parallelism\n",
    "    print(f\"Parallelized trMiB of task {task}: {task_parallelised_trMiB:.2f}\")\n",
    "    \n",
    "    # read operation average trMiB\n",
    "    read_trMiB = df[(df['taskName'] == task) & (df['operation'] == 1)]['trMiB'].mean()\n",
    "    print(f\"Avg read trMiB of task {task}: {read_trMiB:.2f}\")\n",
    "    \n",
    "    # write operation average trMiB\n",
    "    write_trMiB = df[(df['taskName'] == task) & (df['operation'] == 0)]['trMiB'].mean()\n",
    "    print(f\"Avg write trMiB of task {task}: {write_trMiB:.2f}\")\n",
    "    \n",
    "    print(\"==========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add new columns for colors and labels based on the 'operation' column\n",
    "# df['color'] = df['operation'].map({0: 'blue', 1: 'red'})\n",
    "# df['label'] = df['operation'].map({0: 'write', 1: 'read'})\n",
    "# df['marker'] = df['operation'].map({0: 'x', 1: 'o'})  # 'o' for circle, 's' for square\n",
    "\n",
    "# # 2D plot: Relationship between aggregateFilesizeMB and totalTime\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# for label in df['label'].unique():\n",
    "#     subset = df[df['label'] == label]\n",
    "#     marker = subset['marker'].iloc[0]\n",
    "#     facecolor = 'none' if marker == 'o' else subset['color'].iloc[0]\n",
    "#     plt.scatter(subset['trMiB'], subset['opCount'], \n",
    "#                 label=label, \n",
    "#                 edgecolor=subset['color'].iloc[0],  # Edge color for circle markers\n",
    "#                 facecolors=facecolor,  # Only the circle markers will be hollow\n",
    "#                 marker=marker,\n",
    "#                 s=80,\n",
    "#                 alpha=0.5)\n",
    "\n",
    "# plt.title('Relationship between Transfer Size and Total Time')\n",
    "# plt.xlabel('I/O Bandwidth (MB/s)')\n",
    "# # plt.xlabel('Data size (MB)')\n",
    "# plt.ylabel('Operation count')\n",
    "# plt.grid(True)\n",
    "# plt.legend(title=\"Operation\")\n",
    "# plt.show()\n",
    "\n",
    "# Define mapping of operation and randomness to I/O type\n",
    "def map_io_type(row):\n",
    "    if row['operation'] == 0 and row['randomOffset'] == 0:\n",
    "        return 'sequential write'\n",
    "    elif row['operation'] == 0 and row['randomOffset'] == 1:\n",
    "        return 'random write'\n",
    "    elif row['operation'] == 1 and row['randomOffset'] == 0:\n",
    "        return 'sequential read'\n",
    "    elif row['operation'] == 1 and row['randomOffset'] == 1:\n",
    "        return 'random read'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply the mapping\n",
    "df['io_type'] = df.apply(map_io_type, axis=1)\n",
    "\n",
    "# Assign colors and markers to each I/O type\n",
    "io_color_map = {\n",
    "    'sequential write': 'blue',\n",
    "    'random write': 'purple',\n",
    "    'sequential read': 'green',\n",
    "    'random read': 'orange',\n",
    "}\n",
    "io_marker_map = {\n",
    "    'sequential write': 'x',\n",
    "    'random write': 'D',\n",
    "    'sequential read': 'o',\n",
    "    'random read': 's',\n",
    "}\n",
    "\n",
    "io_short_name_map = {\n",
    "    \"sequential write\": 'Seq W',\n",
    "    \"random write\": 'Rand W',\n",
    "    \"sequential read\": 'Seq R',\n",
    "    \"random read\": 'Rand R',\n",
    "}\n",
    "\n",
    "df['color'] = df['io_type'].map(io_color_map)\n",
    "df['marker'] = df['io_type'].map(io_marker_map)\n",
    "\n",
    "# 2D plot: Relationship between I/O Bandwidth and Operation Count\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for io_type in df['io_type'].unique():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    marker = subset['marker'].iloc[0]\n",
    "    facecolor = 'none' if marker == 'o' else subset['color'].iloc[0]\n",
    "    plt.scatter(\n",
    "        subset['trMiB'], \n",
    "        subset['opCount'], \n",
    "        label=io_type, \n",
    "        edgecolor=subset['color'].iloc[0], \n",
    "        facecolors=facecolor,\n",
    "        marker=marker,\n",
    "        s=80,\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "plt.title('Relationship between I/O Bandwidth and Operation Count')\n",
    "plt.xlabel('I/O Bandwidth (MB/s)')\n",
    "plt.ylabel('Operation Count')\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"I/O Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2D plot: x = opCount, y = trMiB, point size = aggregateFilesizeMB, color = io_type\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "size_scale = FLOW_SCALE\n",
    "\n",
    "# Plot actual data for present I/O types\n",
    "for io_type, color in io_color_map.items():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    \n",
    "    if not subset.empty:\n",
    "        sizes = np.log10(subset['aggregateFilesizeMB']) * size_scale  # Adjust as needed\n",
    "\n",
    "        ax.scatter(\n",
    "            subset['opCount'],\n",
    "            subset['trMiB'],\n",
    "            s=sizes,\n",
    "            label=io_short_name_map[io_type],\n",
    "            color=color,\n",
    "            marker='o',\n",
    "            alpha=0.3\n",
    "        )\n",
    "    else:\n",
    "        ax.scatter([], [], s=80, label=io_short_name_map[io_type], color=color, marker='o', alpha=0.3)\n",
    "\n",
    "# Font size settings\n",
    "plt.rc('font', size=20)\n",
    "plt.rc('axes', titlesize=22)\n",
    "plt.rc('axes', labelsize=22)\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rc('legend', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('Operation Count', labelpad=20)\n",
    "# ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "# # Set log scale for x-axis\n",
    "# ax.set_xscale('log')\n",
    "# ====== Create fixed-size legend entries for I/O types =======\n",
    "legend_handles = []\n",
    "for io_type, color in io_color_map.items():\n",
    "    label = io_short_name_map[io_type]\n",
    "    handle = plt.scatter([], [], s=200, color=color, label=label, alpha=0.3)  # Fixed size\n",
    "    legend_handles.append(handle)\n",
    "    \n",
    "# First legend: I/O types\n",
    "legend1 = ax.legend(\n",
    "    handles=legend_handles,\n",
    "    title=\"I/O Type\",\n",
    "    loc='center',\n",
    "    bbox_to_anchor=(0.36, 0.81, 0.8, 0.0),\n",
    "    framealpha=0.18,\n",
    "    facecolor='white'\n",
    ")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "# Second legend: Dataflow (MB)\n",
    "# Choose some representative values to show in the legend\n",
    "# Get non-zero aggregateFilesizeMB values\n",
    "flow_data = df['aggregateFilesizeMB'].dropna().sort_values()\n",
    "if not flow_data.empty:\n",
    "    min_val = flow_data.min()\n",
    "    if min_val == 0:\n",
    "        min_val = np.percentile(flow_data, 1)\n",
    "    max_val = flow_data.max()\n",
    "    mid1 = np.percentile(flow_data, 33)\n",
    "    mid2 = np.percentile(flow_data, 66)\n",
    "    flow_sizes = [int(mid1), int(max_val)]\n",
    "else:\n",
    "    flow_sizes = [10, 50, 100, 500]  # fallback\n",
    "    \n",
    "scatter_proxies = [plt.scatter([], [], s=np.log10(fs) * size_scale, color='gray', alpha=0.3) for fs in flow_sizes]\n",
    "labels = [f\"{fs} MB\" for fs in flow_sizes]\n",
    "\n",
    "legend2 = ax.legend(scatter_proxies, labels, title=\"Dataflow\", loc='center', \n",
    "                    bbox_to_anchor=(0.36, 0.4, 0.8, 0.0), framealpha=0.1, facecolor='white')\n",
    "ax.add_artist(legend2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f\"{result_path}/op_{plot_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2D plot: x = opCount, y = trMiB, point size = aggregateFilesizeMB, color = io_type\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "size_scale = FLOW_SCALE\n",
    "\n",
    "# Plot actual data for present I/O types\n",
    "for io_type, color in io_color_map.items():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    \n",
    "    if not subset.empty:\n",
    "        sizes = np.log10(subset['aggregateFilesizeMB']) * size_scale  # Adjust as needed\n",
    "\n",
    "        ax.scatter(\n",
    "            subset['stageOrder'],\n",
    "            subset['trMiB'],\n",
    "            s=sizes,\n",
    "            label=io_short_name_map[io_type],\n",
    "            color=color,\n",
    "            marker='o',\n",
    "            alpha=0.3\n",
    "        )\n",
    "    else:\n",
    "        ax.scatter([], [], s=80, label=io_short_name_map[io_type], color=color, marker='o', alpha=0.3)\n",
    "\n",
    "\n",
    "# Font size settings\n",
    "plt.rc('font', size=20)\n",
    "plt.rc('axes', titlesize=22)\n",
    "plt.rc('axes', labelsize=22)\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rc('legend', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('Stage Order', labelpad=20)\n",
    "# ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "# set x-axis ticks to be integer values\n",
    "ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "# # First legend: I/O types\n",
    "# legend1 = ax.legend(title=\"I/O Type\", loc='center', bbox_to_anchor=(0.1, 0.8, 0.8, 0.0), framealpha=0.1, facecolor='yellow')\n",
    "# ax.add_artist(legend1)\n",
    "\n",
    "# ====== Create fixed-size legend entries for I/O types =======\n",
    "legend_handles = []\n",
    "for io_type, color in io_color_map.items():\n",
    "    label = io_short_name_map[io_type]\n",
    "    handle = plt.scatter([], [], s=200, color=color, label=label, alpha=0.3)  # Fixed size\n",
    "    legend_handles.append(handle)\n",
    "    \n",
    "# First legend: I/O types\n",
    "legend1 = ax.legend(\n",
    "    handles=legend_handles,\n",
    "    title=\"I/O Type\",\n",
    "    loc='center',\n",
    "    bbox_to_anchor=(0.1, 0.6, 0.8, 0.0),\n",
    "    framealpha=0.1,\n",
    "    facecolor='white',\n",
    "    fontsize=20\n",
    ")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f\"{result_path}/stage_{plot_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d3_plots = True\n",
    "\n",
    "if d3_plots == True:\n",
    "        \n",
    "    # 3D plot: Relationship between aggregateFilesizeMB, stageOrder, and trMiB\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot for each defined I/O type, whether or not data exists\n",
    "    for io_type, color in io_color_map.items():\n",
    "        subset = df[df['io_type'] == io_type]\n",
    "        marker = io_marker_map[io_type]\n",
    "        facecolor = 'none' if marker == 'o' else color\n",
    "\n",
    "        if not subset.empty:\n",
    "            ax.scatter(\n",
    "                subset['stageOrder'],\n",
    "                subset['trMiB'],\n",
    "                subset['aggregateFilesizeMB'],\n",
    "                label=io_type,\n",
    "                edgecolor=color,\n",
    "                facecolors=facecolor,\n",
    "                marker=marker,\n",
    "                s=80,\n",
    "                alpha=0.5\n",
    "            )\n",
    "        else:\n",
    "            # Dummy point for legend\n",
    "            ax.scatter([], [], [],\n",
    "                    label=io_type,\n",
    "                    edgecolor=color,\n",
    "                    facecolors=facecolor,\n",
    "                    marker=marker,\n",
    "                    s=80,\n",
    "                    alpha=0.5)\n",
    "\n",
    "    # Font size settings\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=24)\n",
    "    plt.rc('axes', labelsize=24)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=22)\n",
    "\n",
    "    # Switched axes\n",
    "    ax.set_xlabel('Stage Order', labelpad=20)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=20)\n",
    "\n",
    "    ax.legend(title=\"I/O Type\", loc='best', bbox_to_anchor=(0.15, 0.6, 0.8, 0.0), framealpha=0.5, facecolor='white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(f\"{result_path}/stage_{plot_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build task ordering from CSV stage-order column (JSON order file removed)\n",
    "stage_col = 'stage_order' if 'stage_order' in df.columns else ('stageOrder' if 'stageOrder' in df.columns else None)\n",
    "if stage_col is not None:\n",
    "    stage_order_df = df[['taskName', stage_col]].dropna(subset=['taskName', stage_col]).copy()\n",
    "    stage_order_df[stage_col] = pd.to_numeric(stage_order_df[stage_col], errors='coerce')\n",
    "    stage_order_df = stage_order_df.dropna(subset=[stage_col]).drop_duplicates(subset=['taskName'], keep='first')\n",
    "    task_stage_order = stage_order_df.set_index('taskName')[stage_col].to_dict()\n",
    "    print(f\"Loaded stage order for {len(task_stage_order)} tasks from CSV column '{stage_col}': {csv_file_path}\")\n",
    "else:\n",
    "    task_stage_order = {}\n",
    "    print(\"Warning: no stage order column found in CSV ('stage_order' or 'stageOrder'). Falling back to default task ordering.\")\n",
    "\n",
    "size_scale = FLOW_SCALE\n",
    "\n",
    "# Ensure tasks are ordered based on stage order from CSV\n",
    "unique_tasks = sorted(\n",
    "    df['taskName'].dropna().unique(), \n",
    "    key=lambda t: task_stage_order.get(t, float('inf'))\n",
    ")\n",
    "\n",
    "print(f\"Unique tasks: {unique_tasks}\")\n",
    "if not unique_tasks:\n",
    "    print(\"No tasks found, skipping plotting.\")\n",
    "    exit()\n",
    "\n",
    "num_tasks = len(unique_tasks)\n",
    "palette = sns.color_palette(\"deep\", 9)  # Use 9 distinct colors\n",
    "marker_shapes = ['o', 's', 'D', '^', 'v', 'P', '*', 'X', '<', '>', 'h', 'H', '8', 'p', 'x', '+', '1', '2', '3', '4']\n",
    "\n",
    "task_group_count = 15\n",
    "task_groups = [unique_tasks[i:i+task_group_count] for i in range(0, num_tasks, task_group_count)]\n",
    "\n",
    "for group_idx, task_group in enumerate(task_groups):\n",
    "    valid_tasks = [task for task in task_group if task in df['taskName'].values]\n",
    "    print(f\"Group {group_idx+1}: {valid_tasks}\")\n",
    "    if not valid_tasks:\n",
    "        print(f\"Skipping empty group {group_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "    color_map = {task: palette[i % len(palette)] for i, task in enumerate(valid_tasks)}\n",
    "    marker_map = {task: marker_shapes[i % len(marker_shapes)] for i, task in enumerate(valid_tasks)}\n",
    "\n",
    "    for task in valid_tasks:\n",
    "        subset = df[df['taskName'] == task]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping task '{task}' (no data)\")\n",
    "            continue\n",
    "\n",
    "        sizes = np.log10(subset['aggregateFilesizeMB']) * size_scale\n",
    "\n",
    "        ax.scatter(\n",
    "            subset['opCount'],\n",
    "            subset['trMiB'],\n",
    "            s=sizes,\n",
    "            label=shorten_task_name(task),\n",
    "            color=color_map[task],\n",
    "            marker=marker_map[task],\n",
    "            alpha=0.3\n",
    "        )\n",
    "\n",
    "    # Font size and labels\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=22)\n",
    "    plt.rc('axes', labelsize=22)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=20)\n",
    "\n",
    "    ax.set_xlabel('Operation Count', labelpad=20)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "\n",
    "    # First legend: Task Name (color + shape)\n",
    "    task_legend_proxies = [\n",
    "        plt.scatter([], [], s=100, color=color_map[task], marker=marker_map[task], alpha=0.3, label=shorten_task_name(task))\n",
    "        for task in valid_tasks\n",
    "    ]\n",
    "    legend1 = ax.legend(handles=task_legend_proxies, title=\"Task Name\", loc='center', \n",
    "                        bbox_to_anchor=(0.36, 0.6, 0.8, 0.0), framealpha=0.1, facecolor='white')\n",
    "    ax.add_artist(legend1)\n",
    "\n",
    "    # Flow size stats if needed\n",
    "    flow_data = df['aggregateFilesizeMB'].dropna().sort_values()\n",
    "    if not flow_data.empty:\n",
    "        min_val = flow_data.min()\n",
    "        if min_val == 0:\n",
    "            min_val = np.percentile(flow_data, 1)\n",
    "        max_val = flow_data.max()\n",
    "        mid1 = np.percentile(flow_data, 33)\n",
    "        mid2 = np.percentile(flow_data, 66)\n",
    "        flow_sizes = [math.ceil(min_val), int(mid1), int(mid2), int(max_val)]\n",
    "    else:\n",
    "        flow_sizes = [10, 50, 100, 500]  # fallback\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(f'{result_path}/taskColor_group{group_idx+1}_' + plot_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load task ordering json file\n",
    "# task_order_dict = {}\n",
    "# with open(task_order_file) as f:\n",
    "#     task_order_dict = json.load(f)\n",
    "\n",
    "# print(task_order_dict)\n",
    "\n",
    "# # Ensure tasks are ordered based on task_order_dict\n",
    "# unique_tasks = sorted(\n",
    "#     df['taskName'].unique(), \n",
    "#     key=lambda t: task_order_dict.get(t, {}).get('stage_order', float('inf'))\n",
    "# )\n",
    "\n",
    "# # Debugging output\n",
    "# print(f\"Unique tasks: {unique_tasks}\")  \n",
    "# if not unique_tasks:\n",
    "#     print(\"No tasks found, skipping plotting.\")\n",
    "#     exit()\n",
    "\n",
    "# num_tasks = len(unique_tasks)\n",
    "\n",
    "# # Define color palette and markers\n",
    "# palette = sns.color_palette(\"deep\", num_tasks)\n",
    "# markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# # Split tasks into groups of 5\n",
    "# task_group_count = 15\n",
    "# task_groups = [unique_tasks[i:i+task_group_count] for i in range(0, num_tasks, task_group_count)]\n",
    "\n",
    "# for group_idx, task_group in enumerate(task_groups):\n",
    "\n",
    "\n",
    "#     # Filter out empty groups\n",
    "#     valid_tasks = [task for task in task_group if task in df['taskName'].values]\n",
    "#     print(f\"Group {group_idx+1}: {valid_tasks}\")\n",
    "#     if not valid_tasks:\n",
    "#         print(f\"Skipping empty group {group_idx+1}\")\n",
    "#         continue\n",
    "\n",
    "#     # 3D plot: Relationship between aggregateFilesizeMB, totalTime, and trMiB\n",
    "#     fig = plt.figure(figsize=(10, 8))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#     x = df['trMiB']\n",
    "#     y = df['opCount']\n",
    "#     z = df['aggregateFilesizeMB']\n",
    "#     # markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']  # Add more markers as needed\n",
    "\n",
    "#     # Generate color and marker maps for current group\n",
    "#     color_map = {task: palette[i % len(palette)] for i, task in enumerate(valid_tasks)}\n",
    "#     marker_map = {task: markers[i % len(markers)] for i, task in enumerate(valid_tasks)}\n",
    "\n",
    "#     df['color'] = df['taskName'].map(lambda task: color_map.get(task, 'black'))  # Black if not found\n",
    "#     df['marker'] = df['taskName'].map(lambda task: marker_map.get(task, 'o'))  # 'o' if not found\n",
    "\n",
    "#     for task in valid_tasks:\n",
    "#         subset = df[df['taskName'] == task]\n",
    "#         if subset.empty:\n",
    "#             print(f\"Skipping task '{task}' (no data)\")\n",
    "#             continue\n",
    "\n",
    "#         ax.scatter(subset['opCount'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "#                    label=shorten_task_name(task), \n",
    "#                    color=color_map[task], \n",
    "#                    marker=marker_map[task],\n",
    "#                    s=120)\n",
    "\n",
    "#     # Set labels and title\n",
    "#     plt.rc('font', size=20)\n",
    "#     plt.rc('axes', titlesize=24)\n",
    "#     plt.rc('axes', labelsize=24)\n",
    "#     plt.rc('xtick', labelsize=20)\n",
    "#     plt.rc('ytick', labelsize=20)\n",
    "#     plt.rc('legend', fontsize=16)\n",
    "    \n",
    "#     ax.set_xlabel('Operation Count', labelpad=20)\n",
    "#     ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "#     ax.set_zlabel('Dataflow (MB)', labelpad=20)\n",
    "\n",
    "#     ax.legend(title=\"Task Name\", loc=\"right\", bbox_to_anchor=(0.14, 0.72, 0.8, 0.0)) # (0, 0.83, 0.8, 0.0) (0, 0.6, 1.3, 0.0)\n",
    "#     fig.tight_layout()\n",
    "#     # plt.figure(constrained_layout=True)\n",
    "\n",
    "#     plt.show()\n",
    "#     fig.savefig(f'{result_path}/taskColor_group{group_idx+1}_'+plot_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "# def extract_group(filename, extension_only=False):\n",
    "#     if extension_only:\n",
    "#         # Only group by file extension\n",
    "#         ext = filename.rsplit('.', 1)[-1] if '.' in filename else ''\n",
    "#         return ext if ext else 'no_extension'\n",
    "\n",
    "#     if \"_\" in filename:\n",
    "#         # Keep the original method if the filename contains '_'\n",
    "#         for pattern in file_group_patterns:\n",
    "#             if re.match(pattern, filename):\n",
    "#                 base, ext = filename.rsplit('.', 1) if '.' in filename else (filename, '')\n",
    "#                 prefix = re.split(r'\\d', base, 1)[0].rstrip('_')  # Extract text before first number\n",
    "#                 return f\"{prefix}.{ext}\" if ext else prefix  # Include file extension\n",
    "#         return \"other\"  # Default group\n",
    "\n",
    "#     elif any(char.isdigit() for char in filename):\n",
    "#         # If the filename contains a number, split before the first number and keep the extension\n",
    "#         base, ext = filename.rsplit('.', 1) if '.' in filename else (filename, '')\n",
    "#         prefix = re.split(r'\\d', base, 1)[0]\n",
    "#         return f\"{prefix}.{ext}\" if ext else prefix\n",
    "\n",
    "#     else:\n",
    "#         # If no underscore and no numbers, return the filename itself as the group\n",
    "#         return filename\n",
    "\n",
    "# # Add a new column to categorize file groups\n",
    "# df[\"file_group\"] = df[\"fileName\"].apply(extract_group, extension_only=extension_grouping)\n",
    "\n",
    "# # Check the unique file groups\n",
    "# print(f'Unique file groups: {df[\"file_group\"].unique()}')\n",
    "\n",
    "# Compile regex patterns once\n",
    "compiled_patterns = [(pattern, re.compile(pattern)) for pattern in file_group_patterns]\n",
    "\n",
    "def extract_group(filename):\n",
    "    for pattern_str, pattern in compiled_patterns:\n",
    "        if pattern.search(filename):\n",
    "            return pattern_str.replace(\"\\\\\", \"\")  # Remove backslashes\n",
    "    return \"group_other\"\n",
    "\n",
    "\n",
    "# Apply to dataframe\n",
    "df[\"file_group\"] = df[\"fileName\"].apply(extract_group)\n",
    "\n",
    "# Check results\n",
    "print(\"Unique file groups:\")\n",
    "print(df[\"file_group\"].value_counts())\n",
    "\n",
    "def padded_range(series, pad_ratio=0.05):\n",
    "    \"\"\"Return min and max with padding based on data range.\"\"\"\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    pad = (max_val - min_val) * pad_ratio\n",
    "    return min_val - pad, max_val + pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # get the number of read I/O entries with taskName mapfeature group by file_group\n",
    "num_read_entries = df[(df['operation'] == 1) & (df['taskName'] == 'mapf')]\n",
    "num_read_entries_per_file = num_read_entries.groupby('file_group').size().reset_index(name='count')\n",
    "num_read_entries_per_file = num_read_entries_per_file.sort_values(by='count', ascending=False)\n",
    "# get the percent of read I/O entries with taskName mapfeature group by file_group\n",
    "num_read_entries_per_file['percent'] = num_read_entries_per_file['count'] / sum(num_read_entries_per_file['count']) * 100\n",
    "# get the average read I/O bandwidth with taskName mapfeature group by file_group\n",
    "num_read_entries_per_file['avg_read_bw'] = num_read_entries.groupby('file_group')['trMiB'].mean().values\n",
    "# get the aggregateFilesizeMB with taskName mapfeature group by file_group\n",
    "num_read_entries_per_file['aggregateFilesizeMB'] = num_read_entries.groupby('file_group')['aggregateFilesizeMB'].sum().values\n",
    "\n",
    "\n",
    "print(num_read_entries_per_file)\n",
    "print(sum(num_read_entries_per_file['aggregateFilesizeMB']))\n",
    "print(num_read_entries_per_file['avg_read_bw'].mean())\n",
    "\n",
    "# do the same for taskName idfea write I/O entries\n",
    "num_idfea_entries = df[(df['operation'] == 0) & (df['taskName'] == 'idfea')]\n",
    "num_idfea_entries_per_file = num_idfea_entries.groupby('file_group').size().reset_index(name='count')\n",
    "num_idfea_entries_per_file = num_idfea_entries_per_file.sort_values(by='count', ascending=False)\n",
    "# get the percent of read I/O entries with taskName idfea group by file_group\n",
    "num_idfea_entries_per_file['percent'] = num_idfea_entries_per_file['count'] / sum(num_idfea_entries_per_file['count']) * 100\n",
    "# get the average read I/O bandwidth with taskName idfea group by file_group\n",
    "num_idfea_entries_per_file['avg_read_bw'] = num_idfea_entries.groupby('file_group')['trMiB'].mean().values\n",
    "# get the aggregateFilesizeMB with taskName idfea group by file_group\n",
    "num_idfea_entries_per_file['aggregateFilesizeMB'] = num_idfea_entries.groupby('file_group')['aggregateFilesizeMB'].sum().values\n",
    "print(num_idfea_entries_per_file)\n",
    "print(sum(num_idfea_entries_per_file['aggregateFilesizeMB']))\n",
    "print(num_idfea_entries_per_file['avg_read_bw'].mean())\n",
    "\n",
    "# get taskName with lowest trMiB\n",
    "min_trMiB = df['trMiB'].min()\n",
    "min_trMiB_task = df[df['trMiB'] == min_trMiB]['taskName'].values[0]\n",
    "print(f\"Task with lowest trMiB: {min_trMiB_task} ({min_trMiB:.2f} MiB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a color palette\n",
    "unique_groups = df[\"file_group\"].unique()\n",
    "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "# Exclude certain groups (e.g., for 1000 Genome)\n",
    "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "num_groups = len(unique_groups)\n",
    "palette = sns.color_palette(\"deep\", 9)\n",
    "\n",
    "# List of distinct marker styles\n",
    "# marker_shapes = ['o', 's', 'D', '^', 'v', 'P', '*', 'X', '<', '>', 'h', 'H', '8', 'p', 'x', '+', '1', '2', '3', '4']\n",
    "\n",
    "marker_shapes = ['o']\n",
    "\n",
    "\n",
    "file_group_count = 20\n",
    "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, len(unique_groups), file_group_count)]\n",
    "\n",
    "size_scale = FLOW_SCALE\n",
    "\n",
    "for batch_idx, group_batch in enumerate(group_batches):\n",
    "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "    if not valid_groups:\n",
    "        print(f\"Skipping empty group {batch_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    \n",
    "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "    marker_map = {group: marker_shapes[i % len(marker_shapes)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "    for group in valid_groups:\n",
    "        subset = df[df[\"file_group\"] == group]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping group '{group}' (no data)\")\n",
    "            continue\n",
    "\n",
    "        sizes = np.log10(subset[\"aggregateFilesizeMB\"]) * size_scale\n",
    "\n",
    "        ax.scatter(\n",
    "            subset[\"opCount\"],\n",
    "            subset[\"trMiB\"],\n",
    "            s=sizes,\n",
    "            label=group,\n",
    "            color=color_map[group],\n",
    "            marker=marker_map[group],\n",
    "            alpha=0.3\n",
    "        )\n",
    "\n",
    "    # Font settings\n",
    "    plt.rc('font', size=22)\n",
    "    plt.rc('axes', titlesize=22)\n",
    "    plt.rc('axes', labelsize=22)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=20)\n",
    "\n",
    "    ax.set_xlabel(\"Operation Count\", labelpad=20)\n",
    "    ax.set_ylabel(\"I/O Bandwidth (MB/s)\", labelpad=20)\n",
    "    \n",
    "    # First legend: File Group (custom markers and colors, fixed size)\n",
    "    scatter_proxies = [\n",
    "        plt.Line2D(\n",
    "            [], [], marker=marker_map[group], color='w',\n",
    "            markerfacecolor=color_map[group], markersize=15, alpha=0.6, \n",
    "            label=group.replace(\"_.*\", \"\").replace(\".*\", \"\").replace(\".nc\",\"\").replace(\"^\",\"\")\n",
    "            )\n",
    "        for group in valid_groups\n",
    "    ]\n",
    "\n",
    "    legend1 = ax.legend(\n",
    "        handles=scatter_proxies,\n",
    "        title=\"File Group\",\n",
    "        loc='center',\n",
    "        bbox_to_anchor=(0.4, 0.64, 0.8, 0.0),\n",
    "        framealpha=0.1,\n",
    "        facecolor='white'\n",
    "    )\n",
    "    ax.add_artist(legend1)\n",
    "\n",
    "    # Optional: Flow size summary (unchanged)\n",
    "    flow_data = df[\"aggregateFilesizeMB\"].dropna().sort_values()\n",
    "    if not flow_data.empty:\n",
    "        min_val = flow_data.min()\n",
    "        if min_val == 0:\n",
    "            min_val = np.percentile(flow_data, 1)\n",
    "        max_val = flow_data.max()\n",
    "        mid1 = np.percentile(flow_data, 33)\n",
    "        mid2 = np.percentile(flow_data, 66)\n",
    "        flow_sizes = [round(min_val, 2), round(mid1, 2), round(mid2, 2), round(max_val, 2)]\n",
    "    else:\n",
    "        flow_sizes = [10, 50, 100, 500]\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{result_path}/fileColor_group{batch_idx+1}_op_' + plot_file_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a color palette\n",
    "unique_groups = df[\"file_group\"].unique()\n",
    "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "# Exclude certain groups (e.g., for 1000 Genome)\n",
    "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "num_groups = len(unique_groups)\n",
    "palette = sns.color_palette(\"deep\", 9)  # Limit to 9 distinct colors\n",
    "\n",
    "# Define a set of marker shapes\n",
    "marker_shapes = ['o', 's', 'D', '^', 'v', 'P', '*', 'X', '<', '>', 'h', 'H', '8', 'p', 'x', '+', '1', '2', '3', '4']\n",
    "\n",
    "# Group batches\n",
    "file_group_count = 20\n",
    "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, len(unique_groups), file_group_count)]\n",
    "\n",
    "size_scale = FLOW_SCALE\n",
    "\n",
    "for batch_idx, group_batch in enumerate(group_batches):\n",
    "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "    if not valid_groups:\n",
    "        print(f\"Skipping empty group {batch_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "    marker_map = {group: marker_shapes[i % len(marker_shapes)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "    for group in valid_groups:\n",
    "        subset = df[df[\"file_group\"] == group]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping group '{group}' (no data)\")\n",
    "            continue\n",
    "\n",
    "        sizes = np.log10(subset[\"aggregateFilesizeMB\"]) * size_scale\n",
    "\n",
    "        ax.scatter(\n",
    "            subset[\"stageOrder\"],\n",
    "            subset[\"trMiB\"],\n",
    "            s=sizes,\n",
    "            label=group,\n",
    "            color=color_map[group],\n",
    "            marker=marker_map[group],\n",
    "            alpha=0.3\n",
    "        )\n",
    "\n",
    "    # Font settings\n",
    "    plt.rc('font', size=22)\n",
    "    plt.rc('axes', titlesize=22)\n",
    "    plt.rc('axes', labelsize=22)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=22)\n",
    "\n",
    "    ax.set_xlabel(\"Stage Order\", labelpad=20)\n",
    "\n",
    "    # First legend: File Group with fixed-size dummy points using same color and marker\n",
    "    scatter_proxies = [\n",
    "        plt.scatter([], [], s=100, color=color_map[group], marker=marker_map[group], alpha=0.3, \n",
    "                    label=group.replace(\"_.*\", \"\").replace(\".*\", \"\").replace(\".nc\",\"\").replace(\"^\",\"\"))\n",
    "        for group in valid_groups\n",
    "    ]\n",
    "    legend1 = ax.legend(handles=scatter_proxies, title=\"File Group\", loc='center',\n",
    "                        bbox_to_anchor=(0.05, 0.6, 0.8, 0.0), framealpha=0.2, facecolor='white',\n",
    "                        labelspacing=0.1,\n",
    "                        )\n",
    "    ax.add_artist(legend1)\n",
    "\n",
    "    # Optional: Flow size stats if needed\n",
    "    flow_data = df[\"aggregateFilesizeMB\"].dropna().sort_values()\n",
    "    if not flow_data.empty:\n",
    "        min_val = flow_data.min()\n",
    "        if min_val == 0:\n",
    "            min_val = np.percentile(flow_data, 1)\n",
    "        max_val = flow_data.max()\n",
    "        mid1 = np.percentile(flow_data, 33)\n",
    "        mid2 = np.percentile(flow_data, 66)\n",
    "        flow_sizes = [math.ceil(min_val), round(mid1, 2), round(mid2, 2), round(max_val, 2)]\n",
    "    else:\n",
    "        flow_sizes = [10, 50, 100, 500]\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{result_path}/fileColor_group{batch_idx+1}_stage_' + plot_file_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if d3_plots == True:\n",
    "    # Generate a color palette\n",
    "    unique_groups = df[\"file_group\"].unique()\n",
    "    print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "    num_groups = len(unique_groups)\n",
    "\n",
    "    # Define color palette and markers\n",
    "    # Define color palette and markers\n",
    "    palette = sns.color_palette(\"deep\", num_groups)\n",
    "    markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "    # Groups to exclude for 1000 genome\n",
    "    exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "    # Remove them from unique_groups\n",
    "    unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "    # Split file groups into groups of n\n",
    "    file_group_count = 20\n",
    "    group_batches = [unique_groups[i:i+file_group_count] for i in range(0, num_groups, file_group_count)]\n",
    "\n",
    "    for batch_idx, group_batch in enumerate(group_batches):\n",
    "        # Skip empty groups\n",
    "        valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "        if not valid_groups:\n",
    "            print(f\"Skipping empty group {batch_idx+1}\")\n",
    "            continue\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Generate color and marker maps for current group\n",
    "        color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "        marker_map = {group: markers[i % len(markers)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "        # Assign colors and markers\n",
    "        df[\"color\"] = df[\"file_group\"].map(lambda group: color_map.get(group, 'black'))\n",
    "        df[\"marker\"] = df[\"file_group\"].map(lambda group: marker_map.get(group, 'o'))\n",
    "\n",
    "        # Iterate through the valid file groups\n",
    "        for group in valid_groups:\n",
    "            subset = df[df[\"file_group\"] == group]\n",
    "            if subset.empty:\n",
    "                print(f\"Skipping group '{group}' (no data)\")\n",
    "                continue\n",
    "\n",
    "            # ax.scatter(subset['trMiB'], subset['stageOrder'], subset['aggregateFilesizeMB'], \n",
    "            ax.scatter(subset['stageOrder'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "                    label=group, \n",
    "                    color=color_map[group],  \n",
    "                    marker=marker_map[group],  \n",
    "                    s=100)\n",
    "        \n",
    "        # Set font sizes\n",
    "        plt.rc('font', size=22)\n",
    "        plt.rc('axes', titlesize=24)\n",
    "        plt.rc('axes', labelsize=24)\n",
    "        plt.rc('xtick', labelsize=20)\n",
    "        plt.rc('ytick', labelsize=20)\n",
    "        plt.rc('legend', fontsize=20)\n",
    "\n",
    "        # Set axis labels\n",
    "        ax.set_xlabel('Stage Order', labelpad=20)\n",
    "        ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "        ax.set_zlabel('Dataflow (MB)', labelpad=20)\n",
    "        \n",
    "        # Ensure y-axis only shows integers\n",
    "        ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "        # Legend updated to reflect file groups\n",
    "        ax.legend(title=\"File Group\", loc=\"best\", bbox_to_anchor=(0.16, 0.5, 0.8, 0.0), labelspacing=0.1, framealpha=0.3, facecolor='white')\n",
    "\n",
    "        # plt.figure(constrained_layout=True)\n",
    "        # fig.tight_layout()\n",
    "        # Show and save figure\n",
    "        fig.savefig(f'{result_path}/fileColor_group{batch_idx+1}_stage_'+plot_file_name)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print if any value is nan\n",
    "print(df['stageOrder'].isna().any())\n",
    "df['stageOrder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split into read and write I/O operations\n",
    "# df_reads = df[df['operation'] == 1].copy()\n",
    "# df_writes = df[df['operation'] == 0].copy()\n",
    "\n",
    "# # Merge based on matching fileName and task linkage (write.taskName == read.prevTask)\n",
    "# df_pc = pd.merge(\n",
    "#     df_reads,\n",
    "#     df_writes,\n",
    "#     left_on=['fileName', 'prevTask'],\n",
    "#     right_on=['fileName', 'taskName'],\n",
    "#     suffixes=('_read', '_write')\n",
    "# )\n",
    "\n",
    "# print(f\"Producer-Consumer pairs: {df_pc.shape[0]}\")\n",
    "\n",
    "# # Ensure required columns exist\n",
    "# required_cols = [\n",
    "#     'transferSize_read', 'transferSize_write',\n",
    "#     'aggregateFilesizeMB_read', 'aggregateFilesizeMB_write',\n",
    "#     'totalTime_read', 'totalTime_write',\n",
    "#     'opCount_read', 'opCount_write',\n",
    "#     'trMiB_read', 'trMiB_write'\n",
    "# ]\n",
    "\n",
    "# # Compute aggregates and average\n",
    "# df_pc['transferSize'] = df_pc['transferSize_read'] + df_pc['transferSize_write']\n",
    "# df_pc['aggregateFilesizeMB'] = df_pc['aggregateFilesizeMB_read'] + df_pc['aggregateFilesizeMB_write']\n",
    "# df_pc['totalTime'] = df_pc['totalTime_read'] + df_pc['totalTime_write']\n",
    "# df_pc['opCount'] = df_pc['opCount_read'] + df_pc['opCount_write']\n",
    "# df_pc['trMiB'] = (df_pc['trMiB_read'] + df_pc['trMiB_write']) / 2\n",
    "\n",
    "# # Create simplified producer-consumer DataFrame\n",
    "# df_producer_consumer = df_pc[[\n",
    "#     'taskName_write', 'taskName_read', 'fileName',\n",
    "#     'transferSize', 'aggregateFilesizeMB', 'totalTime', 'opCount', 'trMiB',\n",
    "#     'stageOrder_write', 'stageOrder_read'\n",
    "# ]].rename(columns={\n",
    "#     'taskName_write': 'producerTask',\n",
    "#     'taskName_read': 'consumerTask'\n",
    "# })\n",
    "\n",
    "manual_pairs = {\n",
    "    (\"idfea\", \"trksg\") : \"n-n\",\n",
    "    (\"trksg\", \"gettr\") : \"n-1\",\n",
    "    (\"idfea\", \"gettr\") : \"n-1\",\n",
    "    (\"gettr\", \"trkst\"): \"1-n\",\n",
    "    (\"idfea\", \"trkst\"): \"1-1\",\n",
    "    (\"trkst\", \"idmcs\"): \"1-1\",\n",
    "    (\"idmcs\", \"mtpf\"): \"1-1\",\n",
    "    (\"mtpf\", \"rmcs\"): \"1-1\",\n",
    "    (\"rmcs\", \"mapf\"): \"1-1\",\n",
    "    (\"idfea\", \"mapf\"): \"n-n\",\n",
    "    (\"mapf\", \"speed\"): \"n-n\",\n",
    "    (\"rmcs\", \"speed\"): \"1-1\",\n",
    "    (\"initial\", \"idfea\"): \"1-1\",\n",
    "    (\"initial\", \"mapf\"): \"1-n\",\n",
    "    (\"speed\", \"final\"): \"1-1\",\n",
    "    (\"trkst\", \"final\"): \"1-1\",\n",
    "}\n",
    "\n",
    "# Combine based on your manual producer-consumer task pairs\n",
    "rows = []\n",
    "for producer, consumer in manual_pairs.keys():\n",
    "    pc_label = producer + '' + consumer\n",
    "    \n",
    "    if consumer == \"final\":\n",
    "        df_subset_write = df[(df['operation'] == 0) & (df['taskName'] == producer) ]\n",
    "        if df_subset_write.empty:\n",
    "            print(f\"Skipping special pair ({pc_label}): no data\")\n",
    "            continue\n",
    "        \n",
    "        if df_subset_write['parallelism'].mean() > 1:\n",
    "            subset_write_bw = df_subset_write['trMiB'].sum() if not df_subset_write.empty else 0\n",
    "        else:\n",
    "            subset_write_bw = df_subset_write['trMiB'].mean() if not df_subset_write.empty else 0\n",
    "        \n",
    "        consumer_file_group = df_subset_pc_read['file_group'].unique()\n",
    "\n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': df_subset_write['parallelism'].mean(),\n",
    "            'consumer_parallelism': 0,\n",
    "            'transferSize': df_subset_write['transferSize'].sum(),\n",
    "            'aggregateFilesizeMB': df_subset_write['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_write['totalTime'].sum(),\n",
    "            'opCount': df_subset_write['opCount'].sum(),\n",
    "            'trMiB': subset_write_bw,\n",
    "            'stageOrder_read': int(df_subset_write['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_write['stageOrder'].mean()),\n",
    "            'pcPair': pc_label,\n",
    "            'file_groups': consumer_file_group,\n",
    "            'label': f\"{producer}{consumer_file_group}{consumer}\",\n",
    "        }\n",
    "        rows.append(row)\n",
    "        continue\n",
    "    if producer == \"initial\":        \n",
    "        df_subset_read = df[(df['operation'] == 1) & (df['taskName'] == consumer) & (df['prevTask'] == producer)]\n",
    "        if df_subset_read.empty:\n",
    "            print(f\"Skipping special pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            # print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_read: {df_subset_read[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "        \n",
    "        if df_subset_read['parallelism'].mean() > 1:\n",
    "            subset_read_bw = df_subset_read['trMiB'].sum() if not df_subset_read.empty else 0\n",
    "        else:\n",
    "            subset_read_bw = df_subset_read['trMiB'].mean() if not df_subset_read.empty else 0\n",
    "        \n",
    "        consumer_file_group = df_subset_read['file_group'].unique()\n",
    "        if consumer == \"indiv\":\n",
    "            # set filegroup to \"^chr.*-.*.tar.gz\"\"\n",
    "            consumer_file_group = [\"^chr.*-.*.tar.gz\"]\n",
    "            \n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': 0,\n",
    "            'consumer_parallelism': df_subset_read['parallelism'].mean(),\n",
    "            'transferSize': df_subset_read['transferSize'].sum(),\n",
    "            'aggregateFilesizeMB': df_subset_read['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_read['totalTime'].sum(),\n",
    "            'opCount': df_subset_read['opCount'].sum(),\n",
    "            'trMiB': subset_read_bw,\n",
    "            'stageOrder_read': int(df_subset_read['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_read['stageOrder'].mean()),\n",
    "            'pcPair': pc_label,\n",
    "            'file_groups': consumer_file_group,\n",
    "            'label': f\"{producer}{consumer_file_group}{consumer}\",\n",
    "        }\n",
    "        rows.append(row)\n",
    "        continue\n",
    "    else:\n",
    "        # Regular handling\n",
    "        df_subset_pc_read = df[(df['operation'] == 1) & (df['taskName'] == consumer) & (df['prevTask'] == producer)]\n",
    "        # check if any of the dataframes are empty\n",
    "        if df_subset_pc_read.empty:\n",
    "            print(f\"Skipping pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_pc: {df_subset_pc_read[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        # get list of unique file names\n",
    "        consumer_files = df_subset_pc_read['fileName'].unique()\n",
    "        consumer_file_group = df_subset_pc_read['file_group'].unique()\n",
    "        # get the dataframe with the same file names\n",
    "        df_subset_pc_write = df[(df['operation'] == 0) & (df['taskName'] == producer) & (df['fileName'].isin(consumer_files))]\n",
    "        \n",
    "        if df_subset_pc_write.empty:\n",
    "            print(f\"Skipping pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_pc: {df_subset_pc_write[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        producer_parallelism = df_subset_pc_write['parallelism'].mean()\n",
    "        consumer_parallelism = df_subset_pc_read['parallelism'].mean()\n",
    "        # print(f\"Producer: {producer}, Consumer: {consumer}, Producer Parallelism: {producer_parallelism}, Consumer Parallelism: {consumer_parallelism}\")\n",
    "        \n",
    "        if producer_parallelism > 1:\n",
    "            # If parallelism > 1, use sum\n",
    "            subset_write_bw = df_subset_pc_write['trMiB'].sum() if not df_subset_pc_write.empty else 0\n",
    "        else:\n",
    "            # If parallelism <= 1, use mean\n",
    "            subset_write_bw = df_subset_pc_write['trMiB'].mean() if not df_subset_pc_write.empty else 0\n",
    "        \n",
    "        if consumer_parallelism > 1:\n",
    "            # If parallelism > 1, use sum\n",
    "            subset_read_bw = df_subset_pc_read['trMiB'].sum() if not df_subset_pc_read.empty else 0\n",
    "        else:\n",
    "            # If parallelism <= 1, use mean\n",
    "            subset_read_bw = df_subset_pc_read['trMiB'].mean() if not df_subset_pc_read.empty else 0\n",
    "\n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': producer_parallelism,\n",
    "            'consumer_parallelism': consumer_parallelism,\n",
    "            'transferSize': df_subset_pc_write['transferSize'].mean() + df_subset_pc_read['transferSize'].mean(),\n",
    "            'aggregateFilesizeMB': df_subset_pc_write['aggregateFilesizeMB'].sum() + df_subset_pc_read['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_pc_write['totalTime'].sum() + df_subset_pc_read['totalTime'].sum(),\n",
    "            'opCount': df_subset_pc_write['opCount'].sum() + df_subset_pc_read['opCount'].sum(),\n",
    "            'trMiB': (subset_write_bw + subset_read_bw) / 2,\n",
    "            'stageOrder_read': int(df_subset_pc_read['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_pc_write['stageOrder'].mean()),\n",
    "            'pcPair': pc_label,\n",
    "            'file_groups': consumer_file_group, # list of unique file groups\n",
    "            'label': f\"{producer}{consumer_file_group}{consumer}\",\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df_pc = pd.DataFrame(rows)\n",
    "\n",
    "# Show summary\n",
    "print(f\"Constructed producer-consumer DataFrame with {len(df_pc)} pairs.\")\n",
    "print(df_pc[['pcPair', 'aggregateFilesizeMB', 'totalTime', 'trMiB', 'producer_parallelism', 'consumer_parallelism']])\n",
    "\n",
    "# Optional: create pair label\n",
    "df_pc['pcPair'] = df_pc['producerTask'] + '' + df_pc['consumerTask']\n",
    "\n",
    "# Get unique producer-consumer pairs\n",
    "unique_pairs = df_pc['pcPair'].unique()\n",
    "if len(unique_pairs) == 0:\n",
    "    print(\"No producer-consumer pairs available for plotting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Unique producer-consumer pairs: {unique_pairs}\")\n",
    "\n",
    "print(df_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== aggregateFilesizeMB Range ===\")\n",
    "print(f\"df_pc: min = {df_pc['aggregateFilesizeMB'].min():.2f}, max = {df_pc['aggregateFilesizeMB'].max():.2f}\")\n",
    "print(f\"df   : min = {df['aggregateFilesizeMB'].min():.2f}, max = {df['aggregateFilesizeMB'].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "palette = sns.color_palette(\"tab20\", len(unique_pairs))\n",
    "pair_group_count = 20\n",
    "pair_groups = [unique_pairs[i:i + pair_group_count] for i in range(0, len(unique_pairs), pair_group_count)]\n",
    "\n",
    "size_scale = FLOW_SCALE  # Control how large the bubbles appear\n",
    "\n",
    "\n",
    "# Add marker styles\n",
    "marker_styles = ['o', 's', '^', 'v', 'D', 'P', '*', 'X', 'h', '<', '>', 'p', 'H', '|', '_']\n",
    "marker_map = {}\n",
    "\n",
    "for group_idx, pair_group in enumerate(pair_groups):\n",
    "    subset_group = df_pc[df_pc['pcPair'].isin(pair_group)]\n",
    "    if subset_group.empty:\n",
    "        print(f\"Skipping empty group {group_idx + 1}\")\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    color_map = {pair: palette[i % len(palette)] for i, pair in enumerate(pair_group)}\n",
    "    marker_map = {pair: marker_styles[i % len(marker_styles)] for i, pair in enumerate(pair_group)}\n",
    "    seen_labels = set()\n",
    "\n",
    "    for _, row in subset_group.iterrows():\n",
    "        pair_label = row['pcPair']\n",
    "        producer, consumer = pair_label.split('')\n",
    "        short_producer = shorten_task_name(producer)\n",
    "        short_consumer = shorten_task_name(consumer)\n",
    "\n",
    "        pattern = manual_pairs.get((producer, consumer), \"unknown\")\n",
    "        producer_parallelism = int(row['producer_parallelism']) if pd.notna(row['producer_parallelism']) else 1\n",
    "        consumer_parallelism = int(row['consumer_parallelism']) if pd.notna(row['consumer_parallelism']) else 1\n",
    "        if \"gettr\" in producer:\n",
    "            producer_parallelism = 1\n",
    "        if \"gettr\" in consumer:\n",
    "            consumer_parallelism = 1\n",
    "\n",
    "        short_label = f\"{short_producer}({producer_parallelism})->[\"\n",
    "        for file_group in row['file_groups']:   \n",
    "            short_label += f\"{file_group},\".replace(\"_.*\", \"\").replace(\".*\",\"\")\n",
    "        # replace the last char with ]\n",
    "        short_label = short_label[:-1] + \"]\"\n",
    "        short_label += f\"{short_consumer}({consumer_parallelism}) [{pattern}]\"\n",
    "        \n",
    "        show_label = pair_label not in seen_labels\n",
    "        seen_labels.add(pair_label)\n",
    "\n",
    "        trMiB = row['trMiB'] if pd.notna(row['trMiB']) else 0\n",
    "        opCount = row['opCount'] if pd.notna(row['opCount']) else 0\n",
    "        filesize = row['aggregateFilesizeMB'] if pd.notna(row['aggregateFilesizeMB']) else 0\n",
    "\n",
    "        ax.scatter(\n",
    "            opCount,\n",
    "            trMiB,\n",
    "            s=np.log10(filesize) * size_scale,\n",
    "            color=color_map[pair_label],\n",
    "            label=short_label if show_label else None,\n",
    "            alpha=0.4,\n",
    "            marker=marker_map[pair_label]\n",
    "        )\n",
    "\n",
    "    # Styling\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 24,\n",
    "        'axes.titlesize': 20,\n",
    "        'axes.labelsize': 20,\n",
    "        'xtick.labelsize': 20,\n",
    "        'ytick.labelsize': 20,\n",
    "        'legend.fontsize': 17\n",
    "    })\n",
    "\n",
    "    ax.set_xlabel('Operation Count', labelpad=23)\n",
    "    # set x axis limit\n",
    "    ax.set_xlim(left=0, right=df_pc['opCount'].max() * 2)\n",
    "\n",
    "    # Create legend with markers\n",
    "    handles = []\n",
    "    labels = []\n",
    "    for pair_label in seen_labels:\n",
    "        producer, consumer = pair_label.split('')\n",
    "        short_producer = shorten_task_name(producer)\n",
    "        short_consumer = shorten_task_name(consumer)\n",
    "        pattern = manual_pairs.get((producer, consumer), \"unknown\")\n",
    "        producer_parallelism = df_pc.loc[df_pc['pcPair'] == pair_label, 'producer_parallelism'].iloc[0]\n",
    "        consumer_parallelism = df_pc.loc[df_pc['pcPair'] == pair_label, 'consumer_parallelism'].iloc[0]\n",
    "        short_label = f\"{short_producer}({int(producer_parallelism)}){short_consumer}({int(consumer_parallelism)}) [{pattern}]\"\n",
    "\n",
    "        patch = plt.scatter([], [], s=200, color=color_map[pair_label], marker=marker_map[pair_label], alpha=0.4, label=short_label)\n",
    "        handles.append(patch)\n",
    "        labels.append(short_label)\n",
    "\n",
    "    # Split long legends\n",
    "    legend1 = ax.legend(handles[:5], labels[:5], \n",
    "                        title=\"Producer  Consumer\", loc=\"upper left\", \n",
    "                        bbox_to_anchor=(0.45, 0.9), framealpha=0.2, facecolor='yellow')\n",
    "    ax.add_artist(legend1)\n",
    "\n",
    "    if len(handles) > 5:\n",
    "        legend2 = ax.legend(handles[5:5+5], labels[5:5+5], loc=\"upper left\", \n",
    "                            bbox_to_anchor=(0.1, 0.45), framealpha=0.2, facecolor='yellow')\n",
    "        ax.add_artist(legend2)\n",
    "    if len(handles) > 10:\n",
    "        legend3 = ax.legend(handles[10:], labels[10:], loc=\"upper left\", \n",
    "                            bbox_to_anchor=(0.55, 0.45), framealpha=0.1, facecolor='yellow')\n",
    "        ax.add_artist(legend3)\n",
    "\n",
    "\n",
    "    # Show the plot properly\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "    # Save figure with bounding box that includes legend\n",
    "    fig.savefig(f\"{result_path}/pcAgg_group{group_idx + 1}_{plot_file_name}\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "def sci_notation(x, pos):\n",
    "    return f'{x:.0e}'\n",
    "\n",
    "\n",
    "# Plot settings\n",
    "palette = sns.color_palette(\"tab20\", len(unique_pairs))\n",
    "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h', 'v', '<', '>', '|']\n",
    "pair_group_count = 15\n",
    "pair_groups = [unique_pairs[i:i+pair_group_count] for i in range(0, len(unique_pairs), pair_group_count)]\n",
    "\n",
    "for group_idx, pair_group in enumerate(pair_groups):\n",
    "    subset_group = df_pc[df_pc['pcPair'].isin(pair_group)]\n",
    "    if subset_group.empty:\n",
    "        print(f\"Skipping empty group {group_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Assign colors and markers\n",
    "    color_map = {pair: palette[i % len(palette)] for i, pair in enumerate(pair_group)}\n",
    "    marker_map = {pair: markers[i % len(markers)] for i, pair in enumerate(pair_group)}\n",
    "\n",
    "    seen_labels = set()  # Track labels we've already added\n",
    "\n",
    "    for _, row in subset_group.iterrows():\n",
    "        pair_label = row['pcPair']\n",
    "        producer, consumer = pair_label.split('')\n",
    "        short_producer = shorten_task_name(producer)\n",
    "        short_consumer = shorten_task_name(consumer)\n",
    "\n",
    "        # Fetch pattern from manual_pairs\n",
    "        pattern = manual_pairs.get((producer, consumer), \"unknown\")\n",
    "        producer_parallelism = int(row['producer_parallelism'])\n",
    "        consumer_parallelism = int(row['consumer_parallelism'])\n",
    "        if \"gettr\" in producer:\n",
    "            producer_parallelism = 1\n",
    "        if \"gettr\" in consumer:\n",
    "            consumer_parallelism = 1\n",
    "            \n",
    "        # short_label = f\"{short_producer}({producer_parallelism}){short_consumer}({consumer_parallelism}) [{pattern}]\"\n",
    "        short_label = f\"{short_producer}({producer_parallelism})[\"\n",
    "        for file_group in row['file_groups']:   \n",
    "            short_label += f\"{file_group},\".replace(\"_.*\", \"\").replace(\".*\",\"\").replace(\".nc\",\"\").replace(\"^\",\"\")\n",
    "        # replace the last char with ]\n",
    "        short_label = short_label[:-1] + \"]\"\n",
    "        short_label += f\"{short_consumer}({consumer_parallelism}) [{pattern}]\"\n",
    "\n",
    "        show_label = pair_label not in seen_labels\n",
    "        seen_labels.add(pair_label)\n",
    "\n",
    "        trMiB = row['trMiB'] if pd.notna(row['trMiB']) else 0\n",
    "        opCount = row['opCount'] if pd.notna(row['opCount']) else 0\n",
    "        filesize = row['aggregateFilesizeMB'] if pd.notna(row['aggregateFilesizeMB']) else 0\n",
    "\n",
    "        ax.scatter(\n",
    "            opCount, trMiB, filesize,\n",
    "            label=short_label if show_label else None,\n",
    "            color=color_map[pair_label],\n",
    "            marker=marker_map[pair_label],\n",
    "            s=300\n",
    "        )\n",
    "        ax.plot([opCount, opCount], [trMiB, trMiB], [0, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
    "        # ax.plot([opCount, opCount], [0, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
    "        ax.plot([0, opCount], [trMiB, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Styling\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 22,\n",
    "        'axes.titlesize': 20,\n",
    "        'axes.labelsize': 20,\n",
    "        'xtick.labelsize': 14,\n",
    "        'ytick.labelsize': 14,\n",
    "        'legend.fontsize': 19,\n",
    "        'legend.title_fontsize': 19,\n",
    "    })\n",
    "\n",
    "    # Reduce 3D tick-number size on all axes\n",
    "    ax.tick_params(axis='x', which='major', labelsize=16, pad=5)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=16, pad=5)\n",
    "    ax.zaxis.set_tick_params(labelsize=16, pad=8)\n",
    "\n",
    "    \n",
    "    ax.set_xlabel('Operation Count', labelpad=23)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=16)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=20)\n",
    "    # set zlim to 0 to max * 1.1\n",
    "    ax.set_zlim(0, df_pc['aggregateFilesizeMB'].max() * 1.5)\n",
    "    \n",
    "    # Explicit two-column split: first column rows 0-11, second column rows 12+\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    left_handles, left_labels = handles[:12], labels[:12]\n",
    "    right_handles, right_labels = handles[12:], labels[12:]\n",
    "\n",
    "    legend_left = ax.legend(\n",
    "        left_handles,\n",
    "        left_labels,\n",
    "        title=\"Producer  [Files]  Consumer\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(-1.4, 0.54),  # long legend on left side of 3D plot\n",
    "        framealpha=0.1,\n",
    "        facecolor='white',\n",
    "        labelspacing=0.5,\n",
    "        alignment='left',\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=19,\n",
    "    )\n",
    "    ax.add_artist(legend_left)\n",
    "\n",
    "    if right_handles:\n",
    "        legend_right = ax.legend(\n",
    "            right_handles,\n",
    "            right_labels,\n",
    "            loc=\"lower center\",\n",
    "            bbox_to_anchor=(0.50, 0.85),  # short legend on top of 3D plot\n",
    "            framealpha=0.1,\n",
    "            facecolor='white',\n",
    "            labelspacing=0.1,\n",
    "            alignment='left',\n",
    "            borderaxespad=0.0,\n",
    "        )\n",
    "        ax.add_artist(legend_right)\n",
    "    # plt.tight_layout()\n",
    "    # fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(sci_notation))\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f'{result_path}/pcAgg_group{group_idx+1}_{plot_file_name}')\n",
    "    \n",
    "    # print out all data points in this graph\n",
    "    print(subset_group[['pcPair', 'aggregateFilesizeMB', 'totalTime', 'trMiB', 'producer_parallelism', 'consumer_parallelism']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['operation', 'taskName', 'prevTask', 'fileName']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "x = df['trMiB']\n",
    "y = df['opCount']\n",
    "z = df['aggregateFilesizeMB']\n",
    "\n",
    "def sci_if_over_4_digits(x, pos):\n",
    "    if np.isclose(x, 0):\n",
    "        return \"0\"\n",
    "    if abs(x) > 9999:\n",
    "        return f\"{x:.0e}\"\n",
    "    if float(x).is_integer():\n",
    "        return f\"{int(x)}\"\n",
    "    return f\"{x:g}\"\n",
    "\n",
    "# Generate a color palette\n",
    "unique_tasks = df['taskName'].unique()\n",
    "num_tasks = len(unique_tasks)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=22)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of tasks\n",
    "num_cols = min(3, num_tasks)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_tasks / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_tasks)]\n",
    "\n",
    "for i, (task, (row, col)) in enumerate(zip(unique_tasks, task_indices)):\n",
    "    subset = df[df['taskName'] == task]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['opCount'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "               label=shorten_task_name(task), \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {task}')\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_xlabel('Operation Count', labelpad=13)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(sci_if_over_4_digits))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(sci_if_over_4_digits))\n",
    "    ax.zaxis.set_major_formatter(FuncFormatter(sci_if_over_4_digits))\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/tasksubplot_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def sci_if_over_4_digits(x, pos):\n",
    "    if np.isclose(x, 0):\n",
    "        return \"0\"\n",
    "    if abs(x) > 9999:\n",
    "        return f\"{x:.0e}\"\n",
    "    if float(x).is_integer():\n",
    "        return f\"{int(x)}\"\n",
    "    return f\"{x:g}\"\n",
    "\n",
    "# Generate a color palette\n",
    "unique_filegroup = df['file_group'].unique()\n",
    "num_filegroups = len(unique_filegroup)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=22)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of unique file groups\n",
    "num_cols = min(3, num_filegroups)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_filegroups / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_filegroups)]\n",
    "\n",
    "for i, (file, (row, col)) in enumerate(zip(unique_filegroup, task_indices)):\n",
    "    subset = df[df['file_group'] == file]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['trMiB'], subset['opCount'], subset['aggregateFilesizeMB'], \n",
    "               label=file, \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {file}')\n",
    "    ax.set_xlabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_ylabel('Operation Count', labelpad=13)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(sci_if_over_4_digits))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(sci_if_over_4_digits))\n",
    "    ax.zaxis.set_major_formatter(FuncFormatter(sci_if_over_4_digits))\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/filegroup_subplot_op_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "def sci_if_over_4_digits(x, pos):\n",
    "    if np.isclose(x, 0):\n",
    "        return \"0\"\n",
    "    if abs(x) > 9999:\n",
    "        return f\"{x:.0e}\"\n",
    "    if float(x).is_integer():\n",
    "        return f\"{int(x)}\"\n",
    "    return f\"{x:g}\"\n",
    "\n",
    "# Generate a color palette\n",
    "unique_filegroup = df['file_group'].unique()\n",
    "num_filegroups = len(unique_filegroup)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=22)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of unique file groups\n",
    "num_cols = min(3, num_filegroups)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_filegroups / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_filegroups)]\n",
    "\n",
    "for i, (file, (row, col)) in enumerate(zip(unique_filegroup, task_indices)):\n",
    "    subset = df[df['file_group'] == file]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['stageOrder'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "               label=file, \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {file}')\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_xlabel('Stage Order', labelpad=13)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(sci_if_over_4_digits))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(sci_if_over_4_digits))\n",
    "    ax.zaxis.set_major_formatter(FuncFormatter(sci_if_over_4_digits))\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/filegroup_subplot_stage_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns, df['stageOrder'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
