{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "\n",
    "project_root = \".\"\n",
    "\n",
    "\n",
    "pattern_configs = {\n",
    "    \"montage\":{\n",
    "        \"task_order_list\" : f\"{project_root}/montage/montage_script_order.json\",\n",
    "        \"csv_file_path\" : f'{project_root}/workflow_data/montage_s8_tr_estimated_fixed.csv',\n",
    "        \"plot_file_name\" : 'montage_3d_relationship_files.pdf',\n",
    "        \"file_group_patterns\" : [\n",
    "            \"^region.*.hdr\",\n",
    "            \"^poss.*.fits\",\n",
    "            \"^pposs.*.fits\",\n",
    "            \"^pposs.*_area.fits\",\n",
    "            \"^1-diff.*.txt\",\n",
    "            \"^1-diff.*.fits\",\n",
    "            \"^1-fit.*.txt\",\n",
    "            \"^1-.*.tbl\",\n",
    "            # \"1-fits.tbl\",\n",
    "            # \"1-images.tbl\",\n",
    "            # \"1-corrections.tbl\",\n",
    "            # \"1-projected.tbl\",\n",
    "            \"^cposs.*.fits\",\n",
    "            \"^cposs.*_area.fits\",\n",
    "            # \"1-corrected.tbl\",\n",
    "            # \"1-updated-corrected.tbl\",\n",
    "            # \"1-updated-corrected.tbl\",\n",
    "            \"^region.hdr\",\n",
    "            \"^1-mosaic.*.fits\",\n",
    "            \"^1-mosaic.png\"\n",
    "        ],\n",
    "        \"result_path\": f\"./montage_plots\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define file grouping file_group_patterns\n",
    "\n",
    "FLOW_SCALE = 1000\n",
    "\n",
    "def shorten_task_name(name):\n",
    "    parts = name.split(\"_\")\n",
    "    # print(f\"Original parts: {parts}\")\n",
    "    shortened = [p[:10] for i, p in enumerate(parts) if i % 2 == 0 or i == len(parts) - 1]\n",
    "    # print(f\"Shortened: {shortened}\")\n",
    "    return \"_\".join(shortened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read CSV file\n",
    "# csv_file_path = '{project_root}/workflow_data/par_9000_1n_pfs_ps300_fixed.csv'\n",
    "# plot_file_name = '9000_3d_relationship_files.pdf'\n",
    "\n",
    "# csv_file_path = '{project_root}/workflow_data/summer_sam_4n_pfs_s9_tr_estimated.csv'\n",
    "# plot_file_name = 'pyflex_3d_relationship_files.pdf'\n",
    "\n",
    "CURR_WF = \"montage\" # pyflex, ddmd, 1kgenome\n",
    "\n",
    "csv_file_path = pattern_configs[CURR_WF][\"csv_file_path\"]\n",
    "plot_file_name = pattern_configs[CURR_WF][\"plot_file_name\"]\n",
    "file_group_patterns = pattern_configs[CURR_WF][\"file_group_patterns\"]\n",
    "extension_grouping = pattern_configs[CURR_WF].get(\"extension_grouping\", False)\n",
    "result_path = pattern_configs[CURR_WF].get(\"result_path\", \"./result_plots\")\n",
    "\n",
    "# make result_path if it doesn't exist\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size, df.shape, df.columns, df['operation'].unique()\n",
    "\n",
    "# Replace with shortened task name in all taskName columns\n",
    "short_taskName_dict = {\n",
    "    \"mProject\": \"Pro\",\n",
    "    \"mDiffFit\": \"DiffF\",\n",
    "    \"mConcatFit\": \"ConF\",\n",
    "    \"mBgModel\": \"BgM\",\n",
    "    \"mBackground\": \"Bg\",\n",
    "    \"mImgtbl\": \"Img\",\n",
    "    \"mAdd\": \"Add\",\n",
    "    \"mViewer\": \"View\",\n",
    "}\n",
    "\n",
    "# Replace task names in the DataFrame\n",
    "df['taskName'] = df['taskName'].replace(short_taskName_dict)\n",
    "# Replace task names in the DataFrame\n",
    "df['prevTask'] = df['prevTask'].replace(short_taskName_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Dataframe Info:\n",
    "\n",
    "# # of unique Stages\n",
    "unique_stages = df['stageOrder'].unique()\n",
    "# # of unique Tasks\n",
    "unique_tasks = df['taskName'].unique()\n",
    "# # Task instance count based on dominant I/O type\n",
    "task_instance_counts = (\n",
    "    df.groupby(['taskName', 'operation'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .max(axis=1)\n",
    ")\n",
    "\n",
    "# If needed as a total count\n",
    "total_task_instances = task_instance_counts.sum()\n",
    "\n",
    "# # of I/O entries (rows in the DataFrame)\n",
    "num_io_entries = df.shape[0]\n",
    "# # of unique I/O files\n",
    "unique_files = df['fileName'].unique()\n",
    "# # average file resues: calculate the average number of times each file is reused for each unique file\n",
    "avg_file_reuses = df.groupby('fileName').size().mean()\n",
    "# Total I/O size\n",
    "total_io_size = df['aggregateFilesizeMB'].sum()\n",
    "\n",
    "# Print all the information\n",
    "print(\"=========================================\")\n",
    "print(f\"Number of unique stages: {len(unique_stages)}\")\n",
    "# print(f\"Number of unique tasks: {len(unique_tasks)}\")\n",
    "print(f\"Total task instances: {total_task_instances}\")\n",
    "print(f\"Total I/O size (MB): {total_io_size:.2f}\")\n",
    "print(f\"Average I/O size per task (MB): {(total_io_size/total_task_instances):.2f}\")\n",
    "print(f\"Total I/O size (GB): {total_io_size / 1024:.2f}\")\n",
    "# print(f\"Number of I/O entries: {num_io_entries}\")\n",
    "print(f\"Number of unique I/O files: {len(unique_files)}\")\n",
    "print(f\"Average file reuses: {avg_file_reuses:.2f}\")\n",
    "print(\"=========================================\")\n",
    "# Print unique stages and tasks and File names\n",
    "print(f\"Unique stages: {unique_stages}\")\n",
    "print(f\"Unique tasks: {unique_tasks}\")\n",
    "print(f\"Unique file names: {unique_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 95%ile of the aggregateFilesizeMB\n",
    "# 50%ile\n",
    "percentile_50 = df['aggregateFilesizeMB'].quantile(0.50)\n",
    "print(f\"50th percentile of aggregateFilesizeMB: {percentile_50:.2f}\")\n",
    "# 75th percentile\n",
    "percentile_75 = df['aggregateFilesizeMB'].quantile(0.75)\n",
    "print(f\"75th percentile of aggregateFilesizeMB: {percentile_75:.2f}\")\n",
    "# 85th percentile\n",
    "percentile_85 = df['aggregateFilesizeMB'].quantile(0.85)\n",
    "print(f\"85th percentile of aggregateFilesizeMB: {percentile_85:.2f}\")\n",
    "# 95th percentile\n",
    "percentile_95 = df['aggregateFilesizeMB'].quantile(0.95)\n",
    "# 99th percentile\n",
    "percentile_99 = df['aggregateFilesizeMB'].quantile(0.99)\n",
    "print(f\"95th percentile of aggregateFilesizeMB: {percentile_95:.2f}\")\n",
    "print(f\"99th percentile of aggregateFilesizeMB: {percentile_99:.2f}\")\n",
    "\n",
    "\n",
    "# get max trMiB of df \n",
    "max_trMiB = df['trMiB'].max()\n",
    "print(f\"Max trMiB of all tasks: {max_trMiB:.2f}\")\n",
    "\n",
    "# get min trMiB of df\n",
    "min_trMiB = df['trMiB'].min()\n",
    "print(f\"Min trMiB of all tasks: {min_trMiB:.2f}\")\n",
    "\n",
    "# get 50 percentile of trMiB\n",
    "percentile_50 = df['trMiB'].quantile(0.50)\n",
    "print(f\"50th percentile of trMiB: {percentile_50:.2f}\")\n",
    "\n",
    "# get 85 percentile of trMiB\n",
    "percentile_85 = df['trMiB'].quantile(0.85)\n",
    "print(f\"85th percentile of trMiB: {percentile_85:.2f}\")\n",
    "\n",
    "# get average trMiB of all tasks\n",
    "avg_trMiB = df['trMiB'].mean()\n",
    "print(f\"Avg trMiB of all tasks: {avg_trMiB:.2f}\")\n",
    "\n",
    "# get the average trMiB of all unique tasks\n",
    "avg_trMiB = df.groupby('taskName')['trMiB'].mean()\n",
    "print(f\"Avg trMiB of all unique tasks: {avg_trMiB}\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# ger average task aggregateFilesizeMB size\n",
    "avg_aggregateFilesizeMB = df.groupby('taskName')['aggregateFilesizeMB'].mean()\n",
    "print(f\"Avg aggregateFilesizeMB of all unique tasks: {avg_aggregateFilesizeMB}\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "\n",
    "unique_tasks = df['taskName'].unique()\n",
    "\n",
    "for task in unique_tasks:\n",
    "    avg_trMiB = df[df['taskName'] == task]['trMiB'].mean()\n",
    "    print(f\"Avg trMiB of task {task}: {avg_trMiB:.2f}\")\n",
    "    # get the average opCount of task\n",
    "    avg_opCount = df[df['taskName'] == task]['opCount'].mean()\n",
    "    print(f\"Avg opCount of task {task}: {avg_opCount:.2f}\")\n",
    "    # get number of entries of task\n",
    "    num_entries = df[df['taskName'] == task].shape[0]\n",
    "    print(f\"Number of entries of task {task}: {num_entries}\")\n",
    "    # get the average aggregateFilesizeMB of task\n",
    "    avg_aggregateFilesizeMB = df[df['taskName'] == task]['aggregateFilesizeMB'].mean()\n",
    "    print(f\"Avg aggregateFilesizeMB of task {task}: {avg_aggregateFilesizeMB:.2f}\")\n",
    "    # get number of read entries of task\n",
    "    num_read_entries = df[(df['taskName'] == task) & (df['operation'] == 1)].shape[0]\n",
    "    print(f\"Number of read entries of task {task}: {num_read_entries}\")\n",
    "    # get number of write entries of task\n",
    "    num_write_entries = df[(df['taskName'] == task) & (df['operation'] == 0)].shape[0]\n",
    "    print(f\"Number of write entries of task {task}: {num_write_entries}\")\n",
    "    \n",
    "    # get the parallelized trMiB of task\n",
    "    task_parallelism = df[df['taskName'] == task]['parallelism'].mean()\n",
    "    print(f\"Parallelism of task {task}: {task_parallelism:.2f}\")\n",
    "    if task_parallelism == 1:\n",
    "        task_total_trMiB = df[df['taskName'] == task]['trMiB'].mean()\n",
    "        print(f\"Total trMiB of task {task}: {task_total_trMiB:.2f}\")\n",
    "    else:\n",
    "        task_total_trMiB = df[df['taskName'] == task]['trMiB'].sum()\n",
    "        print(f\"Total trMiB of task {task}: {task_total_trMiB:.2f}\")\n",
    "    task_parallelised_trMiB = task_total_trMiB / task_parallelism\n",
    "    print(f\"Parallelized trMiB of task {task}: {task_parallelised_trMiB:.2f}\")\n",
    "    \n",
    "    # read operation average trMiB\n",
    "    read_trMiB = df[(df['taskName'] == task) & (df['operation'] == 1)]['trMiB'].mean()\n",
    "    print(f\"Avg read trMiB of task {task}: {read_trMiB:.2f}\")\n",
    "    \n",
    "    # write operation average trMiB\n",
    "    write_trMiB = df[(df['taskName'] == task) & (df['operation'] == 0)]['trMiB'].mean()\n",
    "    print(f\"Avg write trMiB of task {task}: {write_trMiB:.2f}\")\n",
    "    \n",
    "    print(\"==========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add new columns for colors and labels based on the 'operation' column\n",
    "# df['color'] = df['operation'].map({0: 'blue', 1: 'red'})\n",
    "# df['label'] = df['operation'].map({0: 'write', 1: 'read'})\n",
    "# df['marker'] = df['operation'].map({0: 'x', 1: 'o'})  # 'o' for circle, 's' for square\n",
    "\n",
    "# # 2D plot: Relationship between aggregateFilesizeMB and totalTime\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# for label in df['label'].unique():\n",
    "#     subset = df[df['label'] == label]\n",
    "#     marker = subset['marker'].iloc[0]\n",
    "#     facecolor = 'none' if marker == 'o' else subset['color'].iloc[0]\n",
    "#     plt.scatter(subset['trMiB'], subset['opCount'], \n",
    "#                 label=label, \n",
    "#                 edgecolor=subset['color'].iloc[0],  # Edge color for circle markers\n",
    "#                 facecolors=facecolor,  # Only the circle markers will be hollow\n",
    "#                 marker=marker,\n",
    "#                 s=200,\n",
    "#                 alpha=0.5)\n",
    "\n",
    "# plt.title('Relationship between Transfer Size and Total Time')\n",
    "# plt.xlabel('I/O Bandwidth (MB/s)')\n",
    "# # plt.xlabel('Data size (MB)')\n",
    "# plt.ylabel('Operation count')\n",
    "# plt.grid(True)\n",
    "# plt.legend(title=\"Operation\")\n",
    "# plt.show()\n",
    "\n",
    "# Define mapping of operation and randomness to I/O type\n",
    "def map_io_type(row):\n",
    "    if row['operation'] == 0 and row['randomOffset'] == 0:\n",
    "        return 'sequential write'\n",
    "    elif row['operation'] == 0 and row['randomOffset'] == 1:\n",
    "        return 'random write'\n",
    "    elif row['operation'] == 1 and row['randomOffset'] == 0:\n",
    "        return 'sequential read'\n",
    "    elif row['operation'] == 1 and row['randomOffset'] == 1:\n",
    "        return 'random read'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply the mapping\n",
    "df['io_type'] = df.apply(map_io_type, axis=1)\n",
    "\n",
    "# Assign colors and markers to each I/O type\n",
    "io_color_map = {\n",
    "    'sequential write': 'blue',\n",
    "    'random write': 'purple',\n",
    "    'sequential read': 'green',\n",
    "    'random read': 'orange',\n",
    "}\n",
    "io_marker_map = {\n",
    "    'sequential write': 'x',\n",
    "    'random write': 'D',\n",
    "    'sequential read': 'o',\n",
    "    'random read': 's',\n",
    "}\n",
    "\n",
    "io_short_name_map = {\n",
    "    \"sequential write\": 'Seq W',\n",
    "    \"random write\": 'Rand W',\n",
    "    \"sequential read\": 'Seq R',\n",
    "    \"random read\": 'Rand R',\n",
    "}\n",
    "\n",
    "\n",
    "df['color'] = df['io_type'].map(io_color_map)\n",
    "df['marker'] = df['io_type'].map(io_marker_map)\n",
    "\n",
    "# 2D plot: Relationship between I/O Bandwidth and Operation Count\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for io_type in df['io_type'].unique():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    marker = subset['marker'].iloc[0]\n",
    "    facecolor = 'none' if marker == 'o' else subset['color'].iloc[0]\n",
    "    plt.scatter(\n",
    "        subset['trMiB'], \n",
    "        subset['opCount'], \n",
    "        label=io_type, \n",
    "        edgecolor=subset['color'].iloc[0], \n",
    "        facecolors=facecolor,\n",
    "        marker=marker,\n",
    "        s=200,\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "plt.title('Relationship between I/O Bandwidth and Operation Count')\n",
    "plt.xlabel('I/O Bandwidth (MB/s)')\n",
    "plt.ylabel('Operation Count')\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"I/O Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot: Relationship between aggregateFilesizeMB, opCount, and trMiB\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot actual data for present I/O types\n",
    "for io_type, color in io_color_map.items():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    marker = io_marker_map[io_type]\n",
    "    facecolor = 'none' if marker == 'o' else color\n",
    "\n",
    "    if not subset.empty:\n",
    "        ax.scatter(\n",
    "            subset['opCount'],\n",
    "            subset['trMiB'],\n",
    "            subset['aggregateFilesizeMB'],\n",
    "            label=io_short_name_map[io_type],\n",
    "            edgecolor=color,\n",
    "            facecolors=facecolor,\n",
    "            marker=marker,\n",
    "            s=200,\n",
    "            alpha=0.6\n",
    "        )\n",
    "        for _, row in subset.iterrows():\n",
    "            x = row['opCount']\n",
    "            y = row['trMiB']\n",
    "            z = row['aggregateFilesizeMB']\n",
    "            # Line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color, alpha=0.3, linewidth=2)\n",
    "    else:\n",
    "        # Add a dummy scatter for missing data to include in legend\n",
    "        ax.scatter([], [], [], \n",
    "                   label=io_type,\n",
    "                   edgecolor=color,\n",
    "                   facecolors='none' if marker == 'o' else color,\n",
    "                   marker=marker,\n",
    "                   s=200,\n",
    "                   alpha=0.5)\n",
    "\n",
    "# Font size settings\n",
    "plt.rc('font', size=20)\n",
    "plt.rc('axes', titlesize=20)\n",
    "plt.rc('axes', labelsize=20)\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rc('legend', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('Operation Count', labelpad=20)\n",
    "# ax.set_xscale('log')\n",
    "ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=18)\n",
    "ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "\n",
    "ax.legend(title=\"I/O Type\", loc='center', bbox_to_anchor=(0.1, 0.7, 0.8, 0.0),framealpha=0.8,\n",
    "          ncol=2, columnspacing=0.1\n",
    "          )\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f\"{result_path}/op_{plot_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 2D plot: x = opCount, y = trMiB, point size = aggregateFilesizeMB, color = io_type\n",
    "# fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# size_scale = FLOW_SCALE\n",
    "\n",
    "# # Plot actual data for present I/O types\n",
    "# for io_type, color in io_color_map.items():\n",
    "#     subset = df[df['io_type'] == io_type]\n",
    "    \n",
    "#     if not subset.empty:\n",
    "#         sizes = np.log10(subset['aggregateFilesizeMB'] +1.1) * size_scale  # Adjust as needed\n",
    "\n",
    "#         ax.scatter(\n",
    "#             subset['opCount'],\n",
    "#             subset['trMiB'],\n",
    "#             s=sizes,\n",
    "#             label=io_short_name_map[io_type],\n",
    "#             color=color,\n",
    "#             marker='o',\n",
    "#             alpha=0.3\n",
    "#         )\n",
    "#     else:\n",
    "#         ax.scatter([], [], s=80, label=io_short_name_map[io_type], color=color, marker='o', alpha=0.3)\n",
    "\n",
    "# # Font size settings\n",
    "# plt.rc('font', size=20)\n",
    "# plt.rc('axes', titlesize=18)\n",
    "# plt.rc('axes', labelsize=20)\n",
    "# plt.rc('xtick', labelsize=20)\n",
    "# plt.rc('ytick', labelsize=20)\n",
    "# plt.rc('legend', fontsize=18)\n",
    "\n",
    "# ax.set_xlabel('Operation Count', labelpad=20)\n",
    "# # ax.set_xscale('log')\n",
    "# ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "# ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "\n",
    "# # # Set log scale for x-axis\n",
    "# # ax.set_xscale('log')\n",
    "# # ====== Create fixed-size legend entries for I/O types =======\n",
    "# legend_handles = []\n",
    "# for io_type, color in io_color_map.items():\n",
    "#     label = io_short_name_map[io_type]\n",
    "#     handle = plt.scatter([], [], s=200, color=color, label=label, alpha=0.3)  # Fixed size\n",
    "#     legend_handles.append(handle)\n",
    "    \n",
    "# # First legend: I/O types\n",
    "# legend1 = ax.legend(\n",
    "#     handles=legend_handles,\n",
    "#     title=\"I/O Type\",\n",
    "#     loc='center',\n",
    "#     bbox_to_anchor=(0.37, 0.68, 0.8, 0.0),\n",
    "#     framealpha=0.4,\n",
    "#     facecolor='white',\n",
    "#     labelspacing=0.1,\n",
    "#     columnspacing=0.1,\n",
    "# )\n",
    "# ax.add_artist(legend1)\n",
    "\n",
    "# # Second legend: Flow Size (MB)\n",
    "# # Choose some representative values to show in the legend\n",
    "# # Get non-zero aggregateFilesizeMB values\n",
    "# flow_data = df['aggregateFilesizeMB'].dropna().sort_values()\n",
    "# if not flow_data.empty:\n",
    "#     min_val = flow_data.min()\n",
    "#     if min_val == 0:\n",
    "#         min_val = np.percentile(flow_data, 1)\n",
    "#     max_val = flow_data.max()\n",
    "#     mid1 = np.percentile(flow_data, 33)\n",
    "#     mid2 = np.percentile(flow_data, 66)\n",
    "#     flow_sizes = [round(mid1,4), round(max_val,2)]\n",
    "# else:\n",
    "#     flow_sizes = [10, 50, 100, 500]  # fallback\n",
    "    \n",
    "# scatter_proxies = [plt.scatter([], [], s=np.log10(fs+1.1) * size_scale, color='gray', alpha=0.3) for fs in flow_sizes]\n",
    "# labels = [f\"{fs} MB\" for fs in flow_sizes]\n",
    "\n",
    "# legend2 = ax.legend(scatter_proxies, labels, title=\"Flow Size\", loc='center', \n",
    "#                     bbox_to_anchor=(-.11, 0.7, 0.8, 0.0), framealpha=0.2, facecolor='white',\n",
    "#                     labelspacing=0.8, columnspacing=0.1,\n",
    "#                     )\n",
    "# ax.add_artist(legend2)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# fig.savefig(f\"{result_path}/op_{plot_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot: Relationship between aggregateFilesizeMB, stageOrder, and trMiB\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot for each defined I/O type, whether or not data exists\n",
    "for io_type, color in io_color_map.items():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    marker = io_marker_map[io_type]\n",
    "    facecolor = 'none' if marker == 'o' else color\n",
    "\n",
    "    if not subset.empty:\n",
    "        ax.scatter(\n",
    "            subset['stageOrder'],\n",
    "            subset['trMiB'],\n",
    "            subset['aggregateFilesizeMB'],\n",
    "            label=io_short_name_map[io_type],\n",
    "            edgecolor=color,\n",
    "            facecolors=facecolor,\n",
    "            marker=marker,\n",
    "            s=200,\n",
    "            alpha=0.5\n",
    "        )\n",
    "        for _, row in subset.iterrows():\n",
    "            x = row['stageOrder']\n",
    "            y = row['trMiB']\n",
    "            z = row['aggregateFilesizeMB']\n",
    "            # Line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color,\n",
    "                    alpha=0.3, linewidth=2)\n",
    "    else:\n",
    "        # Dummy point for legend\n",
    "        ax.scatter([], [], [],\n",
    "                   label=io_type,\n",
    "                   edgecolor=color,\n",
    "                   facecolors=facecolor,\n",
    "                   marker=marker,\n",
    "                   s=200,\n",
    "                   alpha=0.5)\n",
    "\n",
    "# Font size settings\n",
    "plt.rc('font', size=20)\n",
    "plt.rc('axes', titlesize=20)\n",
    "plt.rc('axes', labelsize=20)\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rc('legend', fontsize=20)\n",
    "\n",
    "# Switched axes\n",
    "ax.set_xlabel('Stage Order', labelpad=20)\n",
    "ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "\n",
    "ax.legend(title=\"I/O Type\", loc='center', bbox_to_anchor=(0.1, 0.6, 0.8, 0.0),framealpha=0.8,\n",
    "          ncol=2, columnspacing=0.1\n",
    "          )\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f\"{result_path}/stage_{plot_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 2D plot: x = opCount, y = trMiB, point size = aggregateFilesizeMB, color = io_type\n",
    "# fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# size_scale = FLOW_SCALE\n",
    "\n",
    "# # Plot actual data for present I/O types\n",
    "# for io_type, color in io_color_map.items():\n",
    "#     subset = df[df['io_type'] == io_type]\n",
    "    \n",
    "#     if not subset.empty:\n",
    "#         sizes = np.log10(subset['aggregateFilesizeMB'] +1.1) * size_scale  # Adjust as needed\n",
    "\n",
    "#         ax.scatter(\n",
    "#             subset['stageOrder'],\n",
    "#             subset['trMiB'],\n",
    "#             s=sizes,\n",
    "#             label=io_short_name_map[io_type],\n",
    "#             color=color,\n",
    "#             marker='o',\n",
    "#             alpha=0.3\n",
    "#         )\n",
    "#     else:\n",
    "#         ax.scatter([], [], s=80, label=io_short_name_map[io_type], color=color, marker='o', alpha=0.3)\n",
    "\n",
    "# # Font size settings\n",
    "# plt.rc('font', size=20)\n",
    "# plt.rc('axes', titlesize=24)\n",
    "# plt.rc('axes', labelsize=24)\n",
    "# plt.rc('xtick', labelsize=20)\n",
    "# plt.rc('ytick', labelsize=20)\n",
    "# plt.rc('legend', fontsize=20)\n",
    "\n",
    "# ax.set_xlabel('Stage Order', labelpad=20)\n",
    "# ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "# ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "\n",
    "# # # Set log scale for x-axis\n",
    "# # ax.set_xscale('log')\n",
    "# # ====== Create fixed-size legend entries for I/O types =======\n",
    "# legend_handles = []\n",
    "# for io_type, color in io_color_map.items():\n",
    "#     label = io_short_name_map[io_type]\n",
    "#     handle = plt.scatter([], [], s=200, color=color, label=label, alpha=0.3)  # Fixed size\n",
    "#     legend_handles.append(handle)\n",
    "    \n",
    "\n",
    "\n",
    "# # Second legend: Flow Size (MB)\n",
    "# # Choose some representative values to show in the legend\n",
    "# # Get non-zero aggregateFilesizeMB values\n",
    "# flow_data = df['aggregateFilesizeMB'].dropna().sort_values()\n",
    "# if not flow_data.empty:\n",
    "#     min_val = flow_data.min()\n",
    "#     if min_val == 0:\n",
    "#         min_val = np.percentile(flow_data, 1)\n",
    "#     max_val = flow_data.max()\n",
    "#     mid1 = np.percentile(flow_data, 33)\n",
    "#     mid2 = np.percentile(flow_data, 66)\n",
    "#     flow_sizes = [round(mid1,4), round(max_val,4)]\n",
    "# else:\n",
    "#     flow_sizes = [10, 50, 100, 500]  # fallback\n",
    "    \n",
    "# scatter_proxies = [plt.scatter([], [], s=np.log10(fs+1.1) * size_scale, color='gray', alpha=0.3) for fs in flow_sizes]\n",
    "# labels = [f\"{fs} MB\" for fs in flow_sizes]# First legend: I/O types\n",
    "# legend1 = ax.legend(\n",
    "#     handles=legend_handles,\n",
    "#     title=\"I/O Type\",\n",
    "#     loc='center',\n",
    "#     bbox_to_anchor=(0.05, 0.7, 0.8, 0.0),\n",
    "#     framealpha=0.4,\n",
    "#     facecolor='white',\n",
    "#     labelspacing=0.1,\n",
    "#     ncol=2,\n",
    "#     columnspacing=0.1,\n",
    "# )\n",
    "# ax.add_artist(legend1)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# fig.savefig(f\"{result_path}/stage_{plot_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "def sci_notation(x, pos):\n",
    "    return f'{x:.0e}'\n",
    "\n",
    "# Build task ordering from CSV stage-order column (JSON order file removed)\n",
    "stage_col = 'stage_order' if 'stage_order' in df.columns else ('stageOrder' if 'stageOrder' in df.columns else None)\n",
    "if stage_col is not None:\n",
    "    stage_order_df = df[['taskName', stage_col]].dropna(subset=['taskName', stage_col]).copy()\n",
    "    stage_order_df[stage_col] = pd.to_numeric(stage_order_df[stage_col], errors='coerce')\n",
    "    stage_order_df = stage_order_df.dropna(subset=[stage_col]).drop_duplicates(subset=['taskName'], keep='first')\n",
    "    task_stage_order = stage_order_df.set_index('taskName')[stage_col].to_dict()\n",
    "    print(f\"Loaded stage order for {len(task_stage_order)} tasks from CSV column '{stage_col}': {csv_file_path}\")\n",
    "else:\n",
    "    task_stage_order = {}\n",
    "    print(\"Warning: no stage order column found in CSV ('stage_order' or 'stageOrder'). Falling back to default task ordering.\")\n",
    "\n",
    "# Ensure tasks are ordered based on stage order from CSV\n",
    "unique_tasks = sorted(\n",
    "    df['taskName'].dropna().unique(), \n",
    "    key=lambda t: task_stage_order.get(t, float('inf'))\n",
    ")\n",
    "\n",
    "# Debugging output\n",
    "print(f\"Unique tasks: {unique_tasks}\")  \n",
    "if not unique_tasks:\n",
    "    print(\"No tasks found, skipping plotting.\")\n",
    "    exit()\n",
    "\n",
    "num_tasks = len(unique_tasks)\n",
    "\n",
    "# Define color palette and markers\n",
    "palette = sns.color_palette(\"deep\", num_tasks)\n",
    "# markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# Split tasks into groups of 5\n",
    "task_group_count = 10\n",
    "task_groups = [unique_tasks[i:i+task_group_count] for i in range(0, num_tasks, task_group_count)]\n",
    "\n",
    "for group_idx, task_group in enumerate(task_groups):\n",
    "    # Filter out empty groups\n",
    "    valid_tasks = [task for task in task_group if task in df['taskName'].values]\n",
    "    print(f\"Group {group_idx+1}: {valid_tasks}\")\n",
    "    if not valid_tasks:\n",
    "        print(f\"Skipping empty group {group_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    # 3D plot: Relationship between aggregateFilesizeMB, totalTime, and trMiB\n",
    "    fig = plt.figure(figsize=(6, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    x = df['trMiB']\n",
    "    y = df['opCount']\n",
    "    z = df['aggregateFilesizeMB']\n",
    "    markers = ['+', 'x', '3', '2', '*']  # Add more markers as needed\n",
    "\n",
    "    # Generate color and marker maps for current group\n",
    "    color_map = {task: palette[i % len(palette)] for i, task in enumerate(valid_tasks)}\n",
    "    marker_map = {task: markers[i % len(markers)] for i, task in enumerate(valid_tasks)}\n",
    "\n",
    "    df['color'] = df['taskName'].map(lambda task: color_map.get(task, 'black'))  # Black if not found\n",
    "    df['marker'] = df['taskName'].map(lambda task: marker_map.get(task, 'o'))  # 'o' if not found\n",
    "\n",
    "    for task in valid_tasks:\n",
    "        subset = df[df['taskName'] == task]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping task '{task}' (no data)\")\n",
    "            continue\n",
    "        \n",
    "        opCount = subset['opCount'] #.iloc[0]\n",
    "        trMiB = subset['trMiB'] #.iloc[0]\n",
    "        filesize = subset['aggregateFilesizeMB'] #.iloc[0]\n",
    "\n",
    "        ax.scatter(opCount, trMiB, filesize, \n",
    "                   label=shorten_task_name(task), \n",
    "                   color=color_map[task], \n",
    "                   marker=marker_map[task],\n",
    "                   s=200)\n",
    "        for _, row in subset.iterrows():\n",
    "            x = row['opCount']\n",
    "            y = row['trMiB']\n",
    "            z = row['aggregateFilesizeMB']\n",
    "            # Line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color_map[task],\n",
    "                    alpha=0.3, linewidth=1)\n",
    "\n",
    "        # if \"mConF\" in task or \"mBg\" in task:\n",
    "        #     ax.plot([opCount, opCount], [trMiB, trMiB], [0, filesize], linestyle='dashed', color='gray', linewidth=2)\n",
    "        #     ax.plot([opCount, opCount], [0, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=2)\n",
    "        #     ax.plot([0, opCount], [trMiB, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=2)\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=20)\n",
    "    plt.rc('axes', labelsize=18)\n",
    "    plt.rc('xtick', labelsize=18)\n",
    "    plt.rc('ytick', labelsize=18)\n",
    "    plt.rc('legend', fontsize=18)\n",
    "    \n",
    "    \n",
    "    ax.set_xlabel('Operation Count', labelpad=20)\n",
    "    # ax.set_xscale('log')\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "    # ax.set_yscale('log')\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=20)\n",
    "    # # show y axis starts from 0\n",
    "    # ax.set_ylim(bottom=0)\n",
    "\n",
    "    ax.legend(title=\"Task Name\", loc=\"right\", bbox_to_anchor=(0.04, 0.58, 0.8, 0.0),\n",
    "              labelspacing=0.2, framealpha=0.6, title_fontsize=18,\n",
    "              ) # (0, 0.83, 0.8, 0.0) (0, 0.6, 1.3, 0.0)\n",
    "    # fig.tight_layout()\n",
    "    # plt.figure(constrained_layout=True)\n",
    "    ax.set_box_aspect([1, 1, 1.8])\n",
    "    # ax.yaxis.set_major_formatter(FuncFormatter(sci_notation))\n",
    "    # ax.xaxis.set_major_formatter(FuncFormatter(sci_notation))\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f'{result_path}/taskColor_group{group_idx+1}_'+plot_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Build task ordering from CSV stage-order column (JSON order file removed)\n",
    "stage_col = 'stage_order' if 'stage_order' in df.columns else ('stageOrder' if 'stageOrder' in df.columns else None)\n",
    "if stage_col is not None:\n",
    "    stage_order_df = df[['taskName', stage_col]].dropna(subset=['taskName', stage_col]).copy()\n",
    "    stage_order_df[stage_col] = pd.to_numeric(stage_order_df[stage_col], errors='coerce')\n",
    "    stage_order_df = stage_order_df.dropna(subset=[stage_col]).drop_duplicates(subset=['taskName'], keep='first')\n",
    "    task_stage_order = stage_order_df.set_index('taskName')[stage_col].to_dict()\n",
    "    print(f\"Loaded stage order for {len(task_stage_order)} tasks from CSV column '{stage_col}': {csv_file_path}\")\n",
    "else:\n",
    "    task_stage_order = {}\n",
    "    print(\"Warning: no stage order column found in CSV ('stage_order' or 'stageOrder'). Falling back to default task ordering.\")\n",
    "\n",
    "# Ensure tasks are ordered based on stage order from CSV\n",
    "unique_tasks = sorted(\n",
    "    df['taskName'].dropna().unique(), \n",
    "    key=lambda t: task_stage_order.get(t, float('inf'))\n",
    ")\n",
    "\n",
    "# Debugging output\n",
    "print(f\"Unique tasks: {unique_tasks}\")  \n",
    "if not unique_tasks:\n",
    "    print(\"No tasks found, skipping plotting.\")\n",
    "    exit()\n",
    "\n",
    "num_tasks = len(unique_tasks)\n",
    "\n",
    "# Define color palette and markers\n",
    "palette = sns.color_palette(\"deep\", num_tasks)\n",
    "# markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# Split tasks into groups of 5\n",
    "task_group_count = 10\n",
    "task_groups = [unique_tasks[i:i+task_group_count] for i in range(0, num_tasks, task_group_count)]\n",
    "\n",
    "for group_idx, task_group in enumerate(task_groups):\n",
    "    # Filter out empty groups\n",
    "    valid_tasks = [task for task in task_group if task in df['taskName'].values]\n",
    "    print(f\"Group {group_idx+1}: {valid_tasks}\")\n",
    "    if not valid_tasks:\n",
    "        print(f\"Skipping empty group {group_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    # 3D plot: Relationship between aggregateFilesizeMB, totalTime, and trMiB\n",
    "    fig = plt.figure(figsize=(6, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    x = df['trMiB']\n",
    "    y = df['stageOrder']\n",
    "    z = df['aggregateFilesizeMB']\n",
    "    markers = ['+', 'x', '3', '2', '*']  # Add more markers as needed\n",
    "\n",
    "    # Generate color and marker maps for current group\n",
    "    color_map = {task: palette[i % len(palette)] for i, task in enumerate(valid_tasks)}\n",
    "    marker_map = {task: markers[i % len(markers)] for i, task in enumerate(valid_tasks)}\n",
    "\n",
    "    df['color'] = df['taskName'].map(lambda task: color_map.get(task, 'black'))  # Black if not found\n",
    "    df['marker'] = df['taskName'].map(lambda task: marker_map.get(task, 'o'))  # 'o' if not found\n",
    "\n",
    "    for task in valid_tasks:\n",
    "        subset = df[df['taskName'] == task]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping task '{task}' (no data)\")\n",
    "            continue\n",
    "        \n",
    "        opCount = subset['stageOrder'] #.iloc[0]\n",
    "        trMiB = subset['trMiB'] #.iloc[0]\n",
    "        filesize = subset['aggregateFilesizeMB'] #.iloc[0]\n",
    "\n",
    "        ax.scatter(opCount, trMiB, filesize, \n",
    "                   label=shorten_task_name(task), \n",
    "                   color=color_map[task], \n",
    "                   marker=marker_map[task],\n",
    "                   s=200)\n",
    "        for _, row in subset.iterrows():\n",
    "            x = row['stageOrder']\n",
    "            y = row['trMiB']\n",
    "            z = row['aggregateFilesizeMB']\n",
    "            # Line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color_map[task],\n",
    "                    alpha=0.3, linewidth=2)\n",
    "\n",
    "        # if \"mConF\" in task or \"mBg\" in task:\n",
    "        #     ax.plot([opCount, opCount], [trMiB, trMiB], [0, filesize], linestyle='dashed', color='gray', linewidth=2)\n",
    "        #     ax.plot([opCount, opCount], [0, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=2)\n",
    "        #     ax.plot([0, opCount], [trMiB, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=2)\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=20)\n",
    "    plt.rc('axes', labelsize=20)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=18)\n",
    "    \n",
    "    \n",
    "    ax.set_xlabel('Stage Order', labelpad=20)\n",
    "    # ax.set_xscale('log')\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "    # ax.set_yscale('log')\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=20)\n",
    "    # # show y axis starts from 0\n",
    "    # ax.set_ylim(bottom=0)\n",
    "\n",
    "    ax.legend(title=\"Task Name\", loc=\"right\", bbox_to_anchor=(0.08, 0.58, 0.8, 0.0),\n",
    "              labelspacing=0.2, framealpha=0.6, title_fontsize=18,\n",
    "              ) # (0, 0.83, 0.8, 0.0) (0, 0.6, 1.3, 0.0)\n",
    "    # fig.tight_layout()\n",
    "    # plt.figure(constrained_layout=True)\n",
    "    ax.set_box_aspect([0.9, 0.9, 1.2])\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f'{result_path}/taskColor_group{group_idx+1}_stage'+plot_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# # Build task ordering from CSV stage-order column (JSON order file removed)\n",
    "# task_order_dict = {}\n",
    "# with open(task_order_file) as f:\n",
    "#     task_order_dict = json.load(f)\n",
    "\n",
    "# print(task_order_dict)\n",
    "\n",
    "# size_scale = FLOW_SCALE\n",
    "\n",
    "# # Ensure tasks are ordered based on task_order_dict\n",
    "# unique_tasks = sorted(\n",
    "#     df['taskName'].unique(), \n",
    "#     key=lambda t: task_order_dict.get(t, {}).get('stage_order', float('inf'))\n",
    "# )\n",
    "\n",
    "# print(f\"Unique tasks: {unique_tasks}\")\n",
    "# if not unique_tasks:\n",
    "#     print(\"No tasks found, skipping plotting.\")\n",
    "#     exit()\n",
    "\n",
    "# num_tasks = len(unique_tasks)\n",
    "# palette = sns.color_palette(\"deep\", 9)  # Use 9 distinct colors\n",
    "# marker_shapes = ['o', 's', 'D', '^', 'v', 'P', '*', 'X', '<', '>', 'h', 'H', '8', 'p', 'x', '+', '1', '2', '3', '4']\n",
    "\n",
    "# task_group_count = 15\n",
    "# task_groups = [unique_tasks[i:i+task_group_count] for i in range(0, num_tasks, task_group_count)]\n",
    "\n",
    "# for group_idx, task_group in enumerate(task_groups):\n",
    "#     valid_tasks = [task for task in task_group if task in df['taskName'].values]\n",
    "#     print(f\"Group {group_idx+1}: {valid_tasks}\")\n",
    "#     if not valid_tasks:\n",
    "#         print(f\"Skipping empty group {group_idx+1}\")\n",
    "#         continue\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(4, 6))\n",
    "    \n",
    "\n",
    "#     color_map = {task: palette[i % len(palette)] for i, task in enumerate(valid_tasks)}\n",
    "#     marker_map = {task: marker_shapes[i % len(marker_shapes)] for i, task in enumerate(valid_tasks)}\n",
    "\n",
    "#     for task in valid_tasks:\n",
    "#         subset = df[df['taskName'] == task]\n",
    "#         if subset.empty:\n",
    "#             print(f\"Skipping task '{task}' (no data)\")\n",
    "#             continue\n",
    "\n",
    "#         sizes = np.log10(subset['aggregateFilesizeMB'] +1.1) * size_scale\n",
    "\n",
    "#         ax.scatter(\n",
    "#             subset['opCount'],\n",
    "#             subset['trMiB'],\n",
    "#             s=sizes,\n",
    "#             label=shorten_task_name(task),\n",
    "#             color=color_map[task],\n",
    "#             marker=marker_map[task],\n",
    "#             alpha=0.3\n",
    "#         )\n",
    "\n",
    "#     # Font size and labels\n",
    "#     plt.rc('font', size=20)\n",
    "#     plt.rc('axes', titlesize=20)\n",
    "#     plt.rc('axes', labelsize=20)\n",
    "#     plt.rc('xtick', labelsize=20)\n",
    "#     plt.rc('ytick', labelsize=20)\n",
    "#     plt.rc('legend', fontsize=20)\n",
    "\n",
    "#     ax.set_xlabel('Operation Count', labelpad=20)\n",
    "#     ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "#     ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "#     # ax.set_xscale('log')\n",
    "\n",
    "#     # First legend: Task Name (color + shape)\n",
    "#     task_legend_proxies = [\n",
    "#         plt.scatter([], [], s=100, color=color_map[task], marker=marker_map[task], alpha=0.3, label=shorten_task_name(task))\n",
    "#         for task in valid_tasks\n",
    "#     ]\n",
    "#     legend1 = ax.legend(handles=task_legend_proxies, title=\"Task Name\", loc='center', \n",
    "#                         bbox_to_anchor=(0.2, 0.58, 0.8, 0.0), framealpha=0.1, facecolor='white')\n",
    "#     ax.add_artist(legend1)\n",
    "\n",
    "#     # Second legend: Dataflow (MB)\n",
    "#     flow_data = df['aggregateFilesizeMB'].dropna().sort_values()\n",
    "#     if not flow_data.empty:\n",
    "#         min_val = flow_data.min()\n",
    "#         if min_val == 0:\n",
    "#             min_val = np.percentile(flow_data, 1)\n",
    "#         max_val = flow_data.max()\n",
    "#         mid1 = np.percentile(flow_data, 33)\n",
    "#         mid2 = np.percentile(flow_data, 66)\n",
    "#         flow_sizes = [math.ceil(min_val), int(mid1), int(mid2), int(max_val)]\n",
    "#     else:\n",
    "#         flow_sizes = [10, 50, 100, 500]  # fallback\n",
    "\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     fig.savefig(f'{result_path}/taskColor_group{group_idx+1}_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Compile regex patterns once\n",
    "compiled_patterns = [(pattern, re.compile(pattern)) for pattern in file_group_patterns]\n",
    "\n",
    "def extract_group(filename):\n",
    "    for pattern_str, pattern in compiled_patterns:\n",
    "        if pattern.search(filename):\n",
    "            return pattern_str.replace(\"\\\\\", \"\")  # Remove backslashes\n",
    "    print(f\"Filename '{filename}' did not match any patterns.\")\n",
    "    return \"group_other\"\n",
    "\n",
    "\n",
    "# Apply to dataframe\n",
    "df[\"file_group\"] = df[\"fileName\"].apply(extract_group)\n",
    "\n",
    "# Check results\n",
    "print(\"Unique file groups:\")\n",
    "print(df[\"file_group\"].value_counts())\n",
    "\n",
    "def padded_range(series, pad_ratio=0.05):\n",
    "    \"\"\"Return min and max with padding based on data range.\"\"\"\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    pad = (max_val - min_val) * pad_ratio\n",
    "    return min_val - pad, max_val + pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Generate a color palette\n",
    "unique_groups = df[\"file_group\"].unique()\n",
    "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "num_groups = len(unique_groups)\n",
    "\n",
    "# Define color palette and markers\n",
    "# Define color palette and markers\n",
    "palette = sns.color_palette(\"deep\", num_groups)\n",
    "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# Groups to exclude for 1000 genome\n",
    "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "# Remove them from unique_groups\n",
    "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "# Split file groups into groups of n\n",
    "file_group_count = 20\n",
    "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, num_groups, file_group_count)]\n",
    "\n",
    "for batch_idx, group_batch in enumerate(group_batches):\n",
    "    # Skip empty groups\n",
    "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "    if not valid_groups:\n",
    "        print(f\"Skipping empty group {batch_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate color and marker maps for current group\n",
    "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "    marker_map = {group: markers[i % len(markers)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "    # Assign colors and markers\n",
    "    df[\"color\"] = df[\"file_group\"].map(lambda group: color_map.get(group, 'black'))\n",
    "    df[\"marker\"] = df[\"file_group\"].map(lambda group: marker_map.get(group, 'o'))\n",
    "\n",
    "    # Iterate through the valid file groups\n",
    "    for group in valid_groups:\n",
    "        subset = df[df[\"file_group\"] == group]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping group '{group}' (no data)\")\n",
    "            continue\n",
    "\n",
    "        # ax.scatter(subset['trMiB'], subset['stageOrder'], subset['aggregateFilesizeMB'], \n",
    "        ax.scatter(subset['stageOrder'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "                   label=group.replace(\".*\", \"\").replace(\"^\", \"\"),  # Clean up group name\n",
    "                   color=color_map[group],  \n",
    "                   marker=marker_map[group],  \n",
    "                   s=200,\n",
    "                   alpha=0.6)\n",
    "                # Add dashed lines from each point to the x and y axes\n",
    "        for _, row in subset.iterrows():\n",
    "            x = row['stageOrder']\n",
    "            y = row['trMiB']\n",
    "            z = row['aggregateFilesizeMB']\n",
    "            # Line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color_map[group], alpha=0.3, linewidth=2)\n",
    "            # Line to y-axis (z stays same, x drops to 0)\n",
    "            # ax.plot([x, 0], [y, y], [z, z], linestyle='dashed', color=color_map[group],   alpha=0.3, linewidth=2)\n",
    "    \n",
    "    # Set font sizes\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=18)\n",
    "    plt.rc('axes', labelsize=20)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=16)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('Stage Order', labelpad=20)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "    \n",
    "    \n",
    "    # Ensure y-axis only shows integers\n",
    "    # ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    # Legend updated to reflect file groups\n",
    "    ax.legend(title=\"File Group\", loc=\"upper right\", bbox_to_anchor=(0.18, 0.82, 0.8, 0.0),\n",
    "              labelspacing=0.2, framealpha=0.6, ncol=2, columnspacing=0.1,\n",
    "              )\n",
    "\n",
    "    # plt.figure(constrained_layout=True)\n",
    "\n",
    "    # Show and save figure\n",
    "    fig.savefig(f'{result_path}/fileColor_group{batch_idx+1}_stage_'+plot_file_name)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate a color palette\n",
    "# unique_groups = df[\"file_group\"].unique()\n",
    "# print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "# # Exclude certain groups (e.g., for 1000 Genome)\n",
    "# exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "# unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "# num_groups = len(unique_groups)\n",
    "# palette = sns.color_palette(\"deep\", 9)  # Limit to 9 distinct colors\n",
    "\n",
    "# # Define a set of marker shapes\n",
    "# marker_shapes = ['o', 's', 'D', '^', 'v', 'P', '*', 'X', '<', '>', 'h', 'H', '8', 'p', 'x', '+', '1', '2', '3', '4']\n",
    "\n",
    "# # Group batches\n",
    "# file_group_count = 20\n",
    "# group_batches = [unique_groups[i:i+file_group_count] for i in range(0, len(unique_groups), file_group_count)]\n",
    "\n",
    "# size_scale = FLOW_SCALE\n",
    "\n",
    "# for batch_idx, group_batch in enumerate(group_batches):\n",
    "#     valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "#     if not valid_groups:\n",
    "#         print(f\"Skipping empty group {batch_idx+1}\")\n",
    "#         continue\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "#     color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "#     marker_map = {group: marker_shapes[i % len(marker_shapes)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "#     for group in valid_groups:\n",
    "#         subset = df[df[\"file_group\"] == group]\n",
    "#         if subset.empty:\n",
    "#             print(f\"Skipping group '{group}' (no data)\")\n",
    "#             continue\n",
    "\n",
    "#         sizes = np.log10(subset[\"aggregateFilesizeMB\"]) * size_scale\n",
    "\n",
    "#         ax.scatter(\n",
    "#             subset[\"stageOrder\"],\n",
    "#             subset[\"trMiB\"],\n",
    "#             s=sizes,\n",
    "#             label=group,\n",
    "#             color=color_map[group],\n",
    "#             marker=marker_map[group],\n",
    "#             alpha=0.3\n",
    "#         )\n",
    "\n",
    "#     # Font settings\n",
    "#     plt.rc('font', size=22)\n",
    "#     plt.rc('axes', titlesize=22)\n",
    "#     plt.rc('axes', labelsize=22)\n",
    "#     plt.rc('xtick', labelsize=20)\n",
    "#     plt.rc('ytick', labelsize=20)\n",
    "#     plt.rc('legend', fontsize=18)\n",
    "\n",
    "#     ax.set_xlabel(\"Stage Order\", labelpad=20)\n",
    "#     # ax.set_ylabel(\"I/O Bandwidth (MB/s)\", labelpad=20)\n",
    "#     # set x only show integer\n",
    "#     ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "#     ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "    \n",
    "#     # First legend: File Group with fixed-size dummy points using same color and marker\n",
    "#     scatter_proxies = [\n",
    "#         plt.scatter([], [], s=100, color=color_map[group], marker=marker_map[group], alpha=0.3, label=group.replace(\".*\",\"\").replace(\"^\",\"\"))\n",
    "#         for group in valid_groups\n",
    "#     ]\n",
    "#     # Second legend: Dataflow (MB)\n",
    "#     flow_data = df[\"aggregateFilesizeMB\"].dropna().sort_values()\n",
    "#     if not flow_data.empty:\n",
    "#         min_val = flow_data.min()\n",
    "#         if min_val == 0:\n",
    "#             min_val = np.percentile(flow_data, 1)\n",
    "#         max_val = flow_data.max()\n",
    "#         mid1 = np.percentile(flow_data, 33)\n",
    "#         mid2 = np.percentile(flow_data, 66)\n",
    "#         flow_sizes = [math.ceil(min_val), int(mid1), int(mid2), int(max_val)]\n",
    "#     else:\n",
    "#         flow_sizes = [10, 50, 100, 500]  # fallback\n",
    "\n",
    "#     # First legend: File Group colors\n",
    "#     legend1 = ax.legend(handles=scatter_proxies, title=\"File Group\", loc='center', alignment='center',\n",
    "#                         bbox_to_anchor=(0.1, 0.62, 0.8, 0.0), \n",
    "#                         framealpha=0.1, facecolor='white', \n",
    "#                         labelspacing=0.1, ncol=2, columnspacing=0.1,\n",
    "#                         )\n",
    "\n",
    "#     ax.add_artist(legend1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'{result_path}/fileColor_group{batch_idx+1}_stage_' + plot_file_name)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Generate a color palette\n",
    "unique_groups = df[\"file_group\"].unique()\n",
    "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "num_groups = len(unique_groups)\n",
    "\n",
    "# Define color palette and markers\n",
    "palette = sns.color_palette(\"deep\", num_groups)\n",
    "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# Groups to exclude for 1000 genome\n",
    "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "# Remove them from unique_groups\n",
    "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "# Split file groups into groups of n\n",
    "file_group_count = 20\n",
    "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, num_groups, file_group_count)]\n",
    "\n",
    "for batch_idx, group_batch in enumerate(group_batches):\n",
    "    # Skip empty groups\n",
    "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "    if not valid_groups:\n",
    "        print(f\"Skipping empty group {batch_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate color and marker maps for current group\n",
    "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "    marker_map = {group: markers[i % len(markers)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "    # Assign colors and markers\n",
    "    df[\"color\"] = df[\"file_group\"].map(lambda group: color_map.get(group, 'black'))\n",
    "    df[\"marker\"] = df[\"file_group\"].map(lambda group: marker_map.get(group, 'o'))\n",
    "\n",
    "    # Iterate through the valid file groups\n",
    "    for group in valid_groups:\n",
    "        subset = df[df[\"file_group\"] == group]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping group '{group}' (no data)\")\n",
    "            continue\n",
    "\n",
    "        ax.scatter(subset['opCount'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "                   label=group.replace(\".*\", \"\").replace(\"^\", \"\"),  # Clean up group name\n",
    "                   color=color_map[group],  \n",
    "                   marker=marker_map[group],  \n",
    "                   s=200)\n",
    "        for _, row in subset.iterrows():\n",
    "            x = row['opCount']\n",
    "            y = row['trMiB']\n",
    "            z = row['aggregateFilesizeMB']\n",
    "            # Line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color_map[group],\n",
    "                    alpha=0.3, linewidth=2)\n",
    "    \n",
    "    # Set font sizes\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=24)\n",
    "    plt.rc('axes', labelsize=24)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=17)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('Operation Count', labelpad=20)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "    \n",
    "    # Ensure y-axis only shows integers\n",
    "    # ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    # ax.set_xlim(df['opCount'].max() * 1.1, 0)\n",
    "\n",
    "    # Legend updated to reflect file groups\n",
    "    ax.legend(title=\"File Groups\", loc=\"right\", bbox_to_anchor=(0.08, 0.58, 0.8, 0.0),\n",
    "              framealpha=0.4,\n",
    "          columnspacing=0.1, labelspacing=0.1,\n",
    "          )\n",
    "    \n",
    "    # plt.figure(constrained_layout=True)\n",
    "    # Show and save figure\n",
    "    fig.savefig(f'{result_path}/fileColor_group{batch_idx+1}_op_'+plot_file_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate a color palette\n",
    "# unique_groups = df[\"file_group\"].unique()\n",
    "# print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "# # Exclude certain groups (e.g., for 1000 Genome)\n",
    "# exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "# unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "# num_groups = len(unique_groups)\n",
    "# palette = sns.color_palette(\"deep\", 9)  # Limit to 9 distinct colors\n",
    "\n",
    "# # Define a set of marker shapes\n",
    "# marker_shapes = ['o', 's', 'D', '^', 'v', 'P', '*', 'X', '<', '>', 'h', 'H', '8', 'p', 'x', '+', '1', '2', '3', '4']\n",
    "\n",
    "# # Group batches\n",
    "# file_group_count = 20\n",
    "# group_batches = [unique_groups[i:i+file_group_count] for i in range(0, len(unique_groups), file_group_count)]\n",
    "\n",
    "# size_scale = FLOW_SCALE\n",
    "\n",
    "# for batch_idx, group_batch in enumerate(group_batches):\n",
    "#     valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "#     if not valid_groups:\n",
    "#         print(f\"Skipping empty group {batch_idx+1}\")\n",
    "#         continue\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "#     color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "#     marker_map = {group: marker_shapes[i % len(marker_shapes)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "#     for group in valid_groups:\n",
    "#         subset = df[df[\"file_group\"] == group]\n",
    "#         if subset.empty:\n",
    "#             print(f\"Skipping group '{group}' (no data)\")\n",
    "#             continue\n",
    "\n",
    "#         sizes = np.log10(subset[\"aggregateFilesizeMB\"] +1.1) * size_scale\n",
    "\n",
    "#         ax.scatter(\n",
    "#             subset[\"opCount\"],\n",
    "#             subset[\"trMiB\"],\n",
    "#             s=sizes,\n",
    "#             label=group,\n",
    "#             color=color_map[group],\n",
    "#             marker=marker_map[group],\n",
    "#             alpha=0.3\n",
    "#         )\n",
    "\n",
    "#     # Font settings\n",
    "#     plt.rc('font', size=22)\n",
    "#     plt.rc('axes', titlesize=22)\n",
    "#     plt.rc('axes', labelsize=22)\n",
    "#     plt.rc('xtick', labelsize=20)\n",
    "#     plt.rc('ytick', labelsize=20)\n",
    "#     plt.rc('legend', fontsize=19)\n",
    "\n",
    "#     ax.set_xlabel(\"Operation Count\", labelpad=20)\n",
    "#     # ax.set_xscale('log')\n",
    "#     # ax.set_ylabel(\"I/O Bandwidth (MB/s)\", labelpad=20)\n",
    "#     # set x only show integer\n",
    "#     # ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "#     # use scientific notation for y axis\n",
    "#     ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "    \n",
    "#     # First legend: File Group with fixed-size dummy points using same color and marker\n",
    "#     scatter_proxies = [\n",
    "#         plt.scatter([], [], s=200, color=color_map[group], marker=marker_map[group], alpha=0.3, label=group.replace(\".*\",\"\").replace(\"^\",\"\"))\n",
    "#         for group in valid_groups\n",
    "#     ]\n",
    "#     # Second legend: Dataflow (MB)\n",
    "#     flow_data = df[\"aggregateFilesizeMB\"].dropna().sort_values()\n",
    "#     if not flow_data.empty:\n",
    "#         min_val = flow_data.min()\n",
    "#         if min_val == 0:\n",
    "#             min_val = np.percentile(flow_data, 1)\n",
    "#         max_val = flow_data.max()\n",
    "#         mid1 = np.percentile(flow_data, 33)\n",
    "#         mid2 = np.percentile(flow_data, 66)\n",
    "#         flow_sizes = [math.ceil(min_val), int(mid1), int(mid2), int(max_val)]\n",
    "#     else:\n",
    "#         flow_sizes = [10, 50, 100, 500]  # fallback\n",
    "\n",
    "#     # First legend: File Group colors\n",
    "#     legend1 = ax.legend(handles=scatter_proxies, title=\"File Group\", loc='center', alignment='center',\n",
    "#                         bbox_to_anchor=(0.1, 0.64, 0.8, 0.0), \n",
    "#                         framealpha=0.1, facecolor='white', \n",
    "#                         labelspacing=0.1, columnspacing=0.1,\n",
    "#                         ncol=2, fontsize=18,\n",
    "#                         )\n",
    "\n",
    "#     ax.add_artist(legend1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'{result_path}/fileColor_group{batch_idx+1}_op_' + plot_file_name)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set taskName == Diff has parallelism of 120\n",
    "df.loc[df['stageOrder'] == 2, 'parallelism'] = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_pairs = {\n",
    "    (\"initial_data\", \"Pro\") : \"1-n\",\n",
    "    (\"initial_data\", \"DiffF\") : \"1-n\",\n",
    "    (\"initial_data\", \"BgM\") : \"1-1\",\n",
    "    (\"initial_data\", \"Bg\") : \"1-n\",\n",
    "    (\"initial_data\", \"Img\") : \"1-1\",\n",
    "    (\"initial_data\", \"Add\") : \"1-1\",\n",
    "    \n",
    "    \n",
    "    (\"Pro\", \"DiffF\") : \"n-n\",\n",
    "    (\"DiffF\", \"ConF\") : \"n-1\",\n",
    "    (\"ConF\", \"BgM\") : \"1-1\",\n",
    "    (\"BgM\", \"Bg\") : \"1-n\",\n",
    "    (\"Pro\", \"Bg\") : \"n-n\",\n",
    "    (\"Bg\", \"Img\") : \"n-1\",\n",
    "    (\"Img\", \"Add\") : \"1-1\",\n",
    "    (\"Bg\", \"Add\") : \"n-1\",\n",
    "    (\"Add\", \"View\") : \"1-1\",\n",
    "    (\"View\", \"final_data\") : \"1-1\",\n",
    "}\n",
    "rows = []\n",
    "for producer, consumer in manual_pairs.keys():\n",
    "    pc_label = producer + '' + consumer\n",
    "    \n",
    "    if consumer == \"final_data\":\n",
    "        df_subset_write = df[(df['operation'] == 0) & (df['taskName'] == producer) ]\n",
    "        if df_subset_write.empty:\n",
    "            print(f\"Skipping special pair ({pc_label}): no data\")\n",
    "            continue\n",
    "        \n",
    "        if df_subset_write['parallelism'].mean() > 1:\n",
    "            subset_write_bw = df_subset_write['trMiB'].sum() if not df_subset_write.empty else 0\n",
    "        else:\n",
    "            subset_write_bw = df_subset_write['trMiB'].mean() if not df_subset_write.empty else 0\n",
    "        \n",
    "        consumer_file_group = df_subset_pc_read['file_group'].unique()\n",
    "\n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': df_subset_write['parallelism'].mean(),\n",
    "            'consumer_parallelism': 0,\n",
    "            'transferSize': df_subset_write['transferSize'].sum(),\n",
    "            'aggregateFilesizeMB': df_subset_write['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_write['totalTime'].sum(),\n",
    "            'opCount': df_subset_write['opCount'].sum(),\n",
    "            'trMiB': subset_write_bw,\n",
    "            'stageOrder_read': int(df_subset_write['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_write['stageOrder'].mean()),\n",
    "            'pcPair': pc_label,\n",
    "            'file_groups': consumer_file_group,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        continue\n",
    "    if producer == \"initial_data\":        \n",
    "        df_subset_read = df[(df['operation'] == 1) & (df['taskName'] == consumer) & (df['prevTask'] == producer)]\n",
    "        if df_subset_read.empty:\n",
    "            print(f\"Skipping special pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            # print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_read: {df_subset_read[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        \n",
    "        if df_subset_read['parallelism'].mean() > 1:\n",
    "            subset_read_bw = df_subset_read['trMiB'].sum() if not df_subset_read.empty else 0\n",
    "        else:\n",
    "            subset_read_bw = df_subset_read['trMiB'].mean() if not df_subset_read.empty else 0\n",
    "        \n",
    "        consumer_file_group = df_subset_read['file_group'].unique()\n",
    "        \n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': 0,\n",
    "            'consumer_parallelism': df_subset_read['parallelism'].mean(),\n",
    "            'transferSize': df_subset_read['transferSize'].sum(),\n",
    "            'aggregateFilesizeMB': df_subset_read['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_read['totalTime'].sum(),\n",
    "            'opCount': df_subset_read['opCount'].sum(),\n",
    "            'trMiB': subset_read_bw,\n",
    "            'stageOrder_read': int(df_subset_read['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_read['stageOrder'].mean()),\n",
    "            'pcPair': pc_label,\n",
    "            'file_groups': consumer_file_group,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        continue\n",
    "    else:\n",
    "        # Regular handling\n",
    "        df_subset_pc_read = df[(df['operation'] == 1) & (df['taskName'] == consumer) & (df['prevTask'] == producer)]\n",
    "        # check if any of the dataframes are empty\n",
    "        if df_subset_pc_read.empty:\n",
    "            print(f\"Skipping pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_pc: {df_subset_pc_read[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        # get list of unique file names\n",
    "        consumer_files = df_subset_pc_read['fileName'].unique()\n",
    "        consumer_file_group = df_subset_pc_read['file_group'].unique()\n",
    "        # get the dataframe with the same file names\n",
    "        df_subset_pc_write = df[(df['operation'] == 0) & (df['taskName'] == producer) ] # this workflow only one consumer\n",
    "        \n",
    "        if df_subset_pc_write.empty:\n",
    "            print(f\"Skipping pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_pc: {df_subset_pc_write[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        producer_parallelism = df_subset_pc_write['parallelism'].mean()\n",
    "        consumer_parallelism = df_subset_pc_read['parallelism'].mean()\n",
    "        # print(f\"Producer: {producer}, Consumer: {consumer}, Producer Parallelism: {producer_parallelism}, Consumer Parallelism: {consumer_parallelism}\")\n",
    "        \n",
    "        if producer_parallelism > 1:\n",
    "            # If parallelism > 1, use sum\n",
    "            subset_write_bw = df_subset_pc_write['trMiB'].sum() if not df_subset_pc_write.empty else 0\n",
    "        else:\n",
    "            # If parallelism <= 1, use mean\n",
    "            subset_write_bw = df_subset_pc_write['trMiB'].mean() if not df_subset_pc_write.empty else 0\n",
    "        \n",
    "        if consumer_parallelism > 1:\n",
    "            # If parallelism > 1, use sum\n",
    "            subset_read_bw = df_subset_pc_read['trMiB'].sum() if not df_subset_pc_read.empty else 0\n",
    "        else:\n",
    "            # If parallelism <= 1, use mean\n",
    "            subset_read_bw = df_subset_pc_read['trMiB'].mean() if not df_subset_pc_read.empty else 0\n",
    "\n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': int(producer_parallelism),\n",
    "            'consumer_parallelism': int(consumer_parallelism),\n",
    "            'transferSize': df_subset_pc_write['transferSize'].mean() + df_subset_pc_read['transferSize'].mean(),\n",
    "            'aggregateFilesizeMB': df_subset_pc_write['aggregateFilesizeMB'].sum() + df_subset_pc_read['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_pc_write['totalTime'].sum() + df_subset_pc_read['totalTime'].sum(),\n",
    "            'opCount': df_subset_pc_write['opCount'].sum() + df_subset_pc_read['opCount'].sum(),\n",
    "            'trMiB': (subset_write_bw + subset_read_bw) / 2,\n",
    "            'stageOrder_read': int(df_subset_pc_read['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_pc_write['stageOrder'].mean()),\n",
    "            'pcPair': pc_label,\n",
    "            'file_groups': consumer_file_group, # list of unique file groups\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df_pc = pd.DataFrame(rows)\n",
    "\n",
    "# Show summary\n",
    "print(f\"Constructed producer-consumer DataFrame with {len(df_pc)} pairs.\")\n",
    "print(df_pc[['pcPair', 'aggregateFilesizeMB', 'totalTime', 'trMiB', 'producer_parallelism', 'consumer_parallelism']])\n",
    "\n",
    "# Optional: create pair label\n",
    "df_pc['pcPair'] = df_pc['producerTask'] + '' + df_pc['consumerTask']\n",
    "\n",
    "# Get unique producer-consumer pairs\n",
    "unique_pairs = df_pc['pcPair'].unique()\n",
    "if len(unique_pairs) == 0:\n",
    "    print(\"No producer-consumer pairs available for plotting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Unique producer-consumer pairs: {unique_pairs}\")\n",
    "\n",
    "print(df_pc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== aggregateFilesizeMB Range ===\")\n",
    "print(f\"df_pc: min = {df_pc['aggregateFilesizeMB'].min():.2f}, max = {df_pc['aggregateFilesizeMB'].max():.2f}\")\n",
    "print(f\"df   : min = {df['aggregateFilesizeMB'].min():.2f}, max = {df['aggregateFilesizeMB'].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "palette = sns.color_palette(\"tab20\", len(unique_pairs))\n",
    "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h', 'v', '<', '>', '|']\n",
    "pair_group_count = 15\n",
    "pair_groups = [unique_pairs[i:i+pair_group_count] for i in range(0, len(unique_pairs), pair_group_count)]\n",
    "\n",
    "for group_idx, pair_group in enumerate(pair_groups):\n",
    "    subset_group = df_pc[df_pc['pcPair'].isin(pair_group)]\n",
    "    if subset_group.empty:\n",
    "        print(f\"Skipping empty group {group_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 8), constrained_layout=True)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Assign colors and markers\n",
    "    color_map = {pair: palette[i % len(palette)] for i, pair in enumerate(pair_group)}\n",
    "    marker_map = {pair: markers[i % len(markers)] for i, pair in enumerate(pair_group)}\n",
    "\n",
    "    seen_labels = set()  # Track labels we've already added\n",
    "\n",
    "    for _, row in subset_group.iterrows():\n",
    "        pair_label = row['pcPair']\n",
    "        producer, consumer = pair_label.split('')\n",
    "        short_producer = shorten_task_name(producer)\n",
    "        short_consumer = shorten_task_name(consumer)\n",
    "\n",
    "        # Fetch pattern from manual_pairs\n",
    "        pattern = manual_pairs.get((producer, consumer), \"unknown\")\n",
    "        producer_parallelism = int(row['producer_parallelism'])\n",
    "        consumer_parallelism = int(row['consumer_parallelism'])\n",
    "        if \"gettr\" in producer:\n",
    "            producer_parallelism = 1\n",
    "        if \"gettr\" in consumer:\n",
    "            consumer_parallelism = 1\n",
    "            \n",
    "        # short_label = f\"{short_producer}({producer_parallelism}){short_consumer}({consumer_parallelism}) [{pattern}]\"\n",
    "        short_label = f\"{short_producer}({producer_parallelism})[\"\n",
    "        if short_producer == \"freq\" or short_producer == \"mut_olp\":\n",
    "            short_label += f\"chr-.tar.gz,\"\n",
    "        else:\n",
    "            for file_group in row['file_groups']:\n",
    "                if \"region\" in file_group:\n",
    "                    short_label += f\"{file_group},\".replace(\"_.*\", \"\").replace(\".*\",\"\").replace(\"^\", \"\")\n",
    "                else:\n",
    "                    short_label += f\"{file_group},\".replace(\"_.*\", \"\").replace(\".*\",\"\").replace(\"^\", \"\")\n",
    "        # replace the last char with ]\n",
    "        short_label = short_label[:-1] + \"]\"\n",
    "        short_label += f\"{short_consumer}({consumer_parallelism}) [{pattern}]\"\n",
    "\n",
    "        show_label = pair_label not in seen_labels\n",
    "        seen_labels.add(pair_label)\n",
    "\n",
    "        trMiB = row['trMiB'] if pd.notna(row['trMiB']) else 0\n",
    "        opCount = row['opCount'] if pd.notna(row['opCount']) else 0\n",
    "        filesize = row['aggregateFilesizeMB'] if pd.notna(row['aggregateFilesizeMB']) else 0\n",
    "\n",
    "        ax.scatter(\n",
    "            opCount, trMiB, filesize,\n",
    "            label=short_label if show_label else None,\n",
    "            color=color_map[pair_label],\n",
    "            marker=marker_map[pair_label],\n",
    "            s=200\n",
    "        )\n",
    "        ax.plot([opCount, opCount], [trMiB, trMiB], [0, filesize], linestyle='dashed', color=color_map[pair_label], linewidth=2)\n",
    "        # ax.plot([opCount, opCount], [0, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=2)\n",
    "        ax.plot([0, opCount], [trMiB, trMiB], [filesize, filesize], linestyle='dashed', color=color_map[pair_label], linewidth=2)\n",
    "\n",
    "\n",
    "\n",
    "    # Styling\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 16,\n",
    "        'axes.titlesize': 16,\n",
    "        'axes.labelsize': 16,\n",
    "        'xtick.labelsize': 18,\n",
    "        'ytick.labelsize': 18,\n",
    "        'legend.fontsize': 22,\n",
    "        'legend.title_fontsize': 20,\n",
    "    })\n",
    "\n",
    "    # set aspect ratio\n",
    "    ax.set_box_aspect([1, 1, 1.5])\n",
    "\n",
    "    ax.set_xlabel('Op Count', labelpad=10)\n",
    "    ax.set_ylabel('I/O BW (MB/s)', labelpad=20)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=20)\n",
    "\n",
    "    # set zlim to 0 to max * 1.1\n",
    "    # ax.set_zlim(0, df_pc['aggregateFilesizeMB'].max() * 1.2)\n",
    "\n",
    "    # Reduce major tick count on all axes for readability\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.zaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "\n",
    "    # Combine all handles/labels into one legend outside the plot (right side)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    legend = ax.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        title=\"Producer  [Files]  Consumer\",\n",
    "        loc=\"upper right\",\n",
    "        framealpha=0.1,\n",
    "        facecolor='white',\n",
    "        labelspacing=0.4,\n",
    "        alignment='center',\n",
    "        borderaxespad=0.0,\n",
    "        bbox_to_anchor=(0.95, 0.92),  # pushes legend to the right of the plot\n",
    "    )\n",
    "    ax.add_artist(legend)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    ax.set_box_aspect([1, 1, 1.3])\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f'{result_path}/pcAgg_group{group_idx+1}_{plot_file_name}')\n",
    "    \n",
    "    # print out all data points in this graph\n",
    "    print(subset_group[['pcPair', 'aggregateFilesizeMB', 'totalTime', 'trMiB', 'producer_parallelism', 'consumer_parallelism', 'opCount']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['operation', 'taskName', 'prevTask', 'fileName']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "x = df['trMiB']\n",
    "y = df['opCount']\n",
    "z = df['aggregateFilesizeMB']\n",
    "\n",
    "# Generate a color palette\n",
    "unique_tasks = df['taskName'].unique()\n",
    "num_tasks = len(unique_tasks)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=24)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of tasks\n",
    "num_cols = min(3, num_tasks)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_tasks / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_tasks)]\n",
    "\n",
    "for i, (task, (row, col)) in enumerate(zip(unique_tasks, task_indices)):\n",
    "    subset = df[df['taskName'] == task]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['opCount'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "               label=shorten_task_name(task), \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {task}')\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_xlabel('Operation Count', labelpad=13)\n",
    "    ax.set_zlabel('Flow Size (MB)', labelpad=10)\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/tasksubplot_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "# Generate a color palette\n",
    "unique_filegroup = df['file_group'].unique()\n",
    "num_filegroups = len(unique_filegroup)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=20)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of unique file groups\n",
    "num_cols = min(3, num_filegroups)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_filegroups / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_filegroups)]\n",
    "\n",
    "for i, (file, (row, col)) in enumerate(zip(unique_filegroup, task_indices)):\n",
    "    subset = df[df['file_group'] == file]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['trMiB'], subset['opCount'], subset['aggregateFilesizeMB'], \n",
    "               label=file, \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {file}')\n",
    "    ax.set_xlabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_ylabel('Operation Count', labelpad=13)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/filegroup_subplot_op_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Generate a color palette\n",
    "unique_filegroup = df['file_group'].unique()\n",
    "num_filegroups = len(unique_filegroup)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=20)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of unique file groups\n",
    "num_cols = min(3, num_filegroups)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_filegroups / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_filegroups)]\n",
    "\n",
    "for i, (file, (row, col)) in enumerate(zip(unique_filegroup, task_indices)):\n",
    "    subset = df[df['file_group'] == file]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['stageOrder'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "               label=file, \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {file}')\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_xlabel('Stage Order', labelpad=13)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/filegroup_subplot_stage_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns, df['stageOrder'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
