{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "project_root = \".\"\n",
    "\n",
    "pattern_configs = {\n",
    "    \"ptychonn\":{\n",
    "        \"task_order_list\" : f\"{project_root}/ptychonn/ptychonn_script_order.json\",\n",
    "        \"csv_file_path\" : f'{project_root}/workflow_data/ptychonn_14m_tr_estimated.csv',\n",
    "        \"plot_file_name\" : 'ptychonn_3d_relationship_files.pdf',\n",
    "        \"file_group_patterns\" : [\n",
    "            \".*amp_pha_10nm_full.npy\",\n",
    "            \".*diff.npz\",\n",
    "            \".*_test.npy\",\n",
    "            \".*_train.npy\",\n",
    "            \".*_test.npy\",\n",
    "            \".*_train.npy\",\n",
    "            \".*.weights.h5\",\n",
    "            \"Y_test_pred.npy\"\n",
    "        ],\n",
    "        \"result_path\": f\"./ptychonn_plots\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define file grouping file_group_patterns\n",
    "\n",
    "\n",
    "\n",
    "def shorten_task_name(name):\n",
    "    parts = name.split(\"_\")\n",
    "    # print(f\"Original parts: {parts}\")\n",
    "    shortened = [p[:10] for i, p in enumerate(parts) if i % 2 == 0 or i == len(parts) - 1]\n",
    "    # print(f\"Shortened: {shortened}\")\n",
    "    return \"_\".join(shortened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read CSV file\n",
    "# csv_file_path = '{project_root}/workflow_data/par_9000_1n_pfs_ps300_fixed.csv'\n",
    "# plot_file_name = '9000_3d_relationship_files.pdf'\n",
    "\n",
    "# csv_file_path = '{project_root}/workflow_data/summer_sam_4n_pfs_s9_tr_estimated.csv'\n",
    "# plot_file_name = 'pyflex_3d_relationship_files.pdf'\n",
    "\n",
    "CURR_WF = \"ptychonn\" # pyflex, ddmd, 1kgenome\n",
    "\n",
    "csv_file_path = pattern_configs[CURR_WF][\"csv_file_path\"]\n",
    "plot_file_name = pattern_configs[CURR_WF][\"plot_file_name\"]\n",
    "file_group_patterns = pattern_configs[CURR_WF][\"file_group_patterns\"]\n",
    "extension_grouping = pattern_configs[CURR_WF].get(\"extension_grouping\", False)\n",
    "result_path = pattern_configs[CURR_WF].get(\"result_path\", \"./result_plots\")\n",
    "\n",
    "# make result_path if it doesn't exist\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size, df.shape, df.columns, df['operation'].unique()\n",
    "\n",
    "# Replace with shortened task name in all taskName columns\n",
    "short_taskName_dict = {\n",
    "    \"preprocess_data\": \"preproc\",\n",
    "    \"PtyNN_train\": \"train\",\n",
    "    \"PtyNN_inference\": \"inference\",\n",
    "}\n",
    "\n",
    "# Replace task names in the DataFrame\n",
    "df['taskName'] = df['taskName'].replace(short_taskName_dict)\n",
    "# Replace task names in the DataFrame\n",
    "df['prevTask'] = df['prevTask'].replace(short_taskName_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Dataframe Info:\n",
    "\n",
    "# # of unique Stages\n",
    "unique_stages = df['stageOrder'].unique()\n",
    "# # of unique Tasks\n",
    "unique_tasks = df['taskName'].unique()\n",
    "# # Task instance count based on dominant I/O type\n",
    "task_instance_counts = (\n",
    "    df.groupby(['taskName', 'operation'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .max(axis=1)\n",
    ")\n",
    "\n",
    "# If needed as a total count\n",
    "total_task_instances = task_instance_counts.sum()\n",
    "\n",
    "# # of I/O entries (rows in the DataFrame)\n",
    "num_io_entries = df.shape[0]\n",
    "# # of unique I/O files\n",
    "unique_files = df['fileName'].unique()\n",
    "# # average file resues: calculate the average number of times each file is reused for each unique file\n",
    "avg_file_reuses = df.groupby('fileName').size().mean()\n",
    "# Total I/O size\n",
    "total_io_size = df['aggregateFilesizeMB'].sum()\n",
    "\n",
    "# Print all the information\n",
    "print(\"=========================================\")\n",
    "print(f\"Number of unique stages: {len(unique_stages)}\")\n",
    "# print(f\"Number of unique tasks: {len(unique_tasks)}\")\n",
    "print(f\"Total task instances: {total_task_instances}\")\n",
    "print(f\"Total I/O size (MB): {total_io_size:.2f}\")\n",
    "print(f\"Total I/O size (GB): {total_io_size / 1024:.2f}\")\n",
    "# print(f\"Number of I/O entries: {num_io_entries}\")\n",
    "print(f\"Number of unique I/O files: {len(unique_files)}\")\n",
    "print(f\"Average file reuses: {avg_file_reuses:.2f}\")\n",
    "print(\"=========================================\")\n",
    "# Print unique stages and tasks and File names\n",
    "print(f\"Unique stages: {unique_stages}\")\n",
    "print(f\"Unique tasks: {unique_tasks}\")\n",
    "print(f\"Unique file names: {unique_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add new columns for colors and labels based on the 'operation' column\n",
    "# df['color'] = df['operation'].map({0: 'blue', 1: 'red'})\n",
    "# df['label'] = df['operation'].map({0: 'write', 1: 'read'})\n",
    "# df['marker'] = df['operation'].map({0: 'x', 1: 'o'})  # 'o' for circle, 's' for square\n",
    "\n",
    "# # 2D plot: Relationship between aggregateFilesizeMB and totalTime\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# for label in df['label'].unique():\n",
    "#     subset = df[df['label'] == label]\n",
    "#     marker = subset['marker'].iloc[0]\n",
    "#     facecolor = 'none' if marker == 'o' else subset['color'].iloc[0]\n",
    "#     plt.scatter(subset['trMiB'], subset['opCount'], \n",
    "#                 label=label, \n",
    "#                 edgecolor=subset['color'].iloc[0],  # Edge color for circle markers\n",
    "#                 facecolors=facecolor,  # Only the circle markers will be hollow\n",
    "#                 marker=marker,\n",
    "#                 s=80,\n",
    "#                 alpha=0.5)\n",
    "\n",
    "# plt.title('Relationship between Transfer Size and Total Time')\n",
    "# plt.xlabel('I/O Bandwidth (MB/s)')\n",
    "# # plt.xlabel('Data size (MB)')\n",
    "# plt.ylabel('Operation count')\n",
    "# plt.grid(True)\n",
    "# plt.legend(title=\"Operation\")\n",
    "# plt.show()\n",
    "\n",
    "# Define mapping of operation and randomness to I/O type\n",
    "def map_io_type(row):\n",
    "    if row['operation'] == 0 and row['randomOffset'] == 0:\n",
    "        return 'sequential write'\n",
    "    elif row['operation'] == 0 and row['randomOffset'] == 1:\n",
    "        return 'random write'\n",
    "    elif row['operation'] == 1 and row['randomOffset'] == 0:\n",
    "        return 'sequential read'\n",
    "    elif row['operation'] == 1 and row['randomOffset'] == 1:\n",
    "        return 'random read'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply the mapping\n",
    "df['io_type'] = df.apply(map_io_type, axis=1)\n",
    "\n",
    "# Assign colors and markers to each I/O type\n",
    "io_color_map = {\n",
    "    'sequential write': 'blue',\n",
    "    'random write': 'purple',\n",
    "    'sequential read': 'green',\n",
    "    'random read': 'orange',\n",
    "}\n",
    "io_marker_map = {\n",
    "    'sequential write': 'x',\n",
    "    'random write': 'D',\n",
    "    'sequential read': 'o',\n",
    "    'random read': 's',\n",
    "}\n",
    "\n",
    "df['color'] = df['io_type'].map(io_color_map)\n",
    "df['marker'] = df['io_type'].map(io_marker_map)\n",
    "\n",
    "# 2D plot: Relationship between I/O Bandwidth and Operation Count\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for io_type in df['io_type'].unique():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    marker = subset['marker'].iloc[0]\n",
    "    facecolor = 'none' if marker == 'o' else subset['color'].iloc[0]\n",
    "    plt.scatter(\n",
    "        subset['trMiB'], \n",
    "        subset['opCount'], \n",
    "        label=io_type, \n",
    "        edgecolor=subset['color'].iloc[0], \n",
    "        facecolors=facecolor,\n",
    "        marker=marker,\n",
    "        s=80,\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "plt.title('Relationship between I/O Bandwidth and Operation Count')\n",
    "plt.xlabel('I/O Bandwidth (MB/s)')\n",
    "plt.ylabel('Operation Count')\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"I/O Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot: Relationship between aggregateFilesizeMB, opCount, and trMiB\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot actual data for present I/O types\n",
    "for io_type, color in io_color_map.items():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    marker = io_marker_map[io_type]\n",
    "    facecolor = 'none' if marker == 'o' else color\n",
    "\n",
    "    if not subset.empty:\n",
    "        ax.scatter(\n",
    "            subset['opCount'],\n",
    "            subset['trMiB'],\n",
    "            subset['aggregateFilesizeMB'],\n",
    "            label=io_type,\n",
    "            edgecolor=color,\n",
    "            facecolors=facecolor,\n",
    "            marker=marker,\n",
    "            s=80,\n",
    "            alpha=0.5\n",
    "        )\n",
    "    else:\n",
    "        # Add a dummy scatter for missing data to include in legend\n",
    "        ax.scatter([], [], [], \n",
    "                   label=io_type,\n",
    "                   edgecolor=color,\n",
    "                   facecolors='none' if marker == 'o' else color,\n",
    "                   marker=marker,\n",
    "                   s=80,\n",
    "                   alpha=0.5)\n",
    "\n",
    "# Font size settings\n",
    "plt.rc('font', size=20)\n",
    "plt.rc('axes', titlesize=24)\n",
    "plt.rc('axes', labelsize=24)\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rc('legend', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('Operation Count', labelpad=20)\n",
    "ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "ax.set_zlabel('Flow Size (MB)', labelpad=20)\n",
    "\n",
    "ax.legend(title=\"I/O Type\", loc=\"right\", bbox_to_anchor=(0.55, 0.7, 0.4, 0.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f\"{result_path}/op_{plot_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot: Relationship between aggregateFilesizeMB, stageOrder, and trMiB\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot for each defined I/O type, whether or not data exists\n",
    "for io_type, color in io_color_map.items():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    marker = io_marker_map[io_type]\n",
    "    facecolor = 'none' if marker == 'o' else color\n",
    "\n",
    "    if not subset.empty:\n",
    "        ax.scatter(\n",
    "            subset['stageOrder'],\n",
    "            subset['trMiB'],\n",
    "            subset['aggregateFilesizeMB'],\n",
    "            label=io_type,\n",
    "            edgecolor=color,\n",
    "            facecolors=facecolor,\n",
    "            marker=marker,\n",
    "            s=80,\n",
    "            alpha=0.5\n",
    "        )\n",
    "    else:\n",
    "        # Dummy point for legend\n",
    "        ax.scatter([], [], [],\n",
    "                   label=io_type,\n",
    "                   edgecolor=color,\n",
    "                   facecolors=facecolor,\n",
    "                   marker=marker,\n",
    "                   s=80,\n",
    "                   alpha=0.5)\n",
    "\n",
    "# Font size settings\n",
    "plt.rc('font', size=20)\n",
    "plt.rc('axes', titlesize=24)\n",
    "plt.rc('axes', labelsize=24)\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rc('legend', fontsize=20)\n",
    "\n",
    "# Switched axes\n",
    "ax.set_xlabel('Stage Order', labelpad=20)\n",
    "ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "ax.set_zlabel('Flow Size (MB)', labelpad=20)\n",
    "\n",
    "ax.legend(title=\"I/O Type\", loc=\"right\", bbox_to_anchor=(0.55, 0.7, 0.4, 0.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f\"{result_path}/stage_{plot_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Build task ordering from CSV stage-order column (JSON order file removed)\n",
    "stage_col = 'stage_order' if 'stage_order' in df.columns else ('stageOrder' if 'stageOrder' in df.columns else None)\n",
    "if stage_col is not None:\n",
    "    stage_order_df = df[['taskName', stage_col]].dropna(subset=['taskName', stage_col]).copy()\n",
    "    stage_order_df[stage_col] = pd.to_numeric(stage_order_df[stage_col], errors='coerce')\n",
    "    stage_order_df = stage_order_df.dropna(subset=[stage_col]).drop_duplicates(subset=['taskName'], keep='first')\n",
    "    task_stage_order = stage_order_df.set_index('taskName')[stage_col].to_dict()\n",
    "    print(f\"Loaded stage order for {len(task_stage_order)} tasks from CSV column '{stage_col}': {csv_file_path}\")\n",
    "else:\n",
    "    task_stage_order = {}\n",
    "    print(\"Warning: no stage order column found in CSV ('stage_order' or 'stageOrder'). Falling back to default task ordering.\")\n",
    "\n",
    "# Ensure tasks are ordered based on stage order from CSV\n",
    "unique_tasks = sorted(\n",
    "    df['taskName'].dropna().unique(), \n",
    "    key=lambda t: task_stage_order.get(t, float('inf'))\n",
    ")\n",
    "\n",
    "# Debugging output\n",
    "print(f\"Unique tasks: {unique_tasks}\")  \n",
    "if not unique_tasks:\n",
    "    print(\"No tasks found, skipping plotting.\")\n",
    "    exit()\n",
    "\n",
    "num_tasks = len(unique_tasks)\n",
    "\n",
    "# Define color palette and markers\n",
    "palette = sns.color_palette(\"deep\", num_tasks)\n",
    "# markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# Split tasks into groups of 5\n",
    "task_group_count = 10\n",
    "task_groups = [unique_tasks[i:i+task_group_count] for i in range(0, num_tasks, task_group_count)]\n",
    "\n",
    "for group_idx, task_group in enumerate(task_groups):\n",
    "\n",
    "\n",
    "    # Filter out empty groups\n",
    "    valid_tasks = [task for task in task_group if task in df['taskName'].values]\n",
    "    print(f\"Group {group_idx+1}: {valid_tasks}\")\n",
    "    if not valid_tasks:\n",
    "        print(f\"Skipping empty group {group_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    # 3D plot: Relationship between aggregateFilesizeMB, totalTime, and trMiB\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    x = df['trMiB']\n",
    "    y = df['opCount']\n",
    "    z = df['aggregateFilesizeMB']\n",
    "    markers = ['+', 'x', '3', '2', '*']  # Add more markers as needed\n",
    "\n",
    "    # Generate color and marker maps for current group\n",
    "    color_map = {task: palette[i % len(palette)] for i, task in enumerate(valid_tasks)}\n",
    "    marker_map = {task: markers[i % len(markers)] for i, task in enumerate(valid_tasks)}\n",
    "\n",
    "    df['color'] = df['taskName'].map(lambda task: color_map.get(task, 'black'))  # Black if not found\n",
    "    df['marker'] = df['taskName'].map(lambda task: marker_map.get(task, 'o'))  # 'o' if not found\n",
    "\n",
    "    for task in valid_tasks:\n",
    "        subset = df[df['taskName'] == task]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping task '{task}' (no data)\")\n",
    "            continue\n",
    "        \n",
    "        opCount = subset['opCount']\n",
    "        trMiB = subset['trMiB']\n",
    "        filesize = subset['aggregateFilesizeMB']\n",
    "\n",
    "        ax.scatter(opCount, trMiB, filesize, \n",
    "                   label=shorten_task_name(task), \n",
    "                   color=color_map[task], \n",
    "                   marker=marker_map[task],\n",
    "                   s=120)\n",
    "\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=24)\n",
    "    plt.rc('axes', labelsize=24)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=20)\n",
    "    \n",
    "    \n",
    "    ax.set_xlabel('Operation Count', labelpad=20)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "    ax.set_zlabel('Flow Size (MB)', labelpad=20)\n",
    "\n",
    "    ax.legend(title=\"Task Name\", loc=\"right\", bbox_to_anchor=(0.05, 0.68, 0.8, 0.0)) # (0, 0.83, 0.8, 0.0) (0, 0.6, 1.3, 0.0)\n",
    "    fig.tight_layout()\n",
    "    # plt.figure(constrained_layout=True)\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f'{result_path}/taskColor_group{group_idx+1}_'+plot_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Compile regex patterns once\n",
    "compiled_patterns = [(pattern, re.compile(pattern)) for pattern in file_group_patterns]\n",
    "\n",
    "def extract_group(filename):\n",
    "    for pattern_str, pattern in compiled_patterns:\n",
    "        if pattern.search(filename):\n",
    "            return pattern_str.replace(\"\\\\\", \"\")  # Remove backslashes\n",
    "    \n",
    "    # print (f\"Filename '{filename}' did not match any patterns.\")\n",
    "    print(f\"Filename '{filename}' did not match any patterns.\")\n",
    "    return \"group_other\"\n",
    "\n",
    "\n",
    "# Apply to dataframe\n",
    "df[\"file_group\"] = df[\"fileName\"].apply(extract_group)\n",
    "\n",
    "# Check results\n",
    "print(\"Unique file groups:\")\n",
    "print(df[\"file_group\"].value_counts())\n",
    "\n",
    "def padded_range(series, pad_ratio=0.05):\n",
    "    \"\"\"Return min and max with padding based on data range.\"\"\"\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    pad = (max_val - min_val) * pad_ratio\n",
    "    return min_val - pad, max_val + pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Generate a color palette\n",
    "unique_groups = df[\"file_group\"].unique()\n",
    "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "num_groups = len(unique_groups)\n",
    "\n",
    "# Define color palette and markers\n",
    "# Define color palette and markers\n",
    "palette = sns.color_palette(\"deep\", num_groups)\n",
    "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# Groups to exclude for 1000 genome\n",
    "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "# Remove them from unique_groups\n",
    "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "# Split file groups into groups of n\n",
    "file_group_count = 20\n",
    "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, num_groups, file_group_count)]\n",
    "\n",
    "for batch_idx, group_batch in enumerate(group_batches):\n",
    "    # Skip empty groups\n",
    "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "    if not valid_groups:\n",
    "        print(f\"Skipping empty group {batch_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate color and marker maps for current group\n",
    "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "    marker_map = {group: markers[i % len(markers)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "    # Assign colors and markers\n",
    "    df[\"color\"] = df[\"file_group\"].map(lambda group: color_map.get(group, 'black'))\n",
    "    df[\"marker\"] = df[\"file_group\"].map(lambda group: marker_map.get(group, 'o'))\n",
    "\n",
    "    # Iterate through the valid file groups\n",
    "    for group in valid_groups:\n",
    "        subset = df[df[\"file_group\"] == group]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping group '{group}' (no data)\")\n",
    "            continue\n",
    "\n",
    "        # ax.scatter(subset['trMiB'], subset['stageOrder'], subset['aggregateFilesizeMB'], \n",
    "        ax.scatter(subset['stageOrder'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "                   label=group, \n",
    "                   color=color_map[group],  \n",
    "                   marker=marker_map[group],  \n",
    "                   s=100)\n",
    "    \n",
    "    # Set font sizes\n",
    "    plt.rc('font', size=22)\n",
    "    plt.rc('axes', titlesize=24)\n",
    "    plt.rc('axes', labelsize=24)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=16)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('Stage Order', labelpad=20)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "    ax.set_zlabel('Flow Size (MB)', labelpad=20)\n",
    "    \n",
    "    # Ensure y-axis only shows integers\n",
    "    ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    # Legend updated to reflect file groups\n",
    "    ax.legend(title=\"File Group\", loc=\"best\", bbox_to_anchor=(0.2, 0.35, 0.4, 0.0))\n",
    "\n",
    "    # plt.figure(constrained_layout=True)\n",
    "\n",
    "    # Show and save figure\n",
    "    fig.savefig(f'{result_path}/fileColor_group{batch_idx+1}_stage_'+plot_file_name)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Generate a color palette\n",
    "unique_groups = df[\"file_group\"].unique()\n",
    "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "num_groups = len(unique_groups)\n",
    "\n",
    "# Define color palette and markers\n",
    "palette = sns.color_palette(\"deep\", num_groups)\n",
    "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# Groups to exclude for 1000 genome\n",
    "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "# Remove them from unique_groups\n",
    "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "# Split file groups into groups of n\n",
    "file_group_count = 20\n",
    "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, num_groups, file_group_count)]\n",
    "\n",
    "for batch_idx, group_batch in enumerate(group_batches):\n",
    "    # Skip empty groups\n",
    "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "    if not valid_groups:\n",
    "        print(f\"Skipping empty group {batch_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate color and marker maps for current group\n",
    "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "    marker_map = {group: markers[i % len(markers)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "    # Assign colors and markers\n",
    "    df[\"color\"] = df[\"file_group\"].map(lambda group: color_map.get(group, 'black'))\n",
    "    df[\"marker\"] = df[\"file_group\"].map(lambda group: marker_map.get(group, 'o'))\n",
    "\n",
    "    # Iterate through the valid file groups\n",
    "    for group in valid_groups:\n",
    "        subset = df[df[\"file_group\"] == group]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping group '{group}' (no data)\")\n",
    "            continue\n",
    "\n",
    "        ax.scatter(subset['opCount'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "                   label=group, \n",
    "                   color=color_map[group],  \n",
    "                   marker=marker_map[group],  \n",
    "                   s=100)\n",
    "    \n",
    "    # Set font sizes\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=24)\n",
    "    plt.rc('axes', labelsize=24)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=16)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('Operation Count', labelpad=20)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "    ax.set_zlabel('Flow Size (MB)', labelpad=20)\n",
    "    \n",
    "    # Ensure y-axis only shows integers\n",
    "    # ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    # ax.set_xlim(df['opCount'].max() * 1.1, 0)\n",
    "\n",
    "    # Legend updated to reflect file groups\n",
    "    ax.legend(title=\"File Group\", loc=\"best\", bbox_to_anchor=(0.28, 0.37, 0.4, 0.0))\n",
    "    \n",
    "    # plt.figure(constrained_layout=True)\n",
    "    # Show and save figure\n",
    "    fig.savefig(f'{result_path}/fileColor_group{batch_idx+1}_op_'+plot_file_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_pairs = {\n",
    "    (\"initial_data\", \"preproc\") : \"n-1\",\n",
    "    (\"preproc\", \"train\") : \"n-1\",\n",
    "    (\"train\", \"inference\") : \"1-1\",\n",
    "    (\"inference\", \"final_data\") : \"1-1\",\n",
    "}\n",
    "\n",
    "# Split into read and write operations\n",
    "df_reads = df[df['operation'] == 1].copy()\n",
    "df_writes = df[df['operation'] == 0].copy()\n",
    "\n",
    "# Aggregate metrics per task\n",
    "read_metrics = df_reads.groupby('taskName').agg({\n",
    "    'transferSize': 'sum',\n",
    "    'aggregateFilesizeMB': 'sum',\n",
    "    'totalTime': 'sum',\n",
    "    'opCount': 'sum',\n",
    "    'trMiB': 'mean',\n",
    "    'stageOrder': 'first'\n",
    "}).rename(columns=lambda x: x + '_read')\n",
    "\n",
    "write_metrics = df_writes.groupby('taskName').agg({\n",
    "    'transferSize': 'sum',\n",
    "    'aggregateFilesizeMB': 'sum',\n",
    "    'totalTime': 'sum',\n",
    "    'opCount': 'sum',\n",
    "    'trMiB': 'mean',\n",
    "    'stageOrder': 'first'\n",
    "}).rename(columns=lambda x: x + '_write')\n",
    "\n",
    "# Combine based on your manual producer-consumer task pairs\n",
    "rows = []\n",
    "for producer, consumer in manual_pairs.keys():\n",
    "    pc_label = producer + '→' + consumer\n",
    "    \n",
    "    if consumer == \"final_data\":\n",
    "        df_subset_write = df[(df['operation'] == 0) & (df['taskName'] == producer) & (df['prevTask'] == consumer)]\n",
    "        if df_subset_write.empty:\n",
    "            print(f\"Skipping special pair ({pc_label}): no data\")\n",
    "            continue\n",
    "        \n",
    "        if df_subset_write['parallelism'].mean() > 1:\n",
    "            subset_write_bw = df_subset_write['trMiB'].sum() if not df_subset_write.empty else 0\n",
    "        else:\n",
    "            subset_write_bw = df_subset_write['trMiB'].mean() if not df_subset_write.empty else 0\n",
    "\n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': df_subset_write['parallelism'].mean(),\n",
    "            'consumer_parallelism': 0,\n",
    "            'transferSize': df_subset_write['transferSize'].sum(),\n",
    "            'aggregateFilesizeMB': df_subset_write['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_write['totalTime'].sum(),\n",
    "            'opCount': df_subset_write['opCount'].sum(),\n",
    "            'trMiB': subset_write_bw,\n",
    "            'stageOrder_read': int(df_subset_write['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_write['stageOrder'].mean()),\n",
    "            'pcPair': pc_label\n",
    "        }\n",
    "        rows.append(row)\n",
    "        continue\n",
    "    if producer == \"initial_data\":        \n",
    "        df_subset_read = df[(df['operation'] == 1) & (df['taskName'] == consumer) & (df['prevTask'] == producer)]\n",
    "        if df_subset_read.empty:\n",
    "            print(f\"Skipping special pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            # print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_read: {df_subset_read[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        \n",
    "        if df_subset_read['parallelism'].mean() > 1:\n",
    "            subset_read_bw = df_subset_read['trMiB'].sum() if not df_subset_read.empty else 0\n",
    "        else:\n",
    "            subset_read_bw = df_subset_read['trMiB'].mean() if not df_subset_read.empty else 0\n",
    "        \n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': 0,\n",
    "            'consumer_parallelism': df_subset_read['parallelism'].mean(),\n",
    "            'transferSize': df_subset_read['transferSize'].sum(),\n",
    "            'aggregateFilesizeMB': df_subset_read['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_read['totalTime'].sum(),\n",
    "            'opCount': df_subset_read['opCount'].sum(),\n",
    "            'trMiB': subset_read_bw,\n",
    "            'stageOrder_read': int(df_subset_read['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_read['stageOrder'].mean()),\n",
    "            'pcPair': pc_label\n",
    "        }\n",
    "        rows.append(row)\n",
    "        continue\n",
    "    else:\n",
    "        # Regular handling\n",
    "        df_subset_pc_read = df[(df['operation'] == 1) & (df['taskName'] == consumer) & (df['prevTask'] == producer)]\n",
    "        # check if any of the dataframes are empty\n",
    "        if df_subset_pc_read.empty:\n",
    "            print(f\"Skipping pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_pc: {df_subset_pc_read[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        # get list of unique file names\n",
    "        consumer_files = df_subset_pc_read['fileName'].unique()\n",
    "        # get the dataframe with the same file names\n",
    "        df_subset_pc_write = df[(df['operation'] == 0) & (df['taskName'] == producer) & (df['fileName'].isin(consumer_files))]\n",
    "        \n",
    "        if df_subset_pc_write.empty:\n",
    "            print(f\"Skipping pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_pc: {df_subset_pc_write[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        producer_parallelism = df_subset_pc_write['parallelism'].mean()\n",
    "        consumer_parallelism = df_subset_pc_read['parallelism'].mean()\n",
    "        # print(f\"Producer: {producer}, Consumer: {consumer}, Producer Parallelism: {producer_parallelism}, Consumer Parallelism: {consumer_parallelism}\")\n",
    "        \n",
    "        if producer_parallelism > 1:\n",
    "            # If parallelism > 1, use sum\n",
    "            subset_write_bw = df_subset_pc_write['trMiB'].sum() if not df_subset_pc_write.empty else 0\n",
    "        else:\n",
    "            # If parallelism <= 1, use mean\n",
    "            subset_write_bw = df_subset_pc_write['trMiB'].mean() if not df_subset_pc_write.empty else 0\n",
    "        \n",
    "        if consumer_parallelism > 1:\n",
    "            # If parallelism > 1, use sum\n",
    "            subset_read_bw = df_subset_pc_read['trMiB'].sum() if not df_subset_pc_read.empty else 0\n",
    "        else:\n",
    "            # If parallelism <= 1, use mean\n",
    "            subset_read_bw = df_subset_pc_read['trMiB'].mean() if not df_subset_pc_read.empty else 0\n",
    "\n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': producer_parallelism,\n",
    "            'consumer_parallelism': consumer_parallelism,\n",
    "            'transferSize': df_subset_pc_write['transferSize'].mean() + df_subset_pc_read['transferSize'].mean(),\n",
    "            'aggregateFilesizeMB': df_subset_pc_write['aggregateFilesizeMB'].sum() + df_subset_pc_read['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_pc_write['totalTime'].sum() + df_subset_pc_read['totalTime'].sum(),\n",
    "            'opCount': df_subset_pc_write['opCount'].sum() + df_subset_pc_read['opCount'].sum(),\n",
    "            'trMiB': (subset_write_bw + subset_read_bw) / 2,\n",
    "            'stageOrder_read': int(df_subset_pc_read['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_pc_write['stageOrder'].mean()),\n",
    "            'pcPair': pc_label\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df_pc = pd.DataFrame(rows)\n",
    "\n",
    "# Show summary\n",
    "print(f\"Constructed producer-consumer DataFrame with {len(df_pc)} pairs.\")\n",
    "print(df_pc[['pcPair', 'aggregateFilesizeMB', 'totalTime', 'trMiB', 'producer_parallelism', 'consumer_parallelism']])\n",
    "\n",
    "# Optional: create pair label\n",
    "df_pc['pcPair'] = df_pc['producerTask'] + '→' + df_pc['consumerTask']\n",
    "\n",
    "# Get unique producer-consumer pairs\n",
    "unique_pairs = df_pc['pcPair'].unique()\n",
    "if len(unique_pairs) == 0:\n",
    "    print(\"No producer-consumer pairs available for plotting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Unique producer-consumer pairs: {unique_pairs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== aggregateFilesizeMB Range ===\")\n",
    "print(f\"df_pc: min = {df_pc['aggregateFilesizeMB'].min():.2f}, max = {df_pc['aggregateFilesizeMB'].max():.2f}\")\n",
    "print(f\"df   : min = {df['aggregateFilesizeMB'].min():.2f}, max = {df['aggregateFilesizeMB'].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot settings\n",
    "palette = sns.color_palette(\"tab20\", len(unique_pairs))\n",
    "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h', 'v', '<', '>', '|']\n",
    "pair_group_count = 15\n",
    "pair_groups = [unique_pairs[i:i+pair_group_count] for i in range(0, len(unique_pairs), pair_group_count)]\n",
    "\n",
    "for group_idx, pair_group in enumerate(pair_groups):\n",
    "    subset_group = df_pc[df_pc['pcPair'].isin(pair_group)]\n",
    "    if subset_group.empty:\n",
    "        print(f\"Skipping empty group {group_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Assign colors and markers\n",
    "    color_map = {pair: palette[i % len(palette)] for i, pair in enumerate(pair_group)}\n",
    "    marker_map = {pair: markers[i % len(markers)] for i, pair in enumerate(pair_group)}\n",
    "\n",
    "    seen_labels = set()  # Track labels we've already added\n",
    "\n",
    "    for _, row in subset_group.iterrows():\n",
    "        pair_label = row['pcPair']\n",
    "        producer, consumer = pair_label.split('→')\n",
    "        short_producer = shorten_task_name(producer)\n",
    "        short_consumer = shorten_task_name(consumer)\n",
    "\n",
    "        # Fetch pattern from manual_pairs\n",
    "        pattern = manual_pairs.get((producer, consumer), \"unknown\")\n",
    "        short_label = f\"{short_producer}({int(row['producer_parallelism'])})→{short_consumer}({int(row['consumer_parallelism'])}) [{pattern}]\"\n",
    "\n",
    "        show_label = pair_label not in seen_labels\n",
    "        seen_labels.add(pair_label)\n",
    "\n",
    "        trMiB = row['trMiB'] if pd.notna(row['trMiB']) else 0\n",
    "        opCount = row['opCount'] if pd.notna(row['opCount']) else 0\n",
    "        filesize = row['aggregateFilesizeMB'] if pd.notna(row['aggregateFilesizeMB']) else 0\n",
    "\n",
    "        ax.scatter(\n",
    "            opCount, trMiB, filesize,\n",
    "            label=short_label if show_label else None,\n",
    "            color=color_map[pair_label],\n",
    "            marker=marker_map[pair_label],\n",
    "            s=120\n",
    "        )\n",
    "        ax.plot([opCount, opCount], [trMiB, trMiB], [0, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
    "        ax.plot([opCount, opCount], [0, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
    "        ax.plot([0, opCount], [trMiB, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
    "\n",
    "    # Set font sizes\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=24)\n",
    "    plt.rc('axes', labelsize=24)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=16)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('Operation Count', labelpad=23)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "    ax.set_zlabel('Flow Size (MB)', labelpad=20)\n",
    "    \n",
    "    # Ensure y-axis only shows integers\n",
    "    ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    # ax.view_init(elev=25, azim=135)\n",
    "    # # Flip x-axis from right to left\n",
    "    # ymax = subset_group['trMiB'].max() * 1.1\n",
    "    # ax.set_ylim(ymax, 0)\n",
    "\n",
    "    # Legend updated to reflect file groups\n",
    "    ax.legend(title=\"Producer → Consumer\", loc=\"center\", bbox_to_anchor=(0.125, 0.5, 0.4, 0.0))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(f'{result_path}/pcAgg_group{group_idx+1}_{plot_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['operation', 'taskName', 'prevTask', 'fileName']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "x = df['trMiB']\n",
    "y = df['opCount']\n",
    "z = df['aggregateFilesizeMB']\n",
    "\n",
    "# Generate a color palette\n",
    "unique_tasks = df['taskName'].unique()\n",
    "num_tasks = len(unique_tasks)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=24)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of tasks\n",
    "num_cols = min(3, num_tasks)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_tasks / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_tasks)]\n",
    "\n",
    "for i, (task, (row, col)) in enumerate(zip(unique_tasks, task_indices)):\n",
    "    subset = df[df['taskName'] == task]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['opCount'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "               label=shorten_task_name(task), \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {task}')\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_xlabel('Operation Count', labelpad=13)\n",
    "    ax.set_zlabel('Flow Size (MB)', labelpad=10)\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/tasksubplot_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "# Generate a color palette\n",
    "unique_filegroup = df['file_group'].unique()\n",
    "num_filegroups = len(unique_filegroup)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=24)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of unique file groups\n",
    "num_cols = min(3, num_filegroups)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_filegroups / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_filegroups)]\n",
    "\n",
    "for i, (file, (row, col)) in enumerate(zip(unique_filegroup, task_indices)):\n",
    "    subset = df[df['file_group'] == file]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['trMiB'], subset['opCount'], subset['aggregateFilesizeMB'], \n",
    "               label=file, \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {file}')\n",
    "    ax.set_xlabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_ylabel('Operation Count', labelpad=13)\n",
    "    ax.set_zlabel('Flow Size (MB)', labelpad=10)\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/filegroup_subplot_op_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Generate a color palette\n",
    "unique_filegroup = df['file_group'].unique()\n",
    "num_filegroups = len(unique_filegroup)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=24)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of unique file groups\n",
    "num_cols = min(3, num_filegroups)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_filegroups / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_filegroups)]\n",
    "\n",
    "for i, (file, (row, col)) in enumerate(zip(unique_filegroup, task_indices)):\n",
    "    subset = df[df['file_group'] == file]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['stageOrder'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "               label=file, \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {file}')\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_xlabel('Stage Order', labelpad=13)\n",
    "    ax.set_zlabel('Flow Size (MB)', labelpad=10)\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/filegroup_subplot_stage_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns, df['stageOrder'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
