{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "project_root = \".\"\n",
    "\n",
    "pattern_configs = {\n",
    "    \"ptychonn\":{\n",
    "        \"task_order_list\" : f\"{project_root}/ptychonn/ptychonn_script_order.json\",\n",
    "        \"csv_file_path\" : f'{project_root}/fastflow_plots/ptychonn_14m_tr_estimated.csv',\n",
    "        \"plot_file_name\" : 'ptychonn_3d_relationship_files.pdf',\n",
    "        \"file_group_patterns\" : [\n",
    "            \".*10nm_full.npy\",\n",
    "            \".*diff.npz\",\n",
    "            \".*_test.npy\",\n",
    "            \".*_train.npy\",\n",
    "            \".*_test.npy\",\n",
    "            \".*_train.npy\",\n",
    "            \".*.weights.h5\",\n",
    "            \"Y_test_pred.npy\"\n",
    "        ],\n",
    "        \"result_path\": f\"./ptychonn_plots\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define file grouping file_group_patterns\n",
    "\n",
    "\n",
    "\n",
    "def shorten_task_name(name):\n",
    "    parts = name.split(\"_\")\n",
    "    # print(f\"Original parts: {parts}\")\n",
    "    shortened = [p[:10] for i, p in enumerate(parts) if i % 2 == 0 or i == len(parts) - 1]\n",
    "    # print(f\"Shortened: {shortened}\")\n",
    "    return \"_\".join(shortened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read CSV file\n",
    "# csv_file_path = '{project_root}/fastflow_plots/par_9000_1n_pfs_ps300_fixed.csv'\n",
    "# plot_file_name = '9000_3d_relationship_files.pdf'\n",
    "\n",
    "# csv_file_path = '{project_root}/fastflow_plots/summer_sam_4n_pfs_s9_tr_estimated.csv'\n",
    "# plot_file_name = 'pyflex_3d_relationship_files.pdf'\n",
    "\n",
    "CURR_WF = \"ptychonn\" # pyflex, ddmd, 1kgenome\n",
    "\n",
    "csv_file_path = pattern_configs[CURR_WF][\"csv_file_path\"]\n",
    "plot_file_name = pattern_configs[CURR_WF][\"plot_file_name\"]\n",
    "file_group_patterns = pattern_configs[CURR_WF][\"file_group_patterns\"]\n",
    "task_order_file = pattern_configs[CURR_WF][\"task_order_list\"]\n",
    "extension_grouping = pattern_configs[CURR_WF].get(\"extension_grouping\", False)\n",
    "result_path = pattern_configs[CURR_WF].get(\"result_path\", \"./result_plots\")\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size, df.shape, df.columns, df['operation'].unique()\n",
    "\n",
    "# Replace with shortened task name in all taskName columns\n",
    "short_taskName_dict = {\n",
    "    \"preprocess_data\": \"preproc\",\n",
    "    \"train\": \"train\",\n",
    "    \"inference\": \"infer\",\n",
    "}\n",
    "\n",
    "# Replace task names in the DataFrame\n",
    "df['taskName'] = df['taskName'].replace(short_taskName_dict)\n",
    "# Replace task names in the DataFrame\n",
    "df['prevTask'] = df['prevTask'].replace(short_taskName_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Dataframe Info:\n",
    "\n",
    "# # of unique Stages\n",
    "unique_stages = df['stageOrder'].unique()\n",
    "# # of unique Tasks\n",
    "unique_tasks = df['taskName'].unique()\n",
    "# # Task instance count based on dominant I/O type\n",
    "task_instance_counts = (\n",
    "    df.groupby(['taskName', 'operation'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .max(axis=1)\n",
    ")\n",
    "\n",
    "# If needed as a total count\n",
    "total_task_instances = task_instance_counts.sum()\n",
    "\n",
    "# # of I/O entries (rows in the DataFrame)\n",
    "num_io_entries = df.shape[0]\n",
    "# # of unique I/O files\n",
    "unique_files = df['fileName'].unique()\n",
    "# # average file resues: calculate the average number of times each file is reused for each unique file\n",
    "avg_file_reuses = df.groupby('fileName').size().mean()\n",
    "# Total I/O size\n",
    "total_io_size = df['aggregateFilesizeMB'].sum()\n",
    "\n",
    "# Print all the information\n",
    "print(\"=========================================\")\n",
    "print(f\"Number of unique stages: {len(unique_stages)}\")\n",
    "# print(f\"Number of unique tasks: {len(unique_tasks)}\")\n",
    "print(f\"Total task instances: {total_task_instances}\")\n",
    "print(f\"Total I/O size (MB): {total_io_size:.2f}\")\n",
    "print(f\"Total I/O size (GB): {total_io_size / 1024:.2f}\")\n",
    "# print(f\"Number of I/O entries: {num_io_entries}\")\n",
    "print(f\"Number of unique I/O files: {len(unique_files)}\")\n",
    "print(f\"Average file reuses: {avg_file_reuses:.2f}\")\n",
    "print(\"=========================================\")\n",
    "# Print unique stages and tasks and File names\n",
    "print(f\"Unique stages: {unique_stages}\")\n",
    "print(f\"Unique tasks: {unique_tasks}\")\n",
    "print(f\"Unique file names: {unique_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 95%ile of the aggregateFilesizeMB\n",
    "# 50%ile\n",
    "percentile_50 = df['aggregateFilesizeMB'].quantile(0.50)\n",
    "print(f\"50th percentile of aggregateFilesizeMB: {percentile_50:.2f}\")\n",
    "# 75th percentile\n",
    "percentile_75 = df['aggregateFilesizeMB'].quantile(0.75)\n",
    "print(f\"75th percentile of aggregateFilesizeMB: {percentile_75:.2f}\")\n",
    "# 85th percentile\n",
    "percentile_85 = df['aggregateFilesizeMB'].quantile(0.85)\n",
    "print(f\"85th percentile of aggregateFilesizeMB: {percentile_85:.2f}\")\n",
    "# 95th percentile\n",
    "percentile_95 = df['aggregateFilesizeMB'].quantile(0.95)\n",
    "# 99th percentile\n",
    "percentile_99 = df['aggregateFilesizeMB'].quantile(0.99)\n",
    "print(f\"95th percentile of aggregateFilesizeMB: {percentile_95:.2f}\")\n",
    "print(f\"99th percentile of aggregateFilesizeMB: {percentile_99:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get the max trMiB of taskName tracksingle\n",
    "max_trMiB = df[df['taskName'] == 'IterDe']['trMiB'].max()\n",
    "print(f\"Max trMiB of taskName tracksingle: {max_trMiB:.2f}\")\n",
    "\n",
    "max_aggregateFilesizeMB = df[df['taskName'] == 'IterDe']['aggregateFilesizeMB'].max()\n",
    "print(f\"Max aggregateFilesizeMB of taskName tracksingle: {max_aggregateFilesizeMB:.2f}\")\n",
    "\n",
    "# get max trMiB of df \n",
    "max_trMiB = df['trMiB'].max()\n",
    "print(f\"Max trMiB of all tasks: {max_trMiB:.2f}\")\n",
    "\n",
    "# get min trMiB of df\n",
    "min_trMiB = df['trMiB'].min()\n",
    "print(f\"Min trMiB of all tasks: {min_trMiB:.2f}\")\n",
    "\n",
    "# get 50 percentile of trMiB\n",
    "percentile_50 = df['trMiB'].quantile(0.50)\n",
    "print(f\"50th percentile of trMiB: {percentile_50:.2f}\")\n",
    "\n",
    "# get 75 percentile of trMiB\n",
    "percentile_75 = df['trMiB'].quantile(0.75)\n",
    "print(f\"75th percentile of trMiB: {percentile_75:.2f}\")\n",
    "\n",
    "# get 85 percentile of trMiB\n",
    "percentile_85 = df['trMiB'].quantile(0.85)\n",
    "print(f\"85th percentile of trMiB: {percentile_85:.2f}\")\n",
    "\n",
    "# get average trMiB of all tasks\n",
    "avg_trMiB = df['trMiB'].mean()\n",
    "print(f\"Avg trMiB of all tasks: {avg_trMiB:.2f}\")\n",
    "\n",
    "# get the average trMiB of all unique tasks\n",
    "avg_trMiB = df.groupby('taskName')['trMiB'].mean()\n",
    "print(f\"Avg trMiB of all unique tasks: {avg_trMiB}\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "unique_tasks = df['taskName'].unique()\n",
    "\n",
    "for task in unique_tasks:\n",
    "    avg_trMiB = df[df['taskName'] == task]['trMiB'].mean()\n",
    "    print(f\"Avg trMiB of task {task}: {avg_trMiB:.2f}\")\n",
    "    # get the average opCount of task\n",
    "    avg_opCount = df[df['taskName'] == task]['opCount'].mean()\n",
    "    print(f\"Avg opCount of task {task}: {avg_opCount:.2f}\")\n",
    "    # get number of entries of task\n",
    "    num_entries = df[df['taskName'] == task].shape[0]\n",
    "    print(f\"Number of entries of task {task}: {num_entries}\")\n",
    "    # get the average aggregateFilesizeMB of task\n",
    "    avg_aggregateFilesizeMB = df[df['taskName'] == task]['aggregateFilesizeMB'].mean()\n",
    "    print(f\"Avg aggregateFilesizeMB of task {task}: {avg_aggregateFilesizeMB:.2f}\")\n",
    "    # get number of read entries of task\n",
    "    num_read_entries = df[(df['taskName'] == task) & (df['operation'] == 1)].shape[0]\n",
    "    print(f\"Number of read entries of task {task}: {num_read_entries}\")\n",
    "    # get number of write entries of task\n",
    "    num_write_entries = df[(df['taskName'] == task) & (df['operation'] == 0)].shape[0]\n",
    "    print(f\"Number of write entries of task {task}: {num_write_entries}\")\n",
    "    \n",
    "    # get the parallelized trMiB of task\n",
    "    task_parallelism = df[df['taskName'] == task]['parallelism'].mean()\n",
    "    print(f\"Parallelism of task {task}: {task_parallelism:.2f}\")\n",
    "    if task_parallelism == 1:\n",
    "        task_total_trMiB = df[df['taskName'] == task]['trMiB'].mean()\n",
    "        print(f\"Total trMiB of task {task}: {task_total_trMiB:.2f}\")\n",
    "    else:\n",
    "        task_total_trMiB = df[df['taskName'] == task]['trMiB'].sum()\n",
    "        print(f\"Total trMiB of task {task}: {task_total_trMiB:.2f}\")\n",
    "    task_parallelised_trMiB = task_total_trMiB / task_parallelism\n",
    "    print(f\"Parallelized trMiB of task {task}: {task_parallelised_trMiB:.2f}\")\n",
    "    \n",
    "    # read operation average trMiB\n",
    "    read_trMiB = df[(df['taskName'] == task) & (df['operation'] == 1)]['trMiB'].mean()\n",
    "    print(f\"Avg read trMiB of task {task}: {read_trMiB:.2f}\")\n",
    "    \n",
    "    # write operation average trMiB\n",
    "    write_trMiB = df[(df['taskName'] == task) & (df['operation'] == 0)]['trMiB'].mean()\n",
    "    print(f\"Avg write trMiB of task {task}: {write_trMiB:.2f}\")\n",
    "    \n",
    "    print(\"==========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add new columns for colors and labels based on the 'operation' column\n",
    "# df['color'] = df['operation'].map({0: 'blue', 1: 'red'})\n",
    "# df['label'] = df['operation'].map({0: 'write', 1: 'read'})\n",
    "# df['marker'] = df['operation'].map({0: 'x', 1: 'o'})  # 'o' for circle, 's' for square\n",
    "\n",
    "# # 2D plot: Relationship between aggregateFilesizeMB and totalTime\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# for label in df['label'].unique():\n",
    "#     subset = df[df['label'] == label]\n",
    "#     marker = subset['marker'].iloc[0]\n",
    "#     facecolor = 'none' if marker == 'o' else subset['color'].iloc[0]\n",
    "#     plt.scatter(subset['trMiB'], subset['opCount'], \n",
    "#                 label=label, \n",
    "#                 edgecolor=subset['color'].iloc[0],  # Edge color for circle markers\n",
    "#                 facecolors=facecolor,  # Only the circle markers will be hollow\n",
    "#                 marker=marker,\n",
    "#                 s=200,\n",
    "#                 alpha=0.5)\n",
    "\n",
    "# plt.title('Relationship between Transfer Size and Total Time')\n",
    "# plt.xlabel('I/O Bandwidth (MB/s)')\n",
    "# # plt.xlabel('Data size (MB)')\n",
    "# plt.ylabel('Operation count')\n",
    "# plt.grid(True)\n",
    "# plt.legend(title=\"Operation\")\n",
    "# plt.show()\n",
    "\n",
    "# Define mapping of operation and randomness to I/O type\n",
    "def map_io_type(row):\n",
    "    if row['operation'] == 0 and row['randomOffset'] == 0:\n",
    "        return 'sequential write'\n",
    "    elif row['operation'] == 0 and row['randomOffset'] == 1:\n",
    "        return 'random write'\n",
    "    elif row['operation'] == 1 and row['randomOffset'] == 0:\n",
    "        return 'sequential read'\n",
    "    elif row['operation'] == 1 and row['randomOffset'] == 1:\n",
    "        return 'random read'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply the mapping\n",
    "df['io_type'] = df.apply(map_io_type, axis=1)\n",
    "\n",
    "# Assign colors and markers to each I/O type\n",
    "io_color_map = {\n",
    "    'sequential write': 'blue',\n",
    "    'random write': 'purple',\n",
    "    'sequential read': 'green',\n",
    "    'random read': 'orange',\n",
    "}\n",
    "io_marker_map = {\n",
    "    'sequential write': 'x',\n",
    "    'random write': 'D',\n",
    "    'sequential read': 'o',\n",
    "    'random read': 's',\n",
    "}\n",
    "\n",
    "io_short_name_map = {\n",
    "    \"sequential write\": 'Seq W',\n",
    "    \"random write\": 'Rand W',\n",
    "    \"sequential read\": 'Seq R',\n",
    "    \"random read\": 'Rand R',\n",
    "}\n",
    "\n",
    "df['color'] = df['io_type'].map(io_color_map)\n",
    "df['marker'] = df['io_type'].map(io_marker_map)\n",
    "\n",
    "# 2D plot: Relationship between I/O Bandwidth and Operation Count\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for io_type in df['io_type'].unique():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    marker = subset['marker'].iloc[0]\n",
    "    facecolor = 'none' if marker == 'o' else subset['color'].iloc[0]\n",
    "    plt.scatter(\n",
    "        subset['trMiB'], \n",
    "        subset['opCount'], \n",
    "        label=io_short_name_map[io_type],\n",
    "        edgecolor=subset['color'].iloc[0], \n",
    "        facecolors=facecolor,\n",
    "        marker=marker,\n",
    "        s=200,\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "plt.title('Relationship between I/O Bandwidth and Operation Count')\n",
    "plt.xlabel('I/O Bandwidth (MB/s)')\n",
    "plt.ylabel('Operation Count')\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"I/O Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot: Relationship between aggregateFilesizeMB, stageOrder, and trMiB\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot for each defined I/O type, whether or not data exists\n",
    "for io_type, color in io_color_map.items():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    marker = io_marker_map[io_type]\n",
    "    facecolor = 'none' if marker == 'o' else color\n",
    "\n",
    "    if not subset.empty:\n",
    "        ax.scatter(\n",
    "            subset['opCount'],\n",
    "            subset['trMiB'],\n",
    "            subset['aggregateFilesizeMB'],\n",
    "            label=io_short_name_map[io_type],\n",
    "            edgecolor=color,  \n",
    "            facecolors=facecolor,\n",
    "            marker=marker,\n",
    "            s=200,\n",
    "            alpha=0.5\n",
    "        )\n",
    "\n",
    "        # Add dashed lines from each point to the x and y axes\n",
    "        for _, row in subset.iterrows():\n",
    "            x = row['opCount']\n",
    "            y = row['trMiB']\n",
    "            z = row['aggregateFilesizeMB']\n",
    "            # Line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color,   alpha=0.3, linewidth=1)\n",
    "            # Line to y-axis (z stays same, x drops to 0)\n",
    "            ax.plot([x, 0], [y, y], [z, z], linestyle='dashed', color=color,   alpha=0.3, linewidth=1)\n",
    "\n",
    "    else:\n",
    "        # Dummy point for legend\n",
    "        ax.scatter([], [], [],\n",
    "                   label=io_short_name_map[io_type],\n",
    "                   edgecolor=color,  \n",
    "                   facecolors=facecolor,\n",
    "                   marker=marker,\n",
    "                   s=200,\n",
    "                   alpha=0.5)\n",
    "\n",
    "# Font size settings\n",
    "plt.rc('font', size=20)\n",
    "plt.rc('axes', titlesize=24)\n",
    "plt.rc('axes', labelsize=24)\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rc('legend', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('Operation Count', labelpad=10)\n",
    "ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "\n",
    "ax.legend(title=\"I/O Type\", loc=\"right\", bbox_to_anchor=(0.5, 0.6, 0.4, 0.0))\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f\"{result_path}/op_{plot_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot: Relationship between aggregateFilesizeMB, stageOrder, and trMiB\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot for each defined I/O type, whether or not data exists\n",
    "for io_type, color in io_color_map.items():\n",
    "    subset = df[df['io_type'] == io_type]\n",
    "    marker = io_marker_map[io_type]\n",
    "    facecolor = 'none' if marker == 'o' else color\n",
    "\n",
    "    if not subset.empty:\n",
    "        ax.scatter(\n",
    "            subset['stageOrder'],\n",
    "            subset['trMiB'],\n",
    "            subset['aggregateFilesizeMB'],\n",
    "            label=io_short_name_map[io_type],\n",
    "            edgecolor=color,  \n",
    "            facecolors=facecolor,\n",
    "            marker=marker,\n",
    "            s=200,\n",
    "            alpha=0.5\n",
    "        )\n",
    "        # Add dashed lines from each point to the x and y axes\n",
    "        for _, row in subset.iterrows():\n",
    "            x = row['stageOrder']\n",
    "            y = row['trMiB']\n",
    "            z = row['aggregateFilesizeMB']\n",
    "            # Line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color,   alpha=0.3, linewidth=1)\n",
    "            # Line to y-axis (z stays same, x drops to 0)\n",
    "            ax.plot([x, 0], [y, y], [z, z], linestyle='dashed', color=color,   alpha=0.3, linewidth=1)\n",
    "    else:\n",
    "        # Dummy point for legend\n",
    "        ax.scatter([], [], [],\n",
    "                   label=io_short_name_map[io_type],\n",
    "                   edgecolor=color,  \n",
    "                   facecolors=facecolor,\n",
    "                   marker=marker,\n",
    "                   s=200,\n",
    "                   alpha=0.5)\n",
    "\n",
    "# Font size settings\n",
    "plt.rc('font', size=20)\n",
    "plt.rc('axes', titlesize=24)\n",
    "plt.rc('axes', labelsize=24)\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rc('legend', fontsize=20)\n",
    "\n",
    "# Switched axes\n",
    "ax.set_xlabel('Stage Order', labelpad=10)\n",
    "ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "\n",
    "ax.legend(title=\"I/O Type\", loc=\"right\", bbox_to_anchor=(0.5, 0.6, 0.4, 0.0))\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f\"{result_path}/stage_{plot_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "\n",
    "# Load task ordering json file\n",
    "task_order_dict = {}\n",
    "with open(task_order_file) as f:\n",
    "    task_order_dict = json.load(f)\n",
    "\n",
    "print(task_order_dict)\n",
    "\n",
    "# Ensure tasks are ordered based on task_order_dict\n",
    "unique_tasks = sorted(\n",
    "    df['taskName'].unique(), \n",
    "    key=lambda t: task_order_dict.get(t, {}).get('stage_order', float('inf'))\n",
    ")\n",
    "\n",
    "# Debugging output\n",
    "print(f\"Unique tasks: {unique_tasks}\")  \n",
    "if not unique_tasks:\n",
    "    print(\"No tasks found, skipping plotting.\")\n",
    "    exit()\n",
    "\n",
    "num_tasks = len(unique_tasks)\n",
    "\n",
    "# Define color palette and markers\n",
    "palette = sns.color_palette(\"deep\", num_tasks)\n",
    "# markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# Split tasks into groups of 5\n",
    "task_group_count = 10\n",
    "task_groups = [unique_tasks[i:i+task_group_count] for i in range(0, num_tasks, task_group_count)]\n",
    "\n",
    "for group_idx, task_group in enumerate(task_groups):\n",
    "\n",
    "\n",
    "    # Filter out empty groups\n",
    "    valid_tasks = [task for task in task_group if task in df['taskName'].values]\n",
    "    print(f\"Group {group_idx+1}: {valid_tasks}\")\n",
    "    if not valid_tasks:\n",
    "        print(f\"Skipping empty group {group_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    # 3D plot: Relationship between aggregateFilesizeMB, totalTime, and trMiB\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    x = df['trMiB']\n",
    "    y = df['opCount']\n",
    "    z = df['aggregateFilesizeMB']\n",
    "    markers = ['+', 'x', '3', '2', '*']  # Add more markers as needed\n",
    "\n",
    "    # Generate color and marker maps for current group\n",
    "    color_map = {task: palette[i % len(palette)] for i, task in enumerate(valid_tasks)}\n",
    "    marker_map = {task: markers[i % len(markers)] for i, task in enumerate(valid_tasks)}\n",
    "\n",
    "    df['color'] = df['taskName'].map(lambda task: color_map.get(task, 'black'))  # Black if not found\n",
    "    df['marker'] = df['taskName'].map(lambda task: marker_map.get(task, 'o'))  # 'o' if not found\n",
    "\n",
    "    for task in valid_tasks:\n",
    "        subset = df[df['taskName'] == task]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping task '{task}' (no data)\")\n",
    "            continue\n",
    "        \n",
    "        opCount = subset['opCount']\n",
    "        trMiB = subset['trMiB']\n",
    "        filesize = subset['aggregateFilesizeMB']\n",
    "\n",
    "        for x, y, z in zip(opCount, trMiB, filesize):\n",
    "            ax.scatter(x, y, z,\n",
    "                    color=color_map[task],\n",
    "                    marker=marker_map[task],\n",
    "                    s=200)\n",
    "\n",
    "            # Dashed line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color_map[task], alpha=0.3, linewidth=1)\n",
    "\n",
    "            # Dashed line to y-axis (z stays same, x drops to 0)\n",
    "            ax.plot([x, 0], [y, y], [z, z], linestyle='dashed', color=color_map[task], alpha=0.3, linewidth=1)\n",
    "        ax.scatter([], [], [], label=shorten_task_name(task), color=color_map[task], marker=marker_map[task], s=200)\n",
    "\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=18)\n",
    "    plt.rc('axes', labelsize=18)\n",
    "    plt.rc('xtick', labelsize=18)\n",
    "    plt.rc('ytick', labelsize=18)\n",
    "    plt.rc('legend', fontsize=22)\n",
    "    \n",
    "    \n",
    "    ax.set_xlabel('Operation Count', labelpad=15)\n",
    "    ax.set_ylabel('I/O BW (MB/s)', labelpad=15)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=15)\n",
    "\n",
    "    ax.legend(title=\"Task Name\", loc=\"right\", bbox_to_anchor=(0.05, 0.58, 0.8, 0.0),\n",
    "              labelspacing=0.2, framealpha=0.5, \n",
    "              ) # (0, 0.83, 0.8, 0.0) (0, 0.6, 1.3, 0.0)\n",
    "    # fig.tight_layout()\n",
    "    # plt.figure(constrained_layout=True)\n",
    "    ax.set_box_aspect([1, 1, 1.4])  # Aspect ratio is 1:1:1\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f'{result_path}/taskColor_group{group_idx+1}_'+plot_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Compile regex patterns once\n",
    "compiled_patterns = [(pattern, re.compile(pattern)) for pattern in file_group_patterns]\n",
    "\n",
    "def extract_group(filename):\n",
    "    for pattern_str, pattern in compiled_patterns:\n",
    "        if pattern.search(filename):\n",
    "            return pattern_str.replace(\"\\\\\", \"\")  # Remove backslashes\n",
    "    \n",
    "    # print (f\"Filename '{filename}' did not match any patterns.\")\n",
    "    print(f\"Filename '{filename}' did not match any patterns.\")\n",
    "    return \"group_other\"\n",
    "\n",
    "\n",
    "# Apply to dataframe\n",
    "df[\"file_group\"] = df[\"fileName\"].apply(extract_group)\n",
    "\n",
    "# Check results\n",
    "print(\"Unique file groups:\")\n",
    "print(df[\"file_group\"].value_counts())\n",
    "\n",
    "def padded_range(series, pad_ratio=0.05):\n",
    "    \"\"\"Return min and max with padding based on data range.\"\"\"\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    pad = (max_val - min_val) * pad_ratio\n",
    "    return min_val - pad, max_val + pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Generate a color palette\n",
    "unique_groups = df[\"file_group\"].unique()\n",
    "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "num_groups = len(unique_groups)\n",
    "\n",
    "# Define color palette and markers\n",
    "# Define color palette and markers\n",
    "palette = sns.color_palette(\"deep\", num_groups)\n",
    "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# Groups to exclude for 1000 genome\n",
    "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "# Remove them from unique_groups\n",
    "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "# Split file groups into groups of n\n",
    "file_group_count = 20\n",
    "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, num_groups, file_group_count)]\n",
    "\n",
    "for batch_idx, group_batch in enumerate(group_batches):\n",
    "    # Skip empty groups\n",
    "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "    if not valid_groups:\n",
    "        print(f\"Skipping empty group {batch_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate color and marker maps for current group\n",
    "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "    marker_map = {group: markers[i % len(markers)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "    # Assign colors and markers\n",
    "    df[\"color\"] = df[\"file_group\"].map(lambda group: color_map.get(group, 'black'))\n",
    "    df[\"marker\"] = df[\"file_group\"].map(lambda group: marker_map.get(group, 'o'))\n",
    "\n",
    "    # Iterate through the valid file groups\n",
    "    for group in valid_groups:\n",
    "        subset = df[df[\"file_group\"] == group]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping group '{group}' (no data)\")\n",
    "            continue\n",
    "\n",
    "        # ax.scatter(subset['trMiB'], subset['stageOrder'], subset['aggregateFilesizeMB'], \n",
    "        ax.scatter(subset['stageOrder'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "                   label=group.replace(\"_.*\",\"\"), \n",
    "                   color=color_map[group],  \n",
    "                   marker=marker_map[group],  \n",
    "                   s=200)\n",
    "        # Add dashed lines from each point to the x and y axes\n",
    "        for _, row in subset.iterrows():\n",
    "            x = row['stageOrder']\n",
    "            y = row['trMiB']\n",
    "            z = row['aggregateFilesizeMB']\n",
    "            # Line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color_map[group], alpha=0.3, linewidth=1)\n",
    "            # Line to y-axis (z stays same, x drops to 0)\n",
    "            ax.plot([x, 0], [y, y], [z, z], linestyle='dashed', color=color_map[group], alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Set font sizes\n",
    "    plt.rc('font', size=22)\n",
    "    plt.rc('axes', titlesize=24)\n",
    "    plt.rc('axes', labelsize=24)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=20)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('Stage Order', labelpad=10)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "    \n",
    "    # Ensure y-axis only shows integers\n",
    "    ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    # use less ticks on the y-axis\n",
    "    \n",
    "    \n",
    "\n",
    "    # Legend updated to reflect file groups\n",
    "    ax.legend(title=\"File Group\", loc=\"best\", bbox_to_anchor=(0.2, 0.36, 0.4, 0.0),\n",
    "              labelspacing=0.1, framealpha=0.5,)\n",
    "\n",
    "    # plt.figure(constrained_layout=True)\n",
    "\n",
    "    # Show and save figure\n",
    "    fig.savefig(f'{result_path}/fileColor_group{batch_idx+1}_stage_'+plot_file_name)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Generate a color palette\n",
    "unique_groups = df[\"file_group\"].unique()\n",
    "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
    "\n",
    "num_groups = len(unique_groups)\n",
    "\n",
    "# Define color palette and markers\n",
    "palette = sns.color_palette(\"deep\", num_groups)\n",
    "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
    "\n",
    "# Groups to exclude for 1000 genome\n",
    "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
    "# Remove them from unique_groups\n",
    "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
    "\n",
    "# Split file groups into groups of n\n",
    "file_group_count = 20\n",
    "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, num_groups, file_group_count)]\n",
    "\n",
    "for batch_idx, group_batch in enumerate(group_batches):\n",
    "    # Skip empty groups\n",
    "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
    "    if not valid_groups:\n",
    "        print(f\"Skipping empty group {batch_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate color and marker maps for current group\n",
    "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
    "    marker_map = {group: markers[i % len(markers)] for i, group in enumerate(valid_groups)}\n",
    "\n",
    "    # Assign colors and markers\n",
    "    df[\"color\"] = df[\"file_group\"].map(lambda group: color_map.get(group, 'black'))\n",
    "    df[\"marker\"] = df[\"file_group\"].map(lambda group: marker_map.get(group, 'o'))\n",
    "\n",
    "    # Iterate through the valid file groups\n",
    "    for group in valid_groups:\n",
    "        subset = df[df[\"file_group\"] == group]\n",
    "        if subset.empty:\n",
    "            print(f\"Skipping group '{group}' (no data)\")\n",
    "            continue\n",
    "\n",
    "        ax.scatter(subset['opCount'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "                   label=group.replace(\".*_\",\"\").replace(\".*\", \"\").replace(\".w\", \"w\"),\n",
    "                   color=color_map[group],  \n",
    "                   marker=marker_map[group],  \n",
    "                   s=200)\n",
    "        # Add dashed lines from each point to the x and y axes\n",
    "        for _, row in subset.iterrows():\n",
    "            x = row['opCount']\n",
    "            y = row['trMiB']\n",
    "            z = row['aggregateFilesizeMB']\n",
    "            # Line to x-axis (z stays same, y drops to 0)\n",
    "            ax.plot([x, x], [y, 0], [z, z], linestyle='dashed', color=color_map[group],\n",
    "                    alpha=0.3, linewidth=1)\n",
    "            # Line to y-axis (z stays same, x drops to 0)\n",
    "            ax.plot([x, 0], [y, y], [z, z], linestyle='dashed', color=color_map[group],\n",
    "                    alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Set font sizes\n",
    "    plt.rc('font', size=20)\n",
    "    plt.rc('axes', titlesize=24)\n",
    "    plt.rc('axes', labelsize=24)\n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.rc('ytick', labelsize=20)\n",
    "    plt.rc('legend', fontsize=20)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('Operation Count', labelpad=10)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "    \n",
    "    # Ensure y-axis only shows integers\n",
    "    # ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    # ax.set_xlim(df['opCount'].max() * 1.1, 0)\n",
    "\n",
    "    # Legend updated to reflect file groups\n",
    "    ax.legend(title=\"File Group\", loc=\"best\", bbox_to_anchor=(0.3, 0.35, 0.4, 0.0),\n",
    "              labelspacing=0.15, framealpha=0.5, columnspacing=0.1,\n",
    "              )\n",
    "    \n",
    "    # plt.figure(constrained_layout=True)\n",
    "    # Show and save figure\n",
    "    fig.savefig(f'{result_path}/fileColor_group{batch_idx+1}_op_'+plot_file_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_pairs = {\n",
    "    (\"initial_data\", \"preproc\") : \"n-1\",\n",
    "    (\"preproc\", \"train\") : \"n-1\",\n",
    "    (\"train\", \"infer\") : \"1-1\",\n",
    "    (\"preproc\", \"infer\") : \"1-1\",\n",
    "    (\"infer\", \"final_data\") : \"1-1\",\n",
    "}\n",
    "\n",
    "# Combine based on your manual producer-consumer task pairs\n",
    "rows = []\n",
    "for producer, consumer in manual_pairs.keys():\n",
    "    pc_label = producer + '→' + consumer\n",
    "    \n",
    "    if consumer == \"final_data\":\n",
    "        df_subset_write = df[(df['operation'] == 0) & (df['taskName'] == producer) ]\n",
    "        if df_subset_write.empty:\n",
    "            print(f\"Skipping special pair ({pc_label}): no data\")\n",
    "            continue\n",
    "        \n",
    "        if df_subset_write['parallelism'].mean() > 1:\n",
    "            subset_write_bw = df_subset_write['trMiB'].sum() if not df_subset_write.empty else 0\n",
    "        else:\n",
    "            subset_write_bw = df_subset_write['trMiB'].mean() if not df_subset_write.empty else 0\n",
    "        \n",
    "        consumer_file_group = df_subset_pc_read['file_group'].unique()\n",
    "\n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': df_subset_write['parallelism'].mean(),\n",
    "            'consumer_parallelism': 0,\n",
    "            'transferSize': df_subset_write['transferSize'].sum(),\n",
    "            'aggregateFilesizeMB': df_subset_write['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_write['totalTime'].sum(),\n",
    "            'opCount': df_subset_write['opCount'].sum(),\n",
    "            'trMiB': subset_write_bw,\n",
    "            'stageOrder_read': int(df_subset_write['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_write['stageOrder'].mean()),\n",
    "            'pcPair': pc_label,\n",
    "            'file_groups': consumer_file_group,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        continue\n",
    "    if producer == \"initial_data\":        \n",
    "        df_subset_read = df[(df['operation'] == 1) & (df['taskName'] == consumer) & (df['prevTask'] == producer)]\n",
    "        if df_subset_read.empty:\n",
    "            print(f\"Skipping special pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            # print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_read: {df_subset_read[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        \n",
    "        if df_subset_read['parallelism'].mean() > 1:\n",
    "            subset_read_bw = df_subset_read['trMiB'].sum() if not df_subset_read.empty else 0\n",
    "        else:\n",
    "            subset_read_bw = df_subset_read['trMiB'].mean() if not df_subset_read.empty else 0\n",
    "        \n",
    "        consumer_file_group = df_subset_read['file_group'].unique()\n",
    "        \n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': 0,\n",
    "            'consumer_parallelism': df_subset_read['parallelism'].mean(),\n",
    "            'transferSize': df_subset_read['transferSize'].sum(),\n",
    "            'aggregateFilesizeMB': df_subset_read['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_read['totalTime'].sum(),\n",
    "            'opCount': df_subset_read['opCount'].sum(),\n",
    "            'trMiB': subset_read_bw,\n",
    "            'stageOrder_read': int(df_subset_read['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_read['stageOrder'].mean()),\n",
    "            'pcPair': pc_label,\n",
    "            'file_groups': consumer_file_group,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        continue\n",
    "    else:\n",
    "        # Regular handling\n",
    "        df_subset_pc_read = df[(df['operation'] == 1) & (df['taskName'] == consumer) & (df['prevTask'] == producer)]\n",
    "        # check if any of the dataframes are empty\n",
    "        if df_subset_pc_read.empty:\n",
    "            print(f\"Skipping pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_pc: {df_subset_pc_read[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        # get list of unique file names\n",
    "        consumer_files = df_subset_pc_read['fileName'].unique()\n",
    "        consumer_file_group = df_subset_pc_read['file_group'].unique()\n",
    "        # get the dataframe with the same file names\n",
    "        df_subset_pc_write = df[(df['operation'] == 0) & (df['taskName'] == producer) ] # this workflow only one consumer\n",
    "        \n",
    "        if df_subset_pc_write.empty:\n",
    "            print(f\"Skipping pair ({pc_label}): no data\")\n",
    "            # Printe the dataframe's taskName and prevTask\n",
    "            print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_pc: {df_subset_pc_write[['prevTask', 'taskName']]}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        producer_parallelism = df_subset_pc_write['parallelism'].mean()\n",
    "        consumer_parallelism = df_subset_pc_read['parallelism'].mean()\n",
    "        # print(f\"Producer: {producer}, Consumer: {consumer}, Producer Parallelism: {producer_parallelism}, Consumer Parallelism: {consumer_parallelism}\")\n",
    "        \n",
    "        if producer_parallelism > 1:\n",
    "            # If parallelism > 1, use sum\n",
    "            subset_write_bw = df_subset_pc_write['trMiB'].sum() if not df_subset_pc_write.empty else 0\n",
    "        else:\n",
    "            # If parallelism <= 1, use mean\n",
    "            subset_write_bw = df_subset_pc_write['trMiB'].mean() if not df_subset_pc_write.empty else 0\n",
    "        \n",
    "        if consumer_parallelism > 1:\n",
    "            # If parallelism > 1, use sum\n",
    "            subset_read_bw = df_subset_pc_read['trMiB'].sum() if not df_subset_pc_read.empty else 0\n",
    "        else:\n",
    "            # If parallelism <= 1, use mean\n",
    "            subset_read_bw = df_subset_pc_read['trMiB'].mean() if not df_subset_pc_read.empty else 0\n",
    "\n",
    "        row = {\n",
    "            'producerTask': producer,\n",
    "            'consumerTask': consumer,\n",
    "            'producer_parallelism': producer_parallelism,\n",
    "            'consumer_parallelism': consumer_parallelism,\n",
    "            'transferSize': df_subset_pc_write['transferSize'].mean() + df_subset_pc_read['transferSize'].mean(),\n",
    "            'aggregateFilesizeMB': df_subset_pc_write['aggregateFilesizeMB'].sum() + df_subset_pc_read['aggregateFilesizeMB'].sum(),\n",
    "            'totalTime': df_subset_pc_write['totalTime'].sum() + df_subset_pc_read['totalTime'].sum(),\n",
    "            'opCount': df_subset_pc_write['opCount'].sum() + df_subset_pc_read['opCount'].sum(),\n",
    "            'trMiB': (subset_write_bw + subset_read_bw) / 2,\n",
    "            'stageOrder_read': int(df_subset_pc_read['stageOrder'].mean()),\n",
    "            'stageOrder_write': int(df_subset_pc_write['stageOrder'].mean()),\n",
    "            'pcPair': pc_label,\n",
    "            'file_groups': consumer_file_group, # list of unique file groups\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df_pc = pd.DataFrame(rows)\n",
    "\n",
    "# Show summary\n",
    "print(f\"Constructed producer-consumer DataFrame with {len(df_pc)} pairs.\")\n",
    "print(df_pc[['pcPair', 'aggregateFilesizeMB', 'totalTime', 'trMiB', 'producer_parallelism', 'consumer_parallelism']])\n",
    "\n",
    "# Optional: create pair label\n",
    "df_pc['pcPair'] = df_pc['producerTask'] + '→' + df_pc['consumerTask']\n",
    "\n",
    "# Get unique producer-consumer pairs\n",
    "unique_pairs = df_pc['pcPair'].unique()\n",
    "if len(unique_pairs) == 0:\n",
    "    print(\"No producer-consumer pairs available for plotting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Unique producer-consumer pairs: {unique_pairs}\")\n",
    "\n",
    "print(df_pc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== aggregateFilesizeMB Range ===\")\n",
    "print(f\"df_pc: min = {df_pc['aggregateFilesizeMB'].min():.2f}, max = {df_pc['aggregateFilesizeMB'].max():.2f}\")\n",
    "print(f\"df   : min = {df['aggregateFilesizeMB'].min():.2f}, max = {df['aggregateFilesizeMB'].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "palette = sns.color_palette(\"tab20\", len(unique_pairs))\n",
    "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h', 'v', '<', '>', '|']\n",
    "pair_group_count = 15\n",
    "pair_groups = [unique_pairs[i:i+pair_group_count] for i in range(0, len(unique_pairs), pair_group_count)]\n",
    "\n",
    "for group_idx, pair_group in enumerate(pair_groups):\n",
    "    subset_group = df_pc[df_pc['pcPair'].isin(pair_group)]\n",
    "    if subset_group.empty:\n",
    "        print(f\"Skipping empty group {group_idx+1}\")\n",
    "        continue\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Assign colors and markers\n",
    "    color_map = {pair: palette[i % len(palette)] for i, pair in enumerate(pair_group)}\n",
    "    marker_map = {pair: markers[i % len(markers)] for i, pair in enumerate(pair_group)}\n",
    "\n",
    "    seen_labels = set()  # Track labels we've already added\n",
    "\n",
    "    for _, row in subset_group.iterrows():\n",
    "        pair_label = row['pcPair']\n",
    "        producer, consumer = pair_label.split('→')\n",
    "        short_producer = shorten_task_name(producer)\n",
    "        short_consumer = shorten_task_name(consumer)\n",
    "\n",
    "        # Fetch pattern from manual_pairs\n",
    "        pattern = manual_pairs.get((producer, consumer), \"unknown\")\n",
    "        producer_parallelism = int(row['producer_parallelism'])\n",
    "        consumer_parallelism = int(row['consumer_parallelism'])\n",
    "        if \"gettr\" in producer:\n",
    "            producer_parallelism = 1\n",
    "        if \"gettr\" in consumer:\n",
    "            consumer_parallelism = 1\n",
    "            \n",
    "        # short_label = f\"{short_producer}({producer_parallelism})→{short_consumer}({consumer_parallelism}) [{pattern}]\"\n",
    "        short_label = f\"{short_producer}({producer_parallelism})→[\"\n",
    "        if short_producer == \"freq\" or short_producer == \"mut_olp\":\n",
    "            short_label += f\"chr-.tar.gz,\"\n",
    "        else:\n",
    "            for file_group in row['file_groups']:   \n",
    "                short_label += f\"{file_group},\".replace(\"_.*\", \"\").replace(\".*\",\"\")\n",
    "        # replace the last char with ]\n",
    "        short_label = short_label[:-1] + \"]\"\n",
    "        short_label += f\"→{short_consumer}({consumer_parallelism}) [{pattern}]\"\n",
    "\n",
    "        show_label = pair_label not in seen_labels\n",
    "        seen_labels.add(pair_label)\n",
    "\n",
    "        trMiB = row['trMiB'] if pd.notna(row['trMiB']) else 0\n",
    "        opCount = row['opCount'] if pd.notna(row['opCount']) else 0\n",
    "        filesize = row['aggregateFilesizeMB'] if pd.notna(row['aggregateFilesizeMB']) else 0\n",
    "\n",
    "        ax.scatter(\n",
    "            opCount, trMiB, filesize,\n",
    "            label=short_label if show_label else None,\n",
    "            color=color_map[pair_label],\n",
    "            marker=marker_map[pair_label],\n",
    "            s=320\n",
    "        )\n",
    "        ax.plot([opCount, opCount], [trMiB, trMiB], [0, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
    "        # ax.plot([opCount, opCount], [0, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
    "        ax.plot([0, opCount], [trMiB, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Styling\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 22,\n",
    "        'axes.titlesize': 20,\n",
    "        'axes.labelsize': 20,\n",
    "        'xtick.labelsize': 20,\n",
    "        'ytick.labelsize': 20,\n",
    "        'legend.fontsize': 17,\n",
    "        'legend.title_fontsize': 18,\n",
    "    })\n",
    "\n",
    "    \n",
    "    ax.set_xlabel('Operation Count', labelpad=20)\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=20)\n",
    "    # set zlim to 0 to max * 1.1\n",
    "    ax.set_zlim(0, df_pc['aggregateFilesizeMB'].max() * 1.5)\n",
    "\n",
    "    # Combine all handles/labels into one legend outside the plot (right side)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # do text fix of labels .replace(\"filan_data\", \"final\").replace(\"initial_data\", \"initial\")\n",
    "    labels = [label.replace(\"filan_data\", \"final\").replace(\"initial_data\", \"initial\") for label in labels] \n",
    "    \n",
    "    legend = ax.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        title=\"Producer → [Files] → Consumer\",\n",
    "        loc=\"upper right\",\n",
    "        framealpha=0.2,\n",
    "        facecolor='white',\n",
    "        labelspacing=0.1,\n",
    "        alignment='center',\n",
    "        borderaxespad=0.0,\n",
    "        bbox_to_anchor=(1.12, 0.99),  # pushes legend to the right of the plot\n",
    "    )\n",
    "    ax.add_artist(legend)\n",
    "    \n",
    "    ax.set_box_aspect([1, 1, 0.5])  # Aspect ratio is 1:1:0.5\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95)\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f'{result_path}/pcAgg_group{group_idx+1}_{plot_file_name}')\n",
    "    \n",
    "    # print out all data points in this graph\n",
    "    print(subset_group[['pcPair', 'aggregateFilesizeMB', 'totalTime', 'trMiB', 'producer_parallelism', 'consumer_parallelism']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['operation', 'taskName', 'prevTask', 'fileName']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "x = df['trMiB']\n",
    "y = df['opCount']\n",
    "z = df['aggregateFilesizeMB']\n",
    "\n",
    "# Generate a color palette\n",
    "unique_tasks = df['taskName'].unique()\n",
    "num_tasks = len(unique_tasks)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=24)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of tasks\n",
    "num_cols = min(3, num_tasks)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_tasks / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_tasks)]\n",
    "\n",
    "for i, (task, (row, col)) in enumerate(zip(unique_tasks, task_indices)):\n",
    "    subset = df[df['taskName'] == task]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['opCount'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "               label=shorten_task_name(task), \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {task}')\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_xlabel('Operation Count', labelpad=13)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/tasksubplot_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "# Generate a color palette\n",
    "unique_filegroup = df['file_group'].unique()\n",
    "num_filegroups = len(unique_filegroup)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=24)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of unique file groups\n",
    "num_cols = min(3, num_filegroups)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_filegroups / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_filegroups)]\n",
    "\n",
    "for i, (file, (row, col)) in enumerate(zip(unique_filegroup, task_indices)):\n",
    "    subset = df[df['file_group'] == file]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['trMiB'], subset['opCount'], subset['aggregateFilesizeMB'], \n",
    "               label=file, \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {file}')\n",
    "    ax.set_xlabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_ylabel('Operation Count', labelpad=13)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/filegroup_subplot_op_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Generate a color palette\n",
    "unique_filegroup = df['file_group'].unique()\n",
    "num_filegroups = len(unique_filegroup)\n",
    "\n",
    "# Set the font sizes for various plot elements\n",
    "plt.rc('font', size=18)             # Default text size\n",
    "plt.rc('axes', titlesize=16)        # Axes title font size\n",
    "plt.rc('axes', labelsize=24)        # Axes label font size\n",
    "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
    "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
    "plt.rc('legend', fontsize=16)       # Legend font size\n",
    "\n",
    "# Determine the grid layout based on the number of unique file groups\n",
    "num_cols = min(3, num_filegroups)  # Max 3 columns per row\n",
    "num_rows = int(np.ceil(num_filegroups / num_cols))  # Calculate required rows\n",
    "\n",
    "# Create a figure and define the GridSpec\n",
    "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "\n",
    "# Generate grid positions dynamically\n",
    "task_indices = [(i // num_cols, i % num_cols) for i in range(num_filegroups)]\n",
    "\n",
    "for i, (file, (row, col)) in enumerate(zip(unique_filegroup, task_indices)):\n",
    "    subset = df[df['file_group'] == file]\n",
    "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
    "    ax.scatter(subset['stageOrder'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
    "               label=file, \n",
    "               color='b')\n",
    "    ax.set_title(f'3D Plot for {file}')\n",
    "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
    "    ax.set_xlabel('Stage Order', labelpad=13)\n",
    "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
    "\n",
    "plt.figure(constrained_layout=True)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
    "plt.show()\n",
    "fig.savefig(f'{result_path}/filegroup_subplot_stage_' + plot_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns, df['stageOrder'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
