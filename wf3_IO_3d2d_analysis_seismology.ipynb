{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %matplotlib widget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import math\n",
        "import seaborn as sns\n",
        "import json\n",
        "import matplotlib.ticker as ticker\n",
        "import os\n",
        "\n",
        "project_root = \".\"\n",
        "\n",
        "pattern_configs = {\n",
        "    \"seismology\":{\n",
        "        \"task_order_list\" : f\"{project_root}/seismology/seismology_script_order.json\",\n",
        "        \"csv_file_path\" : f'{project_root}/workflow_data/seismology_fixed.csv',\n",
        "        \"plot_file_name\" : 'seismology_3d_relationship_files.pdf',\n",
        "        \"file_group_patterns\" : [\n",
        "            \"egf.*\\\\.lht\",\n",
        "            \"mshock.*\\\\.lht\",\n",
        "            \".*\\\\_ldsp.lht\",\n",
        "            \"decon.out\",\n",
        "            \".*\\\\_iter_g1.stf\",\n",
        "            \"good-fits.tar.gz\",\n",
        "        ],\n",
        "        \"result_path\": f\"./seismology_plots\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define file grouping file_group_patterns\n",
        "\n",
        "FLOW_SCALE = 400\n",
        "\n",
        "def shorten_task_name(name):\n",
        "    parts = name.split(\"_\")\n",
        "    # print(f\"Original parts: {parts}\")\n",
        "    shortened = [p[:10] for i, p in enumerate(parts) if i % 2 == 0 or i == len(parts) - 1]\n",
        "    # print(f\"Shortened: {shortened}\")\n",
        "    return \"_\".join(shortened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Read CSV file\n",
        "# csv_file_path = '{project_root}/workflow_data/par_9000_1n_pfs_ps300_fixed.csv'\n",
        "# plot_file_name = '9000_3d_relationship_files.pdf'\n",
        "\n",
        "# csv_file_path = '{project_root}/workflow_data/summer_sam_4n_pfs_s9_tr_estimated.csv'\n",
        "# plot_file_name = 'pyflex_3d_relationship_files.pdf'\n",
        "\n",
        "CURR_WF = \"seismology\" # pyflex, ddmd, 1kgenome\n",
        "\n",
        "csv_file_path = pattern_configs[CURR_WF][\"csv_file_path\"]\n",
        "plot_file_name = pattern_configs[CURR_WF][\"plot_file_name\"]\n",
        "file_group_patterns = pattern_configs[CURR_WF][\"file_group_patterns\"]\n",
        "task_order_file = pattern_configs[CURR_WF][\"task_order_list\"]\n",
        "extension_grouping = pattern_configs[CURR_WF].get(\"extension_grouping\", False)\n",
        "result_path = pattern_configs[CURR_WF].get(\"result_path\", \"./result_plots\")\n",
        "\n",
        "# make result_path if it doesn't exist\n",
        "if not os.path.exists(result_path):\n",
        "    os.makedirs(result_path)\n",
        "\n",
        "df = pd.read_csv(csv_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.size, df.shape, df.columns, df['operation'].unique()\n",
        "\n",
        "# Replace with shortened task name in all taskName columns\n",
        "short_taskName_dict = {\n",
        "    \"sG1IterDecon\": \"IterDe\",\n",
        "    \"siftSTFByMisfit\": \"siftSTF\",\n",
        "}\n",
        "\n",
        "# Replace task names in the DataFrame\n",
        "df['taskName'] = df['taskName'].replace(short_taskName_dict)\n",
        "# Replace task names in the DataFrame\n",
        "df['prevTask'] = df['prevTask'].replace(short_taskName_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze Dataframe Info:\n",
        "\n",
        "# # of unique Stages\n",
        "unique_stages = df['stageOrder'].unique()\n",
        "# # of unique Tasks\n",
        "unique_tasks = df['taskName'].unique()\n",
        "# # Task instance count based on dominant I/O type\n",
        "task_instance_counts = (\n",
        "    df.groupby(['taskName', 'operation'])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        "    .max(axis=1)\n",
        ")\n",
        "\n",
        "# If needed as a total count\n",
        "total_task_instances = task_instance_counts.sum()\n",
        "\n",
        "# # of I/O entries (rows in the DataFrame)\n",
        "num_io_entries = df.shape[0]\n",
        "# # of unique I/O files\n",
        "unique_files = df['fileName'].unique()\n",
        "# # average file resues: calculate the average number of times each file is reused for each unique file\n",
        "avg_file_reuses = df.groupby('fileName').size().mean()\n",
        "# Total I/O size\n",
        "total_io_size = df['aggregateFilesizeMB'].sum()\n",
        "\n",
        "# Print all the information\n",
        "print(\"=========================================\")\n",
        "print(f\"Number of unique stages: {len(unique_stages)}\")\n",
        "# print(f\"Number of unique tasks: {len(unique_tasks)}\")\n",
        "print(f\"Total task instances: {total_task_instances}\")\n",
        "print(f\"Total I/O size (MB): {total_io_size:.2f}\")\n",
        "print(f\"Total I/O size (GB): {total_io_size / 1024:.2f}\")\n",
        "# print(f\"Number of I/O entries: {num_io_entries}\")\n",
        "print(f\"Number of unique I/O files: {len(unique_files)}\")\n",
        "print(f\"Average file reuses: {avg_file_reuses:.2f}\")\n",
        "print(\"=========================================\")\n",
        "# Print unique stages and tasks and File names\n",
        "print(f\"Unique stages: {unique_stages}\")\n",
        "print(f\"Unique tasks: {unique_tasks}\")\n",
        "print(f\"Unique file names: {unique_files}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the 95%ile of the aggregateFilesizeMB\n",
        "# 50%ile\n",
        "percentile_50 = df['aggregateFilesizeMB'].quantile(0.50)\n",
        "print(f\"50th percentile of aggregateFilesizeMB: {percentile_50:.2f}\")\n",
        "# 75th percentile\n",
        "percentile_75 = df['aggregateFilesizeMB'].quantile(0.75)\n",
        "print(f\"75th percentile of aggregateFilesizeMB: {percentile_75:.2f}\")\n",
        "# 85th percentile\n",
        "percentile_85 = df['aggregateFilesizeMB'].quantile(0.85)\n",
        "print(f\"85th percentile of aggregateFilesizeMB: {percentile_85:.2f}\")\n",
        "# 95th percentile\n",
        "percentile_95 = df['aggregateFilesizeMB'].quantile(0.95)\n",
        "# 99th percentile\n",
        "percentile_99 = df['aggregateFilesizeMB'].quantile(0.99)\n",
        "print(f\"95th percentile of aggregateFilesizeMB: {percentile_95:.2f}\")\n",
        "print(f\"99th percentile of aggregateFilesizeMB: {percentile_99:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# get the max trMiB of taskName tracksingle\n",
        "max_trMiB = df[df['taskName'] == 'IterDe']['trMiB'].max()\n",
        "print(f\"Max trMiB of taskName tracksingle: {max_trMiB:.2f}\")\n",
        "\n",
        "max_aggregateFilesizeMB = df[df['taskName'] == 'IterDe']['aggregateFilesizeMB'].max()\n",
        "print(f\"Max aggregateFilesizeMB of taskName tracksingle: {max_aggregateFilesizeMB:.2f}\")\n",
        "\n",
        "# get max trMiB of df \n",
        "max_trMiB = df['trMiB'].max()\n",
        "print(f\"Max trMiB of all tasks: {max_trMiB:.2f}\")\n",
        "\n",
        "# get min trMiB of df\n",
        "min_trMiB = df['trMiB'].min()\n",
        "print(f\"Min trMiB of all tasks: {min_trMiB:.2f}\")\n",
        "\n",
        "# get 50 percentile of trMiB\n",
        "percentile_50 = df['trMiB'].quantile(0.50)\n",
        "print(f\"50th percentile of trMiB: {percentile_50:.2f}\")\n",
        "\n",
        "# get 75 percentile of trMiB\n",
        "percentile_75 = df['trMiB'].quantile(0.75)\n",
        "print(f\"75th percentile of trMiB: {percentile_75:.2f}\")\n",
        "\n",
        "# get 85 percentile of trMiB\n",
        "percentile_85 = df['trMiB'].quantile(0.85)\n",
        "print(f\"85th percentile of trMiB: {percentile_85:.2f}\")\n",
        "\n",
        "# get average trMiB of all tasks\n",
        "avg_trMiB = df['trMiB'].mean()\n",
        "print(f\"Avg trMiB of all tasks: {avg_trMiB:.2f}\")\n",
        "\n",
        "# get the average trMiB of all unique tasks\n",
        "avg_trMiB = df.groupby('taskName')['trMiB'].mean()\n",
        "print(f\"Avg trMiB of all unique tasks: {avg_trMiB}\")\n",
        "\n",
        "# get the average opCount of task IterDe\n",
        "avg_opCount = df[df['taskName'] == 'IterDe']['opCount'].mean()\n",
        "print(f\"Avg opCount of task IterDe: {avg_opCount:.2f}\")\n",
        "# get number of entries of task IterDe\n",
        "num_entries = df[df['taskName'] == 'IterDe'].shape[0]\n",
        "print(f\"Number of entries of task IterDe: {num_entries}\")\n",
        "\n",
        "\n",
        "# get the average opCount of task siftSTF\n",
        "avg_opCount = df[df['taskName'] == 'siftSTF']['opCount'].mean()\n",
        "print(f\"Avg opCount of task siftSTF: {avg_opCount:.2f}\")\n",
        "# get number of entries of task siftSTF\n",
        "num_entries = df[df['taskName'] == 'siftSTF'].shape[0]\n",
        "print(f\"Number of entries of task siftSTF: {num_entries}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Add new columns for colors and labels based on the 'operation' column\n",
        "# df['color'] = df['operation'].map({0: 'blue', 1: 'red'})\n",
        "# df['label'] = df['operation'].map({0: 'write', 1: 'read'})\n",
        "# df['marker'] = df['operation'].map({0: 'x', 1: 'o'})  # 'o' for circle, 's' for square\n",
        "\n",
        "# # 2D plot: Relationship between aggregateFilesizeMB and totalTime\n",
        "# plt.figure(figsize=(10, 6))\n",
        "\n",
        "# for label in df['label'].unique():\n",
        "#     subset = df[df['label'] == label]\n",
        "#     marker = subset['marker'].iloc[0]\n",
        "#     facecolor = 'none' if marker == 'o' else subset['color'].iloc[0]\n",
        "#     plt.scatter(subset['trMiB'], subset['opCount'], \n",
        "#                 label=label, \n",
        "#                 edgecolor=subset['color'].iloc[0],  # Edge color for circle markers\n",
        "#                 facecolors=facecolor,  # Only the circle markers will be hollow\n",
        "#                 marker=marker,\n",
        "#                 s=80,\n",
        "#                 alpha=0.5)\n",
        "\n",
        "# plt.title('Relationship between Transfer Size and Total Time')\n",
        "# plt.xlabel('I/O Bandwidth (MB/s)')\n",
        "# # plt.xlabel('Data size (MB)')\n",
        "# plt.ylabel('Operation count')\n",
        "# plt.grid(True)\n",
        "# plt.legend(title=\"Operation\")\n",
        "# plt.show()\n",
        "\n",
        "# Define mapping of operation and randomness to I/O type\n",
        "def map_io_type(row):\n",
        "    if row['operation'] == 0 and row['randomOffset'] == 0:\n",
        "        return 'sequential write'\n",
        "    elif row['operation'] == 0 and row['randomOffset'] == 1:\n",
        "        return 'random write'\n",
        "    elif row['operation'] == 1 and row['randomOffset'] == 0:\n",
        "        return 'sequential read'\n",
        "    elif row['operation'] == 1 and row['randomOffset'] == 1:\n",
        "        return 'random read'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "# Apply the mapping\n",
        "df['io_type'] = df.apply(map_io_type, axis=1)\n",
        "\n",
        "# Assign colors and markers to each I/O type\n",
        "io_color_map = {\n",
        "    'sequential write': 'blue',\n",
        "    'random write': 'purple',\n",
        "    'sequential read': 'green',\n",
        "    'random read': 'orange',\n",
        "}\n",
        "io_marker_map = {\n",
        "    'sequential write': 'x',\n",
        "    'random write': 'D',\n",
        "    'sequential read': 'o',\n",
        "    'random read': 's',\n",
        "}\n",
        "io_short_name_map = {\n",
        "    \"sequential write\": 'Seq W',\n",
        "    \"random write\": 'Rand W',\n",
        "    \"sequential read\": 'Seq R',\n",
        "    \"random read\": 'Rand R',\n",
        "}\n",
        "\n",
        "df['color'] = df['io_type'].map(io_color_map)\n",
        "df['marker'] = df['io_type'].map(io_marker_map)\n",
        "\n",
        "# 2D plot: Relationship between I/O Bandwidth and Operation Count\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for io_type in df['io_type'].unique():\n",
        "    subset = df[df['io_type'] == io_type]\n",
        "    marker = subset['marker'].iloc[0]\n",
        "    facecolor = 'none' if marker == 'o' else subset['color'].iloc[0]\n",
        "    plt.scatter(\n",
        "        subset['trMiB'], \n",
        "        subset['opCount'], \n",
        "        label=io_type, \n",
        "        edgecolor=subset['color'].iloc[0], \n",
        "        facecolors=facecolor,\n",
        "        marker=marker,\n",
        "        s=80,\n",
        "        alpha=0.5\n",
        "    )\n",
        "\n",
        "plt.title('Relationship between I/O Bandwidth and Operation Count')\n",
        "plt.xlabel('I/O Bandwidth (MB/s)')\n",
        "plt.ylabel('Operation Count')\n",
        "plt.grid(True)\n",
        "plt.legend(title=\"I/O Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2D plot: x = opCount, y = trMiB, point size = aggregateFilesizeMB, color = io_type\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "size_scale = FLOW_SCALE\n",
        "\n",
        "# Plot actual data for present I/O types\n",
        "for io_type, color in io_color_map.items():\n",
        "    subset = df[df['io_type'] == io_type]\n",
        "    \n",
        "    if not subset.empty:\n",
        "        sizes = subset['aggregateFilesizeMB'] * size_scale  # Adjust as needed\n",
        "\n",
        "        ax.scatter(\n",
        "            subset['opCount'],\n",
        "            subset['trMiB'],\n",
        "            s=sizes,\n",
        "            label=io_short_name_map[io_type],\n",
        "            color=color,\n",
        "            marker='o',\n",
        "            alpha=0.3\n",
        "        )\n",
        "    else:\n",
        "        ax.scatter([], [], s=80, label=io_short_name_map[io_type], color=color, marker='o', alpha=0.3)\n",
        "\n",
        "# Font size settings\n",
        "plt.rc('font', size=20)\n",
        "# plt.rc('axes', titlesize=22)\n",
        "# plt.rc('axes', labelsize=22)\n",
        "# plt.rc('xtick', labelsize=20)\n",
        "# plt.rc('ytick', labelsize=20)\n",
        "# plt.rc('legend', fontsize=20)\n",
        "\n",
        "ax.set_xlabel('Operation Count', labelpad=20)\n",
        "ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
        "# # Set log scale for x-axis\n",
        "# ax.set_xscale('log')\n",
        "# ====== Create fixed-size legend entries for I/O types =======\n",
        "legend_handles = []\n",
        "for io_type, color in io_color_map.items():\n",
        "    label = io_short_name_map[io_type]\n",
        "    handle = plt.scatter([], [], s=200, color=color, label=label, alpha=0.3)  # Fixed size\n",
        "    legend_handles.append(handle)\n",
        "    \n",
        "# First legend: I/O types\n",
        "legend1 = ax.legend(\n",
        "    handles=legend_handles,\n",
        "    title=\"I/O Type\",\n",
        "    loc='center',\n",
        "    bbox_to_anchor=(0.3, 0.7, 0.8, 0.0),\n",
        "    framealpha=0.18,\n",
        "    facecolor='white'\n",
        ")\n",
        "ax.add_artist(legend1)\n",
        "\n",
        "# Second legend: Dataflow (MB)\n",
        "# Choose some representative values to show in the legend\n",
        "# Get non-zero aggregateFilesizeMB values\n",
        "flow_data = df['aggregateFilesizeMB'].dropna().sort_values()\n",
        "if not flow_data.empty:\n",
        "    min_val = flow_data.min()\n",
        "    if min_val == 0:\n",
        "        min_val = np.percentile(flow_data, 1)\n",
        "    max_val = flow_data.max()\n",
        "    mid1 = np.percentile(flow_data, 33)\n",
        "    mid2 = np.percentile(flow_data, 66)\n",
        "    flow_sizes = [round(mid1,2), round(max_val,2)]\n",
        "else:\n",
        "    flow_sizes = [10, 50, 100, 500]  # fallback\n",
        "    \n",
        "scatter_proxies = [plt.scatter([], [], s=fs * size_scale, color='gray', alpha=0.3) for fs in flow_sizes]\n",
        "labels = [f\"{fs} MB\" for fs in flow_sizes]\n",
        "\n",
        "legend2 = ax.legend(scatter_proxies, labels, title=\"Dataflow\", loc='center', \n",
        "                    bbox_to_anchor=(0.3, 0.28, 0.8, 0.0), framealpha=0.1, facecolor='white')\n",
        "ax.add_artist(legend2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig(f\"{result_path}/op_{plot_file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2D plot: x = opCount, y = trMiB, point size = aggregateFilesizeMB, color = io_type\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "\n",
        "size_scale = FLOW_SCALE\n",
        "\n",
        "# Plot actual data for present I/O types\n",
        "for io_type, color in io_color_map.items():\n",
        "    subset = df[df['io_type'] == io_type]\n",
        "    \n",
        "    if not subset.empty:\n",
        "        sizes = subset['aggregateFilesizeMB'] * size_scale  # Adjust as needed\n",
        "\n",
        "        ax.scatter(\n",
        "            subset['stageOrder'],\n",
        "            subset['trMiB'],\n",
        "            s=sizes,\n",
        "            label=io_short_name_map[io_type],\n",
        "            color=color,\n",
        "            marker='o',\n",
        "            alpha=0.3\n",
        "        )\n",
        "    else:\n",
        "        ax.scatter([], [], s=80, label=io_short_name_map[io_type], color=color, marker='o', alpha=0.3)\n",
        "\n",
        "\n",
        "# Font size settings\n",
        "plt.rc('font', size=20)\n",
        "plt.rc('axes', titlesize=24)\n",
        "plt.rc('axes', labelsize=24)\n",
        "plt.rc('xtick', labelsize=20)\n",
        "plt.rc('ytick', labelsize=20)\n",
        "plt.rc('legend', fontsize=20)\n",
        "\n",
        "ax.set_xlabel('Stage Order', labelpad=20)\n",
        "ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
        "\n",
        "# ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
        "\n",
        "# # First legend: I/O types\n",
        "# legend1 = ax.legend(title=\"I/O Type\", loc='center', bbox_to_anchor=(0.1, 0.8, 0.8, 0.0), framealpha=0.1, facecolor='yellow')\n",
        "# ax.add_artist(legend1)\n",
        "\n",
        "# ====== Create fixed-size legend entries for I/O types =======\n",
        "legend_handles = []\n",
        "for io_type, color in io_color_map.items():\n",
        "    label = io_short_name_map[io_type]\n",
        "    handle = plt.scatter([], [], s=200, color=color, label=label, alpha=0.3)  # Fixed size\n",
        "    legend_handles.append(handle)\n",
        "    \n",
        "# First legend: I/O types\n",
        "legend1 = ax.legend(\n",
        "    handles=legend_handles,\n",
        "    title=\"I/O Type\",\n",
        "    loc='center',\n",
        "    bbox_to_anchor=(0.1, 0.55, 0.8, 0.0),\n",
        "    framealpha=0.1,\n",
        "    facecolor='white'\n",
        ")\n",
        "ax.add_artist(legend1)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig(f\"{result_path}/stage_{plot_file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build task ordering from CSV stage_order (JSON order file removed)\n",
        "if 'stage_order' in df.columns:\n",
        "    stage_order_df = df[['taskName', 'stage_order']].dropna(subset=['taskName', 'stage_order']).copy()\n",
        "    stage_order_df['stage_order'] = pd.to_numeric(stage_order_df['stage_order'], errors='coerce')\n",
        "    stage_order_df = stage_order_df.dropna(subset=['stage_order']).drop_duplicates(subset=['taskName'], keep='first')\n",
        "    task_stage_order = stage_order_df.set_index('taskName')['stage_order'].to_dict()\n",
        "    print(f\"Loaded stage order for {len(task_stage_order)} tasks from CSV: {csv_file_path}\")\n",
        "else:\n",
        "    task_stage_order = {}\n",
        "    print(\"Warning: 'stage_order' column not found in CSV. Falling back to default task ordering.\")\n",
        "\n",
        "size_scale = FLOW_SCALE\n",
        "\n",
        "# Ensure tasks are ordered based on stage order from CSV\n",
        "unique_tasks = sorted(\n",
        "    df['taskName'].dropna().unique(), \n",
        "    key=lambda t: task_stage_order.get(t, float('inf'))\n",
        ")\n",
        "\n",
        "print(f\"Unique tasks: {unique_tasks}\")\n",
        "if not unique_tasks:\n",
        "    print(\"No tasks found, skipping plotting.\")\n",
        "    exit()\n",
        "\n",
        "num_tasks = len(unique_tasks)\n",
        "palette = sns.color_palette(\"deep\", 9)  # Use 9 distinct colors\n",
        "\n",
        "task_group_count = 15\n",
        "task_groups = [unique_tasks[i:i+task_group_count] for i in range(0, num_tasks, task_group_count)]\n",
        "\n",
        "\n",
        "for group_idx, task_group in enumerate(task_groups):\n",
        "    valid_tasks = [task for task in task_group if task in df['taskName'].values]\n",
        "    print(f\"Group {group_idx+1}: {valid_tasks}\")\n",
        "    if not valid_tasks:\n",
        "        print(f\"Skipping empty group {group_idx+1}\")\n",
        "        continue\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n",
        "\n",
        "    color_map = {task: palette[i % len(palette)] for i, task in enumerate(valid_tasks)}\n",
        "\n",
        "    for task in valid_tasks:\n",
        "        subset = df[df['taskName'] == task]\n",
        "        if subset.empty:\n",
        "            print(f\"Skipping task '{task}' (no data)\")\n",
        "            continue\n",
        "\n",
        "        sizes = subset['aggregateFilesizeMB'] * size_scale\n",
        "\n",
        "        ax.scatter(\n",
        "            subset['opCount'],\n",
        "            subset['trMiB'],\n",
        "            s=sizes,\n",
        "            label=shorten_task_name(task),\n",
        "            color=color_map[task],\n",
        "            marker='o',\n",
        "            alpha=0.3\n",
        "        )\n",
        "\n",
        "    # Font size and labels\n",
        "    plt.rc('font', size=20)\n",
        "    plt.rc('axes', titlesize=20)\n",
        "    plt.rc('axes', labelsize=20)\n",
        "    plt.rc('xtick', labelsize=20)\n",
        "    plt.rc('ytick', labelsize=20)\n",
        "    plt.rc('legend', fontsize=20)\n",
        "\n",
        "    ax.set_xlabel('Operation Count', labelpad=20)\n",
        "    ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
        "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
        "\n",
        "    # First legend: Task Name (fixed size dots, only color varies)\n",
        "    task_legend_proxies = [\n",
        "        plt.scatter([], [], s=100, color=color_map[task], alpha=0.3, label=shorten_task_name(task))\n",
        "        for task in valid_tasks\n",
        "    ]\n",
        "    legend1 = ax.legend(handles=task_legend_proxies, title=\"Task Name\", loc='center', \n",
        "                        bbox_to_anchor=(0.16, 0.6, 0.8, 0.0), framealpha=0.1, facecolor='white')\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Second legend: Dataflow (MB)\n",
        "    flow_data = df['aggregateFilesizeMB'].dropna().sort_values()\n",
        "    if not flow_data.empty:\n",
        "        min_val = flow_data.min()\n",
        "        if min_val == 0:\n",
        "            min_val = np.percentile(flow_data, 1)\n",
        "        max_val = flow_data.max()\n",
        "        mid1 = np.percentile(flow_data, 33)\n",
        "        mid2 = np.percentile(flow_data, 66)\n",
        "        flow_sizes = [math.ceil(min_val), int(mid1), int(mid2), int(max_val)]\n",
        "    else:\n",
        "        flow_sizes = [10, 50, 100, 500]  # fallback\n",
        "\n",
        "    # size_legend_proxies = [\n",
        "    #     plt.scatter([], [], s=fs * size_scale, color='gray', alpha=0.3) for fs in flow_sizes\n",
        "    # ]\n",
        "    # size_labels = [f\"{fs} MB\" for fs in flow_sizes]\n",
        "    # legend2 = ax.legend(size_legend_proxies, size_labels, title=\"Dataflow\", loc='center', bbox_to_anchor=(0.36, 0.43, 0.8, 0.0))\n",
        "    # ax.add_artist(legend2)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(f'{result_path}/taskColor_group{group_idx+1}_' + plot_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "# Compile regex patterns once\n",
        "compiled_patterns = [(pattern, re.compile(pattern)) for pattern in file_group_patterns]\n",
        "\n",
        "def extract_group(filename):\n",
        "    for pattern_str, pattern in compiled_patterns:\n",
        "        # if \"ldsp\" in filename:\n",
        "        #     return \"decon.out\"\n",
        "        if pattern.search(filename):\n",
        "            if \"ldsp\" in pattern_str:\n",
        "                return \"decon.out\"\n",
        "            return pattern_str.replace(\"\\\\\", \"\")  # Remove backslashes\n",
        "    return \"group_other\"\n",
        "\n",
        "\n",
        "# Apply to dataframe\n",
        "df[\"file_group\"] = df[\"fileName\"].apply(extract_group)\n",
        "\n",
        "# Check results\n",
        "print(\"Unique file groups:\")\n",
        "print(df[\"file_group\"].value_counts())\n",
        "\n",
        "def padded_range(series, pad_ratio=0.05):\n",
        "    \"\"\"Return min and max with padding based on data range.\"\"\"\n",
        "    min_val = series.min()\n",
        "    max_val = series.max()\n",
        "    pad = (max_val - min_val) * pad_ratio\n",
        "    return min_val - pad, max_val + pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a color palette\n",
        "unique_groups = df[\"file_group\"].unique()\n",
        "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
        "\n",
        "# Exclude certain groups (e.g., for 1000 Genome)\n",
        "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
        "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
        "\n",
        "num_groups = len(unique_groups)\n",
        "palette = sns.color_palette(\"deep\", 9)  # Limit to 9 distinct colors\n",
        "\n",
        "# Group batches\n",
        "file_group_count = 20\n",
        "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, len(unique_groups), file_group_count)]\n",
        "\n",
        "size_scale = FLOW_SCALE\n",
        "\n",
        "for batch_idx, group_batch in enumerate(group_batches):\n",
        "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
        "    if not valid_groups:\n",
        "        print(f\"Skipping empty group {batch_idx+1}\")\n",
        "        continue\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(4, 4))\n",
        "    \n",
        "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
        "\n",
        "    for group in valid_groups:\n",
        "        subset = df[df[\"file_group\"] == group]\n",
        "        if subset.empty:\n",
        "            print(f\"Skipping group '{group}' (no data)\")\n",
        "            continue\n",
        "\n",
        "        sizes = subset[\"aggregateFilesizeMB\"] * size_scale\n",
        "\n",
        "        ax.scatter(\n",
        "            subset[\"stageOrder\"],\n",
        "            subset[\"trMiB\"],\n",
        "            s=sizes,\n",
        "            label=group,\n",
        "            color=color_map[group],\n",
        "            marker='o',\n",
        "            alpha=0.3\n",
        "        )\n",
        "\n",
        "    # Font settings\n",
        "    plt.rc('font', size=22)\n",
        "    plt.rc('axes', titlesize=24)\n",
        "    plt.rc('axes', labelsize=24)\n",
        "    plt.rc('xtick', labelsize=20)\n",
        "    plt.rc('ytick', labelsize=20)\n",
        "    plt.rc('legend', fontsize=17)\n",
        "\n",
        "    ax.set_xlabel(\"Stage Order\", labelpad=20)\n",
        "    ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
        "    ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
        "\n",
        "    # ax.set_ylabel(\"I/O Bandwidth (MB/s)\", labelpad=20)\n",
        "    \n",
        "    # First legend: File Group colors (with fixed-size dummy points)\n",
        "    scatter_proxies = [\n",
        "        plt.scatter([], [], s=100, color=color_map[group], alpha=0.3, label=group.replace(\".*\",\"\"))\n",
        "        for group in valid_groups\n",
        "    ]\n",
        "    # First legend: File Group colors\n",
        "    legend1 = ax.legend(handles=scatter_proxies, title=\"File Group\", loc='center', \n",
        "                        bbox_to_anchor=(0.14, 0.48, 0.8, 0.0), framealpha=0.1, facecolor='white')\n",
        "\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Second legend: Dataflow (MB)\n",
        "    flow_data = df[\"aggregateFilesizeMB\"].dropna().sort_values()\n",
        "    if not flow_data.empty:\n",
        "        min_val = flow_data.min()\n",
        "        if min_val == 0:\n",
        "            min_val = np.percentile(flow_data, 1)\n",
        "        max_val = flow_data.max()\n",
        "        mid1 = np.percentile(flow_data, 33)\n",
        "        mid2 = np.percentile(flow_data, 66)\n",
        "        flow_sizes = [math.ceil(min_val), int(mid1), int(mid2), int(max_val)]\n",
        "    else:\n",
        "        flow_sizes = [10, 50, 100, 500]  # fallback\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{result_path}/fileColor_group{batch_idx+1}_stage_' + plot_file_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a color palette\n",
        "unique_groups = df[\"file_group\"].unique()\n",
        "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
        "\n",
        "# Exclude certain groups (e.g., for 1000 Genome)\n",
        "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
        "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
        "\n",
        "num_groups = len(unique_groups)\n",
        "palette = sns.color_palette(\"deep\", 9)  # Limit to 9 distinct colors\n",
        "\n",
        "# Group batches\n",
        "file_group_count = 20\n",
        "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, len(unique_groups), file_group_count)]\n",
        "\n",
        "size_scale = FLOW_SCALE\n",
        "\n",
        "for batch_idx, group_batch in enumerate(group_batches):\n",
        "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
        "    if not valid_groups:\n",
        "        print(f\"Skipping empty group {batch_idx+1}\")\n",
        "        continue\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    \n",
        "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
        "\n",
        "    for group in valid_groups:\n",
        "        subset = df[df[\"file_group\"] == group]\n",
        "        if subset.empty:\n",
        "            print(f\"Skipping group '{group}' (no data)\")\n",
        "            continue\n",
        "\n",
        "        sizes = subset[\"aggregateFilesizeMB\"] * size_scale\n",
        "\n",
        "        ax.scatter(\n",
        "            subset[\"opCount\"],\n",
        "            subset[\"trMiB\"],\n",
        "            s=sizes,\n",
        "            label=group,\n",
        "            color=color_map[group],\n",
        "            marker='o',\n",
        "            alpha=0.3\n",
        "        )\n",
        "\n",
        "    # Annotate decon.out overlap cluster (x: 100-500, y near 0)\n",
        "    decon_subset = df[df[\"file_group\"] == \"decon.out\"].copy()\n",
        "    if not decon_subset.empty:\n",
        "        near_zero_max = max(1.0, decon_subset[\"trMiB\"].quantile(0.10))\n",
        "        overlap_mask = (\n",
        "            decon_subset[\"opCount\"].between(100, 500)\n",
        "            & decon_subset[\"trMiB\"].between(0, near_zero_max)\n",
        "        )\n",
        "        overlap_count = int(overlap_mask.sum())\n",
        "\n",
        "        if overlap_count > 0:\n",
        "            overlap_points = decon_subset.loc[overlap_mask, [\"opCount\", \"trMiB\"]]\n",
        "            x_anchor = float(overlap_points[\"opCount\"].median())\n",
        "            y_anchor = float(overlap_points[\"trMiB\"].median())\n",
        "        else:\n",
        "            x_anchor = 300.0\n",
        "            y_anchor = float(near_zero_max)\n",
        "\n",
        "        # Use offset points so box position is independent of y-axis magnitude\n",
        "        ax.annotate(\n",
        "            f\"{overlap_count} overlap data\",\n",
        "            xy=(x_anchor, y_anchor),\n",
        "            xytext=(-30, 30),\n",
        "            textcoords=\"offset points\",\n",
        "            arrowprops=dict(arrowstyle=\"->\", color=\"black\", lw=2),\n",
        "            fontsize=18,\n",
        "            color=color_map.get(\"decon.out\", \"tab:blue\"),\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.75),\n",
        "        )\n",
        "\n",
        "    # Font settings\n",
        "    plt.rc('font', size=22)\n",
        "    plt.rc('axes', titlesize=24)\n",
        "    plt.rc('axes', labelsize=24)\n",
        "    plt.rc('xtick', labelsize=20)\n",
        "    plt.rc('ytick', labelsize=20)\n",
        "    plt.rc('legend', fontsize=17)\n",
        "\n",
        "    ax.set_xlabel(\"Operation Count\", labelpad=20)\n",
        "    ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
        "\n",
        "    # ax.set_ylabel(\"I/O Bandwidth (MB/s)\", labelpad=20)\n",
        "    \n",
        "    # First legend: File Group colors (with fixed-size dummy points)\n",
        "    scatter_proxies = [\n",
        "        plt.scatter([], [], s=100, color=color_map[group], alpha=0.3, label=group.replace(\".*\",\"\"))\n",
        "        for group in valid_groups\n",
        "    ]\n",
        "    # First legend: File Group colors\n",
        "    legend1 = ax.legend(handles=scatter_proxies, title=\"File Group\", loc='center', \n",
        "                        bbox_to_anchor=(0.28, 0.62, 0.8, 0.0), framealpha=0.1, facecolor='white')\n",
        "\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Second legend: Dataflow (MB)\n",
        "    flow_data = df[\"aggregateFilesizeMB\"].dropna().sort_values()\n",
        "    if not flow_data.empty:\n",
        "        min_val = flow_data.min()\n",
        "        if min_val == 0:\n",
        "            min_val = np.percentile(flow_data, 1)\n",
        "        max_val = flow_data.max()\n",
        "        mid1 = np.percentile(flow_data, 33)\n",
        "        mid2 = np.percentile(flow_data, 66)\n",
        "        flow_sizes = [math.ceil(min_val), int(mid1), int(mid2), int(max_val)]\n",
        "    else:\n",
        "        flow_sizes = [10, 50, 100, 500]  # fallback\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{result_path}/fileColor_group{batch_idx+1}_op_' + plot_file_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d3_plots = True\n",
        "\n",
        "if d3_plots == True:\n",
        "    # Generate a color palette\n",
        "    unique_groups = df[\"file_group\"].unique()\n",
        "    print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
        "\n",
        "    num_groups = len(unique_groups)\n",
        "\n",
        "    # Define color palette and markers\n",
        "    # Define color palette and markers\n",
        "    palette = sns.color_palette(\"deep\", num_groups)\n",
        "    markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h']\n",
        "\n",
        "    # Groups to exclude for 1000 genome\n",
        "    exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
        "    # Remove them from unique_groups\n",
        "    unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
        "\n",
        "    # Split file groups into groups of n\n",
        "    file_group_count = 20\n",
        "    group_batches = [unique_groups[i:i+file_group_count] for i in range(0, num_groups, file_group_count)]\n",
        "\n",
        "    for batch_idx, group_batch in enumerate(group_batches):\n",
        "        # Skip empty groups\n",
        "        valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
        "        if not valid_groups:\n",
        "            print(f\"Skipping empty group {batch_idx+1}\")\n",
        "            continue\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 8))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        # Generate color and marker maps for current group\n",
        "        color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
        "        marker_map = {group: markers[i % len(markers)] for i, group in enumerate(valid_groups)}\n",
        "\n",
        "        # Assign colors and markers\n",
        "        df[\"color\"] = df[\"file_group\"].map(lambda group: color_map.get(group, 'black'))\n",
        "        df[\"marker\"] = df[\"file_group\"].map(lambda group: marker_map.get(group, 'o'))\n",
        "\n",
        "        # Iterate through the valid file groups\n",
        "        for group in valid_groups:\n",
        "            subset = df[df[\"file_group\"] == group]\n",
        "            if subset.empty:\n",
        "                print(f\"Skipping group '{group}' (no data)\")\n",
        "                continue\n",
        "\n",
        "            # ax.scatter(subset['trMiB'], subset['stageOrder'], subset['aggregateFilesizeMB'], \n",
        "            ax.scatter(subset['opCount'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
        "                    label=group.replace(\".*\",\"\"),\n",
        "                    color=color_map[group],  \n",
        "                    marker=marker_map[group],  \n",
        "                    s=200)\n",
        "        \n",
        "        # Set font sizes\n",
        "        plt.rc('font', size=22)\n",
        "        plt.rc('axes', titlesize=24)\n",
        "        plt.rc('axes', labelsize=24)\n",
        "        plt.rc('xtick', labelsize=20)\n",
        "        plt.rc('ytick', labelsize=20)\n",
        "        plt.rc('legend', fontsize=22)\n",
        "\n",
        "        # Set axis labels\n",
        "        ax.set_xlabel('Operation Count', labelpad=20)\n",
        "        ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
        "        ax.set_zlabel('Dataflow (MB)', labelpad=20)\n",
        "        \n",
        "        # Ensure y-axis only shows 5 integers\n",
        "        ax.yaxis.set_major_locator(ticker.MaxNLocator(nbins=5, integer=True))\n",
        "\n",
        "        # Legend updated to reflect file groups\n",
        "        ax.legend(title=\"File Group\", loc=\"best\", bbox_to_anchor=(0.16, 0.5, 0.8, 0.0), labelspacing=0.1, framealpha=0.3, facecolor='white')\n",
        "\n",
        "        # plt.figure(constrained_layout=True)\n",
        "        # fig.tight_layout()\n",
        "        # Show and save figure\n",
        "        fig.savefig(f'{result_path}/fileColor_group{batch_idx+1}_op3d_'+plot_file_name)\n",
        "        plt.show()\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a color palette\n",
        "unique_groups = df[\"file_group\"].unique()\n",
        "print(f\"Unique groups [{len(unique_groups)}]: {unique_groups}\")\n",
        "\n",
        "# Exclude certain groups (e.g., for 1000 Genome)\n",
        "exclude_groups = ['AFR', 'EAS', 'SAS', 'ALL', 'EUR', 'GBR', 'AMR']\n",
        "unique_groups = [g for g in unique_groups if g not in exclude_groups]\n",
        "\n",
        "num_groups = len(unique_groups)\n",
        "palette = sns.color_palette(\"deep\", 9)  # Limit to 9 distinct colors\n",
        "\n",
        "# Group batches\n",
        "file_group_count = 20\n",
        "group_batches = [unique_groups[i:i+file_group_count] for i in range(0, len(unique_groups), file_group_count)]\n",
        "\n",
        "size_scale = FLOW_SCALE\n",
        "\n",
        "for batch_idx, group_batch in enumerate(group_batches):\n",
        "    valid_groups = [group for group in group_batch if group in df[\"file_group\"].values]\n",
        "    if not valid_groups:\n",
        "        print(f\"Skipping empty group {batch_idx+1}\")\n",
        "        continue\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5.5, 5))\n",
        "    \n",
        "    color_map = {group: palette[i % len(palette)] for i, group in enumerate(valid_groups)}\n",
        "\n",
        "    for group in valid_groups:\n",
        "        subset = df[df[\"file_group\"] == group]\n",
        "        if subset.empty:\n",
        "            print(f\"Skipping group '{group}' (no data)\")\n",
        "            continue\n",
        "\n",
        "        sizes = subset[\"aggregateFilesizeMB\"] * size_scale\n",
        "\n",
        "        ax.scatter(\n",
        "            subset[\"stageOrder\"],\n",
        "            subset[\"trMiB\"],\n",
        "            s=sizes,\n",
        "            label=group,\n",
        "            color=color_map[group],\n",
        "            marker='o',\n",
        "            alpha=0.3\n",
        "        )\n",
        "\n",
        "    # Font settings\n",
        "    plt.rc('font', size=24)\n",
        "    plt.rc('axes', titlesize=24)\n",
        "    plt.rc('axes', labelsize=24)\n",
        "    plt.rc('xtick', labelsize=20)\n",
        "    plt.rc('ytick', labelsize=20)\n",
        "    plt.rc('legend', fontsize=19)\n",
        "\n",
        "    ax.set_xlabel(\"Stage Order\", labelpad=20)\n",
        "    ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
        "\n",
        "    # ax.set_ylabel(\"I/O Bandwidth (MB/s)\", labelpad=20)\n",
        "    \n",
        "    # First legend: File Group colors (with fixed-size dummy points)\n",
        "    scatter_proxies = [\n",
        "        plt.scatter([], [], s=100, color=color_map[group], alpha=0.3, label=group)\n",
        "        for group in valid_groups\n",
        "    ]\n",
        "    # First legend: File Group colors\n",
        "    legend1 = ax.legend(handles=scatter_proxies, title=\"File Group\", loc='center', \n",
        "                        bbox_to_anchor=(0.05, 0.58, 0.8, 0.0), framealpha=0.8, facecolor='white')\n",
        "\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Second legend: Dataflow (MB)\n",
        "    flow_data = df[\"aggregateFilesizeMB\"].dropna().sort_values()\n",
        "    if not flow_data.empty:\n",
        "        min_val = flow_data.min()\n",
        "        if min_val == 0:\n",
        "            min_val = np.percentile(flow_data, 1)\n",
        "        max_val = flow_data.max()\n",
        "        mid1 = np.percentile(flow_data, 33)\n",
        "        mid2 = np.percentile(flow_data, 66)\n",
        "        flow_sizes = [math.ceil(min_val), int(mid1), int(mid2), int(max_val)]\n",
        "    else:\n",
        "        flow_sizes = [10, 50, 100, 500]  # fallback\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{result_path}/fileColor_group{batch_idx+1}_stage_' + plot_file_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get unique values of taskPID\n",
        "unique_taskPIDs = df['taskPID'].unique()\n",
        "print(f\"Unique taskPIDs: {len(unique_taskPIDs)}\")\n",
        "# set parallelism value of df when stageOrder = 1\n",
        "df.loc[df['stageOrder'] == 1, 'parallelism'] = len(unique_taskPIDs) -1 \n",
        "# set parallelism value of df when stageOrder = 2\n",
        "df.loc[df['stageOrder'] == 2, 'parallelism'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "manual_pairs = {\n",
        "    (\"initial_data\", \"IterDe\") : \"1-1\",\n",
        "    (\"IterDe\", \"siftSTF\") : \"n-1\",\n",
        "    (\"siftSTF\", \"final_data\") : \"1-1\",\n",
        "}\n",
        "\n",
        "# Combine based on your manual producer-consumer task pairs\n",
        "rows = []\n",
        "for producer, consumer in manual_pairs.keys():\n",
        "    pc_label = producer + 'â†’' + consumer\n",
        "    \n",
        "    if consumer == \"final_data\":\n",
        "        df_subset_write = df[(df['operation'] == 0) & (df['taskName'] == producer) ]\n",
        "        if df_subset_write.empty:\n",
        "            print(f\"Skipping special pair ({pc_label}): no data\")\n",
        "            continue\n",
        "        \n",
        "        if df_subset_write['parallelism'].mean() > 1:\n",
        "            subset_write_bw = df_subset_write['trMiB'].sum() if not df_subset_write.empty else 0\n",
        "        else:\n",
        "            subset_write_bw = df_subset_write['trMiB'].mean() if not df_subset_write.empty else 0\n",
        "        \n",
        "        consumer_file_group = df_subset_pc_read['file_group'].unique()\n",
        "\n",
        "        row = {\n",
        "            'producerTask': producer,\n",
        "            'consumerTask': consumer,\n",
        "            'producer_parallelism': df_subset_write['parallelism'].mean(),\n",
        "            'consumer_parallelism': 0,\n",
        "            'transferSize': df_subset_write['transferSize'].sum(),\n",
        "            'aggregateFilesizeMB': df_subset_write['aggregateFilesizeMB'].sum(),\n",
        "            'totalTime': df_subset_write['totalTime'].sum(),\n",
        "            'opCount': df_subset_write['opCount'].sum(),\n",
        "            'trMiB': subset_write_bw,\n",
        "            'stageOrder_read': int(df_subset_write['stageOrder'].mean()),\n",
        "            'stageOrder_write': int(df_subset_write['stageOrder'].mean()),\n",
        "            'pcPair': pc_label,\n",
        "            'file_groups': consumer_file_group,\n",
        "        }\n",
        "        rows.append(row)\n",
        "        continue\n",
        "    if producer == \"initial_data\":        \n",
        "        df_subset_read = df[(df['operation'] == 1) & (df['taskName'] == consumer) & (df['prevTask'] == producer)]\n",
        "        if df_subset_read.empty:\n",
        "            print(f\"Skipping special pair ({pc_label}): no data\")\n",
        "            # Printe the dataframe's taskName and prevTask\n",
        "            # print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_read: {df_subset_read[['prevTask', 'taskName']]}\")\n",
        "            continue\n",
        "        \n",
        "        if df_subset_read['parallelism'].mean() > 1:\n",
        "            subset_read_bw = df_subset_read['trMiB'].sum() if not df_subset_read.empty else 0\n",
        "        else:\n",
        "            subset_read_bw = df_subset_read['trMiB'].mean() if not df_subset_read.empty else 0\n",
        "        \n",
        "        consumer_file_group = df_subset_read['file_group'].unique()\n",
        "        \n",
        "        row = {\n",
        "            'producerTask': producer,\n",
        "            'consumerTask': consumer,\n",
        "            'producer_parallelism': 0,\n",
        "            'consumer_parallelism': df_subset_read['parallelism'].mean(),\n",
        "            'transferSize': df_subset_read['transferSize'].sum(),\n",
        "            'aggregateFilesizeMB': df_subset_read['aggregateFilesizeMB'].sum(),\n",
        "            'totalTime': df_subset_read['totalTime'].sum(),\n",
        "            'opCount': df_subset_read['opCount'].sum(),\n",
        "            'trMiB': subset_read_bw,\n",
        "            'stageOrder_read': int(df_subset_read['stageOrder'].mean()),\n",
        "            'stageOrder_write': int(df_subset_read['stageOrder'].mean()),\n",
        "            'pcPair': pc_label,\n",
        "            'file_groups': consumer_file_group,\n",
        "        }\n",
        "        rows.append(row)\n",
        "        continue\n",
        "    else:\n",
        "        # Regular handling\n",
        "        df_subset_pc_read = df[(df['operation'] == 1) & (df['taskName'] == consumer) & (df['prevTask'] == producer)]\n",
        "        # check if any of the dataframes are empty\n",
        "        if df_subset_pc_read.empty:\n",
        "            print(f\"Skipping pair ({pc_label}): no data\")\n",
        "            # Printe the dataframe's taskName and prevTask\n",
        "            print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_pc: {df_subset_pc_read[['prevTask', 'taskName']]}\")\n",
        "            continue\n",
        "        # get list of unique file names\n",
        "        consumer_files = df_subset_pc_read['fileName'].unique()\n",
        "        consumer_file_group = df_subset_pc_read['file_group'].unique()\n",
        "        # get the dataframe with the same file names\n",
        "        df_subset_pc_write = df[(df['operation'] == 0) & (df['taskName'] == producer) ] # this workflow only one consumer\n",
        "        \n",
        "        if df_subset_pc_write.empty:\n",
        "            print(f\"Skipping pair ({pc_label}): no data\")\n",
        "            # Printe the dataframe's taskName and prevTask\n",
        "            print(f\"Producer: {producer}, Consumer: {consumer}, df_subset_pc: {df_subset_pc_write[['prevTask', 'taskName']]}\")\n",
        "            continue\n",
        "        \n",
        "        \n",
        "        producer_parallelism = df_subset_pc_write['parallelism'].mean()\n",
        "        consumer_parallelism = df_subset_pc_read['parallelism'].mean()\n",
        "        # print(f\"Producer: {producer}, Consumer: {consumer}, Producer Parallelism: {producer_parallelism}, Consumer Parallelism: {consumer_parallelism}\")\n",
        "        \n",
        "        if producer_parallelism > 1:\n",
        "            # If parallelism > 1, use sum\n",
        "            subset_write_bw = df_subset_pc_write['trMiB'].sum() if not df_subset_pc_write.empty else 0\n",
        "        else:\n",
        "            # If parallelism <= 1, use mean\n",
        "            subset_write_bw = df_subset_pc_write['trMiB'].mean() if not df_subset_pc_write.empty else 0\n",
        "        \n",
        "        if consumer_parallelism > 1:\n",
        "            # If parallelism > 1, use sum\n",
        "            subset_read_bw = df_subset_pc_read['trMiB'].sum() if not df_subset_pc_read.empty else 0\n",
        "        else:\n",
        "            # If parallelism <= 1, use mean\n",
        "            subset_read_bw = df_subset_pc_read['trMiB'].mean() if not df_subset_pc_read.empty else 0\n",
        "\n",
        "        row = {\n",
        "            'producerTask': producer,\n",
        "            'consumerTask': consumer,\n",
        "            'producer_parallelism': producer_parallelism,\n",
        "            'consumer_parallelism': consumer_parallelism,\n",
        "            'transferSize': df_subset_pc_write['transferSize'].mean() + df_subset_pc_read['transferSize'].mean(),\n",
        "            'aggregateFilesizeMB': df_subset_pc_write['aggregateFilesizeMB'].sum() + df_subset_pc_read['aggregateFilesizeMB'].sum(),\n",
        "            'totalTime': df_subset_pc_write['totalTime'].sum() + df_subset_pc_read['totalTime'].sum(),\n",
        "            'opCount': df_subset_pc_write['opCount'].sum() + df_subset_pc_read['opCount'].sum(),\n",
        "            'trMiB': (subset_write_bw + subset_read_bw) / 2,\n",
        "            'stageOrder_read': int(df_subset_pc_read['stageOrder'].mean()),\n",
        "            'stageOrder_write': int(df_subset_pc_write['stageOrder'].mean()),\n",
        "            'pcPair': pc_label,\n",
        "            'file_groups': consumer_file_group, # list of unique file groups\n",
        "        }\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "df_pc = pd.DataFrame(rows)\n",
        "\n",
        "# Show summary\n",
        "print(f\"Constructed producer-consumer DataFrame with {len(df_pc)} pairs.\")\n",
        "print(df_pc[['pcPair', 'aggregateFilesizeMB', 'totalTime', 'trMiB', 'producer_parallelism', 'consumer_parallelism']])\n",
        "\n",
        "# Optional: create pair label\n",
        "df_pc['pcPair'] = df_pc['producerTask'] + 'â†’' + df_pc['consumerTask']\n",
        "\n",
        "# Get unique producer-consumer pairs\n",
        "unique_pairs = df_pc['pcPair'].unique()\n",
        "if len(unique_pairs) == 0:\n",
        "    print(\"No producer-consumer pairs available for plotting.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Unique producer-consumer pairs: {unique_pairs}\")\n",
        "\n",
        "print(df_pc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== aggregateFilesizeMB Range ===\")\n",
        "print(f\"df_pc: min = {df_pc['aggregateFilesizeMB'].min():.2f}, max = {df_pc['aggregateFilesizeMB'].max():.2f}\")\n",
        "print(f\"df   : min = {df['aggregateFilesizeMB'].min():.2f}, max = {df['aggregateFilesizeMB'].max():.2f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot settings\n",
        "palette = sns.color_palette(\"tab20\", len(unique_pairs))\n",
        "pair_group_count = 20\n",
        "pair_groups = [unique_pairs[i:i + pair_group_count] for i in range(0, len(unique_pairs), pair_group_count)]\n",
        "\n",
        "size_scale = FLOW_SCALE  # Control how large the bubbles appear\n",
        "\n",
        "\n",
        "# Add marker styles\n",
        "marker_styles = ['o', 's', '^', 'v', 'D', 'P', '*', 'X', 'h', '<', '>', 'p', 'H', '|', '_']\n",
        "marker_map = {}\n",
        "\n",
        "for group_idx, pair_group in enumerate(pair_groups):\n",
        "    subset_group = df_pc[df_pc['pcPair'].isin(pair_group)]\n",
        "    if subset_group.empty:\n",
        "        print(f\"Skipping empty group {group_idx + 1}\")\n",
        "        continue\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "\n",
        "    color_map = {pair: palette[i % len(palette)] for i, pair in enumerate(pair_group)}\n",
        "    marker_map = {pair: marker_styles[i % len(marker_styles)] for i, pair in enumerate(pair_group)}\n",
        "    seen_labels = set()\n",
        "\n",
        "    for _, row in subset_group.iterrows():\n",
        "        pair_label = row['pcPair']\n",
        "        producer, consumer = pair_label.split('â†’')\n",
        "        short_producer = shorten_task_name(producer)\n",
        "        short_consumer = shorten_task_name(consumer)\n",
        "        short_producer = short_producer.replace(\"initial_data\", \"initial\").replace(\"final_data\", \"final\")\n",
        "        short_consumer = short_consumer.replace(\"initial_data\", \"initial\").replace(\"final_data\", \"final\")\n",
        "\n",
        "        pattern = manual_pairs.get((producer, consumer), \"unknown\")\n",
        "        producer_parallelism = int(row['producer_parallelism']) if pd.notna(row['producer_parallelism']) else 1\n",
        "        consumer_parallelism = int(row['consumer_parallelism']) if pd.notna(row['consumer_parallelism']) else 1\n",
        "        if \"gettrks\" in producer:\n",
        "            producer_parallelism = 1\n",
        "        if \"gettrks\" in consumer:\n",
        "            consumer_parallelism = 1\n",
        "\n",
        "        short_label = f\"{short_producer}({producer_parallelism})â†’{short_consumer}({consumer_parallelism}) [{pattern}]\"\n",
        "        show_label = pair_label not in seen_labels\n",
        "        seen_labels.add(pair_label)\n",
        "\n",
        "        trMiB = row['trMiB'] if pd.notna(row['trMiB']) else 0\n",
        "        opCount = row['opCount'] if pd.notna(row['opCount']) else 0\n",
        "        filesize = row['aggregateFilesizeMB'] if pd.notna(row['aggregateFilesizeMB']) else 0\n",
        "\n",
        "        ax.scatter(\n",
        "            opCount,\n",
        "            trMiB,\n",
        "            s=filesize * size_scale,\n",
        "            color=color_map[pair_label],\n",
        "            label=short_label if show_label else None,\n",
        "            alpha=0.4,\n",
        "            marker=marker_map[pair_label]\n",
        "        )\n",
        "\n",
        "    # Styling\n",
        "    plt.rcParams.update({\n",
        "        'font.size': 24,\n",
        "        'axes.titlesize': 24,\n",
        "        'axes.labelsize': 24,\n",
        "        'xtick.labelsize': 24,\n",
        "        'ytick.labelsize': 24,\n",
        "        'legend.fontsize': 16\n",
        "    })\n",
        "\n",
        "    ax.set_xlabel('Operation Count', labelpad=23)\n",
        "    # set x axis limit\n",
        "    ax.set_xlim(left=0, right=df_pc['opCount'].max() * 2)\n",
        "\n",
        "    # Create legend with markers\n",
        "    handles = []\n",
        "    labels = []\n",
        "    for pair_label in seen_labels:\n",
        "        producer, consumer = pair_label.split('â†’')\n",
        "        short_producer = shorten_task_name(producer)\n",
        "        short_consumer = shorten_task_name(consumer)\n",
        "        pattern = manual_pairs.get((producer, consumer), \"unknown\")\n",
        "        producer_parallelism = df_pc.loc[df_pc['pcPair'] == pair_label, 'producer_parallelism'].iloc[0]\n",
        "        consumer_parallelism = df_pc.loc[df_pc['pcPair'] == pair_label, 'consumer_parallelism'].iloc[0]\n",
        "        short_label = f\"{short_producer}({int(producer_parallelism)})â†’{short_consumer}({int(consumer_parallelism)}) [{pattern}]\"\n",
        "\n",
        "        patch = plt.scatter([], [], s=200, color=color_map[pair_label], marker=marker_map[pair_label], alpha=0.4, label=short_label)\n",
        "        handles.append(patch)\n",
        "        labels.append(short_label)\n",
        "\n",
        "    # Split long legends\n",
        "    legend1 = ax.legend(handles[:5], labels[:5], \n",
        "                        title=\"Producer â†’ Consumer\", loc=\"center\", \n",
        "                        bbox_to_anchor=(0.5, 0.32), framealpha=0.2, facecolor='white')\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Show the plot properly\n",
        "    plt.show()\n",
        "    plt.tight_layout()\n",
        "    # Save figure with bounding box that includes legend\n",
        "    fig.savefig(f\"{result_path}/pcAgg_group{group_idx + 1}_{plot_file_name}\", bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "\n",
        "def sci_notation(x, pos):\n",
        "    return f'{x:.0e}'\n",
        "\n",
        "# Plot settings\n",
        "palette = sns.color_palette(\"tab10\", len(unique_pairs))\n",
        "markers = ['+', 'x', '3', '2', '*', '^', 's', 'o', 'D', 'p', 'h', 'v', '<', '>', '|']\n",
        "pair_group_count = 15\n",
        "pair_groups = [unique_pairs[i:i+pair_group_count] for i in range(0, len(unique_pairs), pair_group_count)]\n",
        "\n",
        "for group_idx, pair_group in enumerate(pair_groups):\n",
        "    subset_group = df_pc[df_pc['pcPair'].isin(pair_group)]\n",
        "    if subset_group.empty:\n",
        "        print(f\"Skipping empty group {group_idx+1}\")\n",
        "        continue\n",
        "\n",
        "    # Wider-than-tall layout to leave empty upper space for legend\n",
        "    fig = plt.figure(figsize=(14, 5.6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    # Keep x/y the same length; only compress vertical (z) axis\n",
        "    ax.set_box_aspect((1.2, 1.2, 0.5))\n",
        "\n",
        "    # Assign colors and markers\n",
        "    color_map = {pair: palette[i % len(palette)] for i, pair in enumerate(pair_group)}\n",
        "    marker_map = {pair: markers[i % len(markers)] for i, pair in enumerate(pair_group)}\n",
        "\n",
        "    seen_labels = set()  # Track labels we've already added\n",
        "\n",
        "    for _, row in subset_group.iterrows():\n",
        "        pair_label = row['pcPair']\n",
        "        producer, consumer = pair_label.split('â†’')\n",
        "        short_producer = shorten_task_name(producer)\n",
        "        short_consumer = shorten_task_name(consumer)\n",
        "        short_producer = short_producer.replace(\"initial_data\", \"initial\").replace(\"final_data\", \"final\")\n",
        "        short_consumer = short_consumer.replace(\"initial_data\", \"initial\").replace(\"final_data\", \"final\")\n",
        "\n",
        "        # Fetch pattern from manual_pairs\n",
        "        pattern = manual_pairs.get((producer, consumer), \"unknown\")\n",
        "        producer_parallelism = int(row['producer_parallelism'])\n",
        "        consumer_parallelism = int(row['consumer_parallelism'])\n",
        "        if \"gettr\" in producer:\n",
        "            producer_parallelism = 1\n",
        "        if \"gettr\" in consumer:\n",
        "            consumer_parallelism = 1\n",
        "            \n",
        "        # short_label = f\"{short_producer}({producer_parallelism})â†’{short_consumer}({consumer_parallelism}) [{pattern}]\"\n",
        "        short_label = f\"{short_producer}({producer_parallelism})â†’[\"\n",
        "        if short_producer == \"freq\" or short_producer == \"mut_olp\":\n",
        "            short_label += f\"chr-.tar.gz,\"\n",
        "        else:\n",
        "            for file_group in row['file_groups']:\n",
        "                if \"sift\" in short_producer:\n",
        "                        file_group = \"good-fits.tar.gz\"\n",
        "                short_label += f\"{file_group},\".replace(\"_.*\", \"\").replace(\".*\",\"\")\n",
        "        # replace the last char with ]\n",
        "        short_label = short_label[:-1] + \"]\"\n",
        "        short_label += f\"â†’{short_consumer}({consumer_parallelism}) [{pattern}]\"\n",
        "\n",
        "        show_label = pair_label not in seen_labels\n",
        "        seen_labels.add(pair_label)\n",
        "\n",
        "        trMiB = row['trMiB'] if pd.notna(row['trMiB']) else 0\n",
        "        opCount = row['opCount'] if pd.notna(row['opCount']) else 0\n",
        "        filesize = row['aggregateFilesizeMB'] if pd.notna(row['aggregateFilesizeMB']) else 0\n",
        "\n",
        "        ax.scatter(\n",
        "            opCount, trMiB, filesize,\n",
        "            label=short_label if show_label else None,\n",
        "            color=color_map[pair_label],\n",
        "            marker=marker_map[pair_label],\n",
        "            s=300,\n",
        "            alpha=1\n",
        "        )\n",
        "        ax.plot([opCount, opCount], [trMiB, trMiB], [0, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
        "        # ax.plot([opCount, opCount], [0, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
        "        ax.plot([0, opCount], [trMiB, trMiB], [filesize, filesize], linestyle='dashed', color='gray', linewidth=1)\n",
        "\n",
        "\n",
        "\n",
        "    # Styling\n",
        "    plt.rcParams.update({\n",
        "        'font.size': 18,\n",
        "        'axes.titlesize': 18,\n",
        "        'axes.labelsize': 18,\n",
        "        'xtick.labelsize': 12,\n",
        "        'ytick.labelsize': 12,\n",
        "        'legend.fontsize': 17,\n",
        "        'legend.title_fontsize': 18,\n",
        "    })\n",
        "\n",
        "    ax.tick_params(axis='x', which='major', pad=6, labelsize=16)\n",
        "    ax.tick_params(axis='y', which='major', pad=6, labelsize=16)\n",
        "\n",
        "    ax.set_xlabel('Operation Count', labelpad=20)\n",
        "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=20)\n",
        "    ax.set_zlabel('Dataflow (MB)', labelpad=20)\n",
        "    # set zlim to 0 to max * 1.1\n",
        "    ax.set_zlim(0, df_pc['aggregateFilesizeMB'].max() * 1.5)\n",
        "\n",
        "    # Combine all handles/labels into one legend inside the canvas top area\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    legend = ax.legend(\n",
        "        handles,\n",
        "        labels,\n",
        "        title=\"Producer â†’ [Files] â†’ Consumer\",\n",
        "        loc=\"upper center\",\n",
        "        bbox_to_anchor=(0.5, 1.15),\n",
        "        framealpha=0.2,\n",
        "        facecolor='white',\n",
        "        labelspacing=0.4,\n",
        "        alignment='center',\n",
        "        borderaxespad=0.2\n",
        "    )\n",
        "    ax.add_artist(legend)\n",
        "    # ax.zaxis.set_major_formatter(FuncFormatter(sci_notation))\n",
        "    ax.yaxis.set_major_formatter(FuncFormatter(sci_notation))\n",
        "    ax.xaxis.set_major_formatter(FuncFormatter(sci_notation))\n",
        "    \n",
        "    # only 5 ticks in z axis\n",
        "    ax.zaxis.set_major_locator(ticker.MaxNLocator(4))\n",
        "    ax.yaxis.set_major_locator(ticker.MaxNLocator(5))\n",
        "    \n",
        "    # Keep room for labels while avoiding 3D tight-bbox clipping issues\n",
        "    fig.subplots_adjust(left=0.07, right=0.96, bottom=0.16, top=0.90)\n",
        "\n",
        "    fig.savefig(f'{result_path}/pcAgg_group{group_idx+1}_{plot_file_name}')\n",
        "    plt.show()\n",
        "    \n",
        "    # print out all data points in this graph\n",
        "    print(subset_group[['pcPair', 'aggregateFilesizeMB', 'totalTime', 'trMiB', 'producer_parallelism', 'consumer_parallelism']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df[['operation', 'taskName', 'prevTask', 'fileName']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(unique_groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "\n",
        "x = df['trMiB']\n",
        "y = df['opCount']\n",
        "z = df['aggregateFilesizeMB']\n",
        "\n",
        "# Generate a color palette\n",
        "unique_tasks = df['taskName'].unique()\n",
        "num_tasks = len(unique_tasks)\n",
        "\n",
        "# Set the font sizes for various plot elements\n",
        "plt.rc('font', size=18)             # Default text size\n",
        "plt.rc('axes', titlesize=16)        # Axes title font size\n",
        "plt.rc('axes', labelsize=24)        # Axes label font size\n",
        "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
        "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
        "plt.rc('legend', fontsize=16)       # Legend font size\n",
        "\n",
        "# Determine the grid layout based on the number of tasks\n",
        "num_cols = min(3, num_tasks)  # Max 3 columns per row\n",
        "num_rows = int(np.ceil(num_tasks / num_cols))  # Calculate required rows\n",
        "\n",
        "# Create a figure and define the GridSpec\n",
        "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
        "gs = gridspec.GridSpec(num_rows, num_cols)\n",
        "\n",
        "# Generate grid positions dynamically\n",
        "task_indices = [(i // num_cols, i % num_cols) for i in range(num_tasks)]\n",
        "\n",
        "for i, (task, (row, col)) in enumerate(zip(unique_tasks, task_indices)):\n",
        "    subset = df[df['taskName'] == task]\n",
        "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
        "    ax.scatter(subset['opCount'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
        "               label=shorten_task_name(task), \n",
        "               color='b')\n",
        "    ax.set_title(f'3D Plot for {task}')\n",
        "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
        "    ax.set_xlabel('Operation Count', labelpad=13)\n",
        "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
        "\n",
        "plt.figure(constrained_layout=True)\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
        "plt.show()\n",
        "fig.savefig(f'{result_path}/tasksubplot_' + plot_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "\n",
        "# Generate a color palette\n",
        "unique_filegroup = df['file_group'].unique()\n",
        "num_filegroups = len(unique_filegroup)\n",
        "\n",
        "# Set the font sizes for various plot elements\n",
        "plt.rc('font', size=18)             # Default text size\n",
        "plt.rc('axes', titlesize=16)        # Axes title font size\n",
        "plt.rc('axes', labelsize=24)        # Axes label font size\n",
        "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
        "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
        "plt.rc('legend', fontsize=16)       # Legend font size\n",
        "\n",
        "# Determine the grid layout based on the number of unique file groups\n",
        "num_cols = min(3, num_filegroups)  # Max 3 columns per row\n",
        "num_rows = int(np.ceil(num_filegroups / num_cols))  # Calculate required rows\n",
        "\n",
        "# Create a figure and define the GridSpec\n",
        "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
        "gs = gridspec.GridSpec(num_rows, num_cols)\n",
        "\n",
        "# Generate grid positions dynamically\n",
        "task_indices = [(i // num_cols, i % num_cols) for i in range(num_filegroups)]\n",
        "\n",
        "for i, (file, (row, col)) in enumerate(zip(unique_filegroup, task_indices)):\n",
        "    subset = df[df['file_group'] == file]\n",
        "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
        "    ax.scatter(subset['trMiB'], subset['opCount'], subset['aggregateFilesizeMB'], \n",
        "               label=file, \n",
        "               color='b')\n",
        "    ax.set_title(f'3D Plot for {file}')\n",
        "    ax.set_xlabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
        "    ax.set_ylabel('Operation Count', labelpad=13)\n",
        "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
        "\n",
        "plt.figure(constrained_layout=True)\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
        "plt.show()\n",
        "fig.savefig(f'{result_path}/filegroup_subplot_op_' + plot_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Generate a color palette\n",
        "unique_filegroup = df['file_group'].unique()\n",
        "num_filegroups = len(unique_filegroup)\n",
        "\n",
        "# Set the font sizes for various plot elements\n",
        "plt.rc('font', size=18)             # Default text size\n",
        "plt.rc('axes', titlesize=16)        # Axes title font size\n",
        "plt.rc('axes', labelsize=24)        # Axes label font size\n",
        "plt.rc('xtick', labelsize=20)       # X-tick label font size\n",
        "plt.rc('ytick', labelsize=20)       # Y-tick label font size\n",
        "plt.rc('legend', fontsize=16)       # Legend font size\n",
        "\n",
        "# Determine the grid layout based on the number of unique file groups\n",
        "num_cols = min(3, num_filegroups)  # Max 3 columns per row\n",
        "num_rows = int(np.ceil(num_filegroups / num_cols))  # Calculate required rows\n",
        "\n",
        "# Create a figure and define the GridSpec\n",
        "fig = plt.figure(figsize=(num_cols * 10, num_rows * 10))\n",
        "gs = gridspec.GridSpec(num_rows, num_cols)\n",
        "\n",
        "# Generate grid positions dynamically\n",
        "task_indices = [(i // num_cols, i % num_cols) for i in range(num_filegroups)]\n",
        "\n",
        "for i, (file, (row, col)) in enumerate(zip(unique_filegroup, task_indices)):\n",
        "    subset = df[df['file_group'] == file]\n",
        "    ax = fig.add_subplot(gs[row, col], projection='3d')\n",
        "    ax.scatter(subset['stageOrder'], subset['trMiB'], subset['aggregateFilesizeMB'], \n",
        "               label=file, \n",
        "               color='b')\n",
        "    ax.set_title(f'3D Plot for {file}')\n",
        "    ax.set_ylabel('I/O Bandwidth (MB/s)', labelpad=10)\n",
        "    ax.set_xlabel('Stage Order', labelpad=13)\n",
        "    ax.set_zlabel('Dataflow (MB)', labelpad=10)\n",
        "\n",
        "plt.figure(constrained_layout=True)\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.4) \n",
        "plt.show()\n",
        "fig.savefig(f'{result_path}/filegroup_subplot_stage_' + plot_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns, df['stageOrder'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
